#Exponential smoothing {#ch7}

\#1\#2\#3<span>~\#2|\#3~</span> +<span>h~m~^+^</span>

Exponential smoothing was proposed in the late 1950s (Brown 1959, Holt
1957 and Winters 1960 are key pioneering works) and has motivated some
of the most successful forecasting methods. Forecasts produced using
exponential smoothing methods are weighted averages of past
observations, with the weights decaying exponentially as the
observations get older. In other words, the more recent the observation
the higher the associated weight. This framework generates reliable
forecasts quickly and for a wide range of time series which is a great
advantage and of major importance to applications in industry.

This chapter is divided into two parts. In the first part
(Sections \@ref(sec-7-1-SES)--\@ref(sec-7-6-Taxonomy)) we present in detail the
mechanics of the most important exponential smoothing methods and their
application in forecasting time series with various characteristics.
This is key in understanding the intuition behind these methods. In this
setting, selecting and using a forecasting method may appear to be
somewhat ad hoc. The selection of the method is generally based on
recognising key components of the time series (trend and seasonal) and
how these enter the smoothing method (e.g., in an additive, damped or
multiplicative manner).

In the second part of the chapter (Section \@ref(sec-7-ETS)) we present
statistical models that underlie exponential smoothing methods. These
models generate identical point forecasts to the methods discussed in
the first part of the chapter, but also generate prediction intervals.
Furthermore, this statistical framework allows for genuine model
selection between competing models.

##Simple exponential smoothing {#sec-7-1-SES}

The simplest of the exponentially smoothing methods is naturally called
“simple exponential smoothing” (SES)[^1]. This method is suitable for
forecasting data with no trend or seasonal pattern. For example, the
data in Figure \@ref(fig-7-oil) do not display any clear trending behaviour
or any seasonality, although the mean of the data may be changing slowly
over time. We have already considered the naïve and the average as
possible methods for forecasting such data (Section \@ref(sec-2-methods)).

![Oil production in Saudi Arabia from 1996 to 2007.](oil)

\@ref(fig-7-oil)

oildata \<- window(oil,start=1996,end=2007) plot(oildata, ylab=“Oil
(millions of tonnes)”, xlab=“Year”)

Using the naïve method, all forecasts for the future are equal to the
last observed value of the series, $$\hat{y}_{T+h|T} = y_T,$$ for
$h=1,2,\dots$. Hence, the naïve method assumes that the most current
observation is the only important one and all previous observations
provide no information for the future. This can be thought of as a
weighted average where all the weight is given to the last observation.

Using the average method, all future forecasts are equal to a simple
average of the observed data,
$$\hat{y}_{T+h|T} = \frac1T \sum_{t=1}^T y_t,$$ for $h=1,2,\dots$.
Hence, the average method assumes that all observations are of equal
importance and they are given equal weight when generating forecasts.

We often want something between these two extremes. For example it may
be sensible to attach larger weights to more recent observations than to
observations from the distant past. This is exactly the concept behind
simple exponential smoothing. Forecasts are calculated using weighted
averages where the weights decrease exponentially as observations come
from further in the past --- the smallest weights are associated with the
oldest observations:

$$\label{eq-7-ses}
\pred{y}{T+1}{T} = \alpha y_T + \alpha(1-\alpha) y_{T-1} + \alpha(1-\alpha)^2 y_{T-2}+ \alpha(1-\alpha)^3 y_{T-3}+\cdots,$$

where $0 \le \alpha \le 1$ is the smoothing parameter. The
one-step-ahead forecast for time $T+1$ is a weighted average of all the
observations in the series $y_1,\dots,y_T$. The rate at which the
weights decrease is controlled by the parameter $\alpha$.

Table \@ref(tbl-7-alpha) shows the weights attached to observations for four
different values of $\alpha$ when forecasting using simple exponential
smoothing. Note that the sum of the weights even for a small $\alpha$
will be approximately one for any reasonable sample size.

\@ref(tbl-7-alpha)

<span>lllll</span> &\
Observation & $\alpha = 0.2$ & $\alpha = 0.4$ & $\alpha = 0.6$ &
$\alpha = 0.8$\
$y_{T}$ & 0.2 & 0.4 & 0.6 & 0.8\
$y_{T-1}$ & 0.16 & 0.24 & 0.24 & 0.16\
$y_{T-2}$ & 0.128 & 0.144 & 0.096 & 0.032\
$y_{T-3}$ & 0.1024 & 0.0864 & 0.0384 & 0.0064\
$y_{T-4}$ & $(0.2)(0.8)^4$ & $(0.4)(0.6)^4$ & $(0.6)(0.4)^4$ &
$(0.8)(0.2)^4$\
$y_{T-5}$ & $(0.2)(0.8)^5$ & $(0.4)(0.6)^5$ & $(0.6)(0.4)^5$ &
$(0.8)(0.2)^5$\

For any $\alpha$ between 0 and 1, the weights attached to the
observations decrease exponentially as we go back in time, hence the
name “exponential smoothing”. If $\alpha$ is small (i.e., close to 0),
more weight is given to observations from the more distant past. If
$\alpha$ is large (i.e., close to 1), more weight is given to the more
recent observations. At the extreme case where $\alpha=1$,
$\pred{y}{T+1}{T}=y_T$ and forecasts are equal to the naïve forecasts.

We present two equivalent forms of simple exponential smoothing, each of
which leads to the forecast equation .

### Weighted average form {-}

The forecast at time $t+1$ is equal to a weighted average between the
most recent observation $y_t$ and the most recent forecast
$\pred{y}{t}{t-1}$,

$$\pred{y}{t+1}{t} = \alpha y_t + (1-\alpha) \pred{y}{t}{t-1}$$

for $t=1,\dots,T$, where $0 \le \alpha \le 1$ is the smoothing
parameter.

The process has to start somewhere, so we let the first forecast of
$y_1$ be denoted by $\ell_0$ (which we will have to estimate). Then

$$\begin{align*}
\pred{y}{2}{1} &= \alpha y_1 + (1-\alpha) \ell_0\\
\pred{y}{3}{2} &= \alpha y_2 + (1-\alpha) \pred{y}21\\
\pred{y}{4}{3} &= \alpha y_3 + (1-\alpha) \pred{y}32\\
\vdots\\
\pred{y}{T+1}{T} &= \alpha y_T + (1-\alpha) \pred{y}{T}{T-1}\end{align*}$$

Substituting each equation into the following equation, we obtain

$$\begin{align*}
    \pred{y}{3}{2}   & = \alpha y_2 + (1-\alpha) \left[\alpha y_1 + (1-\alpha) \ell_0\right]                           \\
                     & = \alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0                                       \\
    \pred{y}{4}{3}   & = \alpha y_3 + (1-\alpha) [\alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0]             \\
                     & = \alpha y_3 + \alpha(1-\alpha) y_2 + \alpha(1-\alpha)^2 y_1 + (1-\alpha)^3 \ell_0              \\
                     & ~~\vdots                                                                                        \\
    \pred{y}{T+1}{T} & =  \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} + (1-\alpha)^T \ell_{0}.\label{eq-7-waforecasts}\end{align*}$$

So the weighted average form leads to the same forecast equation .

### Component form {-}

An alternative representation is the component form. For simple
exponential smoothing the only component included is the level,
$\ell_t$. (Other methods considered later in this chapter may also
include a trend $b_t$ and seasonal component $s_t$.) Component form
representations of exponential smoothing methods comprise a forecast
equation and a smoothing equation for each of the components included in
the method. The component form of simple exponential smoothing is given
by:

$$\begin{align*}
\text{Forecast equation}&&\pred{y}{t+1}{t} &= \ell_{t}\\
\text{Smoothing equation}&&\ell_{t} &= \alpha y_{t} + (1 - \alpha)\ell_{t-1},\end{align*}$$

where $\ell_{t}$ is the level (or the smoothed value) of the series at
time $t$. The forecast equation shows that the forecasted value at time
$t+1$ is the estimated level at time $t$. The smoothing equation for the
level (usually referred to as the level equation) gives the estimated
level of the series at each period $t$.

Applying the forecast equation for time $T$ gives,
$\pred{y}{T+1}{T} = \ell_{T}$, the most recent estimated level.

If we replace $\ell_t$ by $\pred{y}{t+1}{t}$ and $\ell_{t-1}$ by
$\pred{y}{t}{t-1}$ in the smoothing equation, we will recover the
weighted average form of simple exponential smoothing.

### Multi {-}-horizon Forecasts

So far we have given forecast equations for only one step ahead. Simple
exponential smoothing has a “flat” forecast function, and therefore for
longer forecast horizons,
$$\pred{y}{T+h}{T}=\pred{y}{T+1}{T}=\ell_T, \qquad h=2,3,\dots.$$
Remember these forecasts will only be suitable if the time series has no
trend or seasonal component.

### Optimization {-}

The application of every exponential smoothing method requires the
smoothing parameters and the initial values to be chosen. In particular,
for simple exponential smoothing, we need to select the values of
$\alpha$ and $\ell_0$. All forecasts can be computed from the data once
we know those values. For the methods that follow there is usually more
than one smoothing parameter and more than one initial component to be
chosen.

There are cases where the smoothing parameters may be chosen in a
subjective manner --- the forecaster specifies the value of the smoothing
parameters based on previous experience. However, a more robust and
objective way to obtain values for the unknown parameters included in
any exponential smoothing method is to estimate them from the observed
data.

In Section \@ref(sec-4-2-LSprinciple) we estimated the coefficients of a
regression model by minimizing the sum of the squared errors (SSE).
Similarly, the unknown parameters and the initial values for any
exponential smoothing method can be estimated by minimizing the SSE. The
errors are specified as $e_t=y_t - \pred{y}{t}{t-1}$ for $t=1,\dots,T$
(the one-step-ahead training errors). Hence we find the values of the
unknown parameters and the initial values that minimize

$$\text{SSE}=\sum_{t=1}^T(y_t - \pred{y}{t}{t-1})^2=\sum_{t=1}^Te_t^2. \label{eq-7-SSE}$$

Unlike the regression case (where we have formulae that return the
values of the regression coefficients which minimize the SSE) this
involves a non-linear minimization problem and we need to use an
optimization tool to perform this.

[Oil production]\@ref(ex-7-SES)

![Simple exponential smoothing applied to oil production in Saudi Arabia
(1996--2007).](fig_7_ses)

\@ref(fig-7-ses)

fit \<- ses(oildata, h=3) plot(fit, plot.conf=FALSE, ylab=“Oil (millions
of tonnes)”, xlab=“Year”, main=“”, type=“o”) lines(fit[[“mean”]],
col=“blue”) lines(fitted(fit), col=“red”, type=“o”)
legend(“topleft”,lty=1, col=c(“black”,“red”,“blue”),
c(“data”,“fitted”,“forecasts”),pch=c(1,1,20))

\@ref(tbl-7-ses)

<span>crYYY</span> & **Time** & **Observation** & **Level** &
**Forecast**\
**Year** & $t$ & $y_t$ & $\ell_t$ & $\hat{y}_{t+1|t}$\
1995 & 0 & -- & 447.5 & --\
1996 & 1 & 446.7 & 446.7 & 447.5\
1997 & 2 & 454.5 & 453.6 & 446.7\
1998 & 3 & 455.7 & 455.4 & 453.6\
1999 & 4 & 423.6 & 427.1 & 455.4\
2000 & 5 & 456.3 & 453.1 & 427.1\
2001 & 6 & 440.6 & 441.9 & 453.1\
2002 & 7 & 425.3 & 427.1 & 441.9\
2003 & 8 & 485.1 & 478.9 & 427.1\
2004 & 9 & 506.0 & 503.1 & 478.9\
2005 & 10 & 526.8 & 524.2 & 503.1\
2006 & 11 & 514.3 & 515.3 & 524.2\
2007 & $T=12$ & 494.2 & 496.5 & 515.3\
& $h$ & & & $\hat{y}_{T+h|T}$\
2008 & $1$ & & & 496.5\
2009 & $2$ & & & 496.5\
2010 & $3$ & & & 496.5\
& & & &\
\
 & 20.1\
 & 25.1\
 & 4.3\
 & 7573.4\

\
$\alpha=0.89$ and $\ell_0=447.5$ are obtained by minimizing SSE over
periods

In this example, simple exponential smoothing is applied to forecast oil
production in Saudi Arabia. The black line in Figure \@ref(fig-7-ses) is a
plot of the data over the period 1996--2007, which shows a changing level
over time but no obvious trending behaviour.

In Table \@ref(tbl-7-ses) we demonstrate the application of simple
exponential smoothing. The second last column shows the estimated level
for times $t=0$ to $t=12$; the last column shows the forecasts for
$h=1,2,3$. Using an optimization tool, we find the values of
$\alpha=0.89$ and $\ell_0=447.5$ that minimize the SSE, subject to the
restriction that $0\le\alpha\le1$. So the SSE value presented in the
last row of the table is smaller for these values of $\alpha$ and
$\ell_0$ than for any other values of $\alpha$ and $\ell_0$.

The forecasts for the period 2008--2010 are plotted in
Figure \@ref(fig-7-ses). Also plotted are one-step-ahead training forecasts
alongside the data over the period 1996--2007.

##Holt’s linear trend method

Holt (1957) extended simple exponential smoothing to allow forecasting
of data with a trend. This method involves a forecast equation and two
smoothing equations (one for the level and one for the trend):

$$\begin{align*}
\text{Forecast equation}&& \pred{y}{t+h}{t} &= \ell_{t} + hb_{t} \\
\text{Level equation}&& \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
\text{Trend equation}&& b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)b_{t-1}\end{align*}$$

where $\ell_t$ denotes an estimate of the level of the series at time
$t$, $b_t$ denotes an estimate of the trend (slope) of the series at
time $t$, $\alpha$ is the smoothing parameter for the level,
$0\le\alpha\le1$ and $\beta^*$ is the smoothing parameter for the trend,
$0\le\beta^*\le1$ (we denote this as $\beta^*$ instead of $\beta$ for
reasons that will be explained in Section \@ref(sec-7-ETS)).

As with simple exponential smoothing, the level equation here shows that
$\ell_t$ is a weighted average of observation $y_t$ and the
one-step-ahead training forecast for time $t$, here given by
$\ell_{t-1} + b_{t-1}$. The trend equation shows that $b_t$ is a
weighted average of the estimated trend at time $t$ based on
$\ell_{t} - \ell_{t-1}$ and $b_{t-1}$, the previous estimate of the
trend.

The forecast function is no longer flat but trending. The $h$-step-ahead
forecast is equal to the last estimated level plus $h$ times the last
estimated trend value. Hence the forecasts are a linear function of $h$.

[Air Passengers] In Table \@ref(tbl-7-Holts) we demonstrate the application
of Holt’s linear method to air transportation data for all passengers
with an Australian airline.

The smoothing paramters, $\alpha$ and $\beta$, and the initial values
$\ell_0$ and $b_0$ are estimated by minimizing the SSE for the one-step
training errors as in Section \@ref(sec-7-1-SES).

\@ref(tbl-7-Holts)=0.11cm

@rrc@

ccc@

cccc

&&& &\
**Year** & $t$ & $y_t$ & $\ell_t$ & $b_t$ & $\hat{y}_{t+1|t}$& $\ell_t$
& $b_t$ & $\hat{y}_{t+1|t}$\
1989 & 0 & & 15.71 & 1.73 & & 18.38 & 2.49 &\
1990 & 1 & 17.55 & 17.55 & 1.73 & 17.44 & 17.66 & 2.35 & 20.73\
1991 & 2 & 21.86 & 21.86 & 1.73 & 19.28 & 21.79 & 2.22 & 19.88\
1992 & 3 & 23.89 & 23.89 & 1.73 & 23.59 & 23.89 & 2.09 & 23.88\
1993 & 4 & 26.93 & 26.93 & 1.73 & 25.62 & 26.89 & 1.98 & 25.86\
1994 & 5 & 26.89 & 26.89 & 1.73 & 28.66 & 26.95 & 1.87 & 28.76\
1995 & 6 & 28.83 & 28.83 & 1.73 & 28.62 & 28.83 & 1.76 & 28.72\
1996 & 7 & 30.08 & 30.08 & 1.73 & 30.56 & 30.09 & 1.66 & 30.49\
1997 & 8 & 30.95 & 30.95 & 1.73 & 31.80 & 30.98 & 1.57 & 31.66\
1998 & 9 & 30.19 & 30.19 & 1.73 & 32.68 & 30.26 & 1.48 & 32.46\
1999 & 10 & 31.58 & 31.58 & 1.73 & 31.91 & 31.58 & 1.40 & 31.66\
2000 & 11 & 32.58 & 32.58 & 1.73 & 33.31 & 32.59 & 1.32 & 32.90\
2001 & 12 & 33.48 & 33.48 & 1.73 & 34.31 & 33.49 & 1.25 & 33.84\
2002 & 13 & 39.02 & 39.02 & 1.73 & 35.21 & 38.87 & 1.18 & 34.67\
2003 & 14 & 41.39 & 41.39 & 1.73 & 40.75 & 41.34 & 1.11 & 39.98\
2004 & 15 & 41.60 & 41.60 & 1.73 & 43.12 & 41.62 & 1.05 & 42.39\
& $h$ & & & & $\hat{y}_{T+h|T}$ & & & $\hat{y}_{T+h|T}$\
2005 & 1 & & & & 43.33 & & & 42.62\
2006 & 2 & & & & 45.05 & & & 43.55\
2007 & 3 & & & & 46.78 & & & 44.44\
2008 & 4 & & & & 48.51 & & & 45.27\
2009 & 5 & & & & 50.24 & & & 46.06\

air \<- window(ausair,start=1990,end=2004) fit \<- holt(air, h=5)

fit$model$state fitted(fit) fit[[“mean”]]

![Forecasting Air Passengers in Australia (thousands of passengers). For
all methods $\alpha=0.8$ and $\beta^*=0.2$, and for the additive damped
trend method $\phi=0.85$. ](fig_7_trend.pdf)

\@ref(fig-7-trend)

fit3 \<- holt(air, damped=TRUE, h=5) plot(fit2, type=“o”, ylab=“Air
passengers in Australia (millions)”, xlab=“Year”, fcol=“white”,
plot.conf=FALSE) lines(fitted(fit1), col=“blue”) lines(fitted(fit2),
col=“red”) lines(fitted(fit3), col=“green”) lines(fit1[[“mean”]],
col=“blue”, type=“o”) lines(fit2[[“mean”]], col=“red”, type=“o”)
lines(fit3[[“mean”]], col=“green”, type=“o”) legend(“topleft”, lty=1,
col=c(“black”,“blue”,“red”,“green”), c(“Data”,“Holt’s linear
trend”,“Additive damped trend”))

##Damped trend methods

The forecasts generated by Holt’s linear method display a constant trend
(increasing or decreasing) indefinitely into the future.

Empirical evidence indicates that these methods tend to over-forecast,
especially for longer forecast horizons. Motivated by this observation,
Gardner and McKenzie (1985) introduced a parameter that “dampens” the
trend to a flat line some time in the future. Methods that include a
damped trend have proven to be very successful and are arguably the most
popular individual methods when forecasts are required automatically for
many series.

In conjunction with the smoothing parameters $\alpha$ and $\beta^*$
(with values between 0 and 1 as in Holt’s method), this method also
includes a damping parameter $0<\phi<1$:

$$\begin{align*}
\pred{y}{t+h}{t} &= \ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t} \\
\ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)\phi b_{t-1}.\end{align*}$$

If $\phi=1$ the method is identical to Holt’s linear method. For values
between $0$ and $1$, $\phi$ dampens the trend so that it approaches a
constant some time in the future. In fact the forecasts converge to
$\ell_T+\phi b_T/(1-\phi)$ as $h\rightarrow\infty$ for any value
$0<\phi<1$. The effect of this is that short-run forecasts are trended
while long-run forecasts are constant.

### Example 7 {-}.2    Air Passengers (continued) {#example-7.2air-passengers-continued .unnumbered}

Figure \@ref(fig-7-trend) shows the one-step training forecasts, and the
forecasts for years 2005--2010 generated from Holt’s linear trend method,
and additive damped trend. The most optimistic forecasts come from the
exponential trend method while the least optimistic come from the damped
trend method, with the forecasts generated by Holt’s linear trend method
somewhere between the two.

[Sheep in Asia]\@ref(ex-7-comp)

In this example we compare the forecasting performance of all the
non-seasonal methods we have considered so far in forecasting the sheep
livestock population in Asia. The data spans the period 1970--2007. We
withhold the period 2001--2007 as a test set, and use the data up to and
including year 2000 for the training set (see Section 2.5 for a
definition of training and test sets). Figure \@ref(fig-7-comp) shows that
data and the forecasts from all methods.

The parameters and initial values of the methods are estimated for all
methods by minimizing SSE (as specified in Equation ) over the training
set. In Table \@ref(tbl-7-comp) we present the estimation results and error
measures over the training and the test sets.

livestock2 \<- window(livestock,start=1970,end=2000) fit1 \<-
ses(livestock2) fit2 \<- holt(livestock2) fit4 \<-
holt(livestock2,damped=TRUE) \# Results for first model: fit1[[“model”]]
accuracy(fit1) \# training set accuracy(fit1,livestock) \# test set

\@ref(tbl-7-comp)=0.11cm

<span>lrrrrr</span> & SES & Holt’s & Exponential & Additive &
Multiplicative\
& & linear & trend & damped & damped\
&\
$\alpha$ & 1.00 & 0.98 & 0.98 & 0.99 & 0.98\
$\beta^*$ & -- & 0 & 0 & 0 & 0\
$\phi$ & -- & -- & -- & 0.98 & 0.98\
& & & & &\
$\ell_0$ & 263.92 & 257.78 & 255.52 & 254.58 & 254.69\
$b_0$ & -- & 5.01 & 1.01 & 5.39 & 1.02\
&\
RMSE & 14.77 & 13.92 & 14.06 & 14.00 & 14.03\
SSE & 6761.47 & 6006.06 & 6128.46 & 6080.26 & 6100.11\
&\
MAE & 20.38 & 10.69 & 9.64 & 14.18 & 11.77\
RMSE & 25.46 & 11.88 & 12.50 & 15.78 & 12.62\
MAPE & 4.60 & 2.54 & 2.33 & 3.26 & 2.76\
MASE & 2.26 & 1.19 & 1.07 & 1.57 & 1.31\

$^\dag$ the parameter is restricted to $\phi\le0.98$. See text for more
details.

For the simple exponential smoothing method, the estimated smoothing
parameter is $\alpha=1$. This is expected as the series is clearly
trending over time and simple exponential smoothing requires the largest
possible adjustment in each step to capture this trend.

![Level and slope components for Holt’s linear trend method and the
additive damped trend method.](fig_7_LevelTrend.pdf)

\@ref(fig-7-leveltrend)

plot(fit2$model$state) plot(fit4$model$state)

![Forecasting livestock, sheep in Asia: comparing forecasting
performance of non-seasonal methods. ](fig_7_comp.pdf)

\@ref(fig-7-comp)

plot(fit3, type=“o”, ylab=“Livestock, sheep in Asia (millions)”, flwd=1,
plot.conf=FALSE) lines(window(livestock,start=2001),type=“o”)
lines(fit1$mean,col=2)
lines(fit2$mean,col=3) lines(fit4$mean,col=5)
lines(fit5$mean,col=6) legend(“topleft”, lty=1, pch=1, col=1:6,
c(“Data”,“SES”,“Holt’s”,“Damped trend”))

For the other methods, there is also a trend component.The smoothing
parameter for the slope parameter is estimated to be zero, indicating
that the trend is not changing over time. Of course, the trend estimated
using the damped trend methods will change in the future due to the
damping.

In Figure \@ref(fig-7-leveltrend), we plot the level and trend components for
Holt’s method and for the damped trend method. The slope of the trend
component for Holt’s method is constant, showing that the trend is
linear. In contrast, the slope of the trend component for the damped
trend method is decreasing, showing that the trend is levelling off.

For the damped trend method, the damping parameter $\phi$ is restricted
to a maximum of 0.98 (the estimation returned an optimal value of
$\phi=1$). This restriction is imposed to ensure that the damped trend
method generates noticeably different forecasts from Holt’s linear
method, otherwise we get identical forecasts.

The SSE measures calculated over the training set show that Holt’s
linear trend method provides the best fit to the data followed by the
additive damped trend method. Simple exponential smoothing generates the
largest one-step training errors. In Figure \@ref(fig-7-comp) we can examine
the forecasts generated by the methods. Pretending that we have not seen
the data over the test-set we would conclude that all forecasts are
quite plausible especially from the methods that account for the trend
in the data.

Comparing the forecasting performance of the methods over the test set
in Table \@ref(tbl-7-comp), ??? is the most accurate method according to the
MAE, MAPE and MASE, while Holt’s linear method is most accurate
according to the RMSE.

Conflicting results like this are very common when performing
forecasting competitions between methods. As forecasting tasks can vary
by many dimensions (length of forecast horizon, size of test set,
forecast error measures, frequency of data, etc.), it is unlikely that
one method will be better than all others for all forecasting scenarios.
What we require from a forecasting method are consistently sensible
forecasts, and these should be frequently evaluated against the task at
hand.

##Holt-Winters seasonal method {#sec-7-Taxonomy}

Holt (1957) and Winters (1960) extended Holt’s method to capture
seasonality. The Holt-Winters seasonal method comprises the forecast
equation and three smoothing equations --- one for the level $\ell_t$, one
for trend $b_t$, and one for the seasonal component denoted by $s_t$,
with smoothing parameters $\alpha$, $\beta^*$ and $\gamma$. We use $m$
to denote the period of the seasonality, i.e., the number of seasons in
a year. For example, for quarterly data $m=4$, and for monthly data
$m=12$.

There are two variations to this method that differ in the nature of the
seasonal component. The additive method is preferred when the seasonal
variations are roughly constant through the series, while the
multiplicative method is preferred when the seasonal variations are
changing proportional to the level of the series. With the additive
method, the seasonal component is expressed in absolute terms in the
scale of the observed series, and in the level equation the series is
seasonally adjusted by subtracting the seasonal component. Within each
year the seasonal component will add up to approximately zero. With the
multiplicative method, the seasonal component is expressed in relative
terms (percentages) and the series is seasonally adjusted by dividing
through by the seasonal component. Within each year, the seasonal
component will sum up to approximately $m$.

### Holt {-}-Winters additive method

The component form for the additive method is:

$$\begin{align*}
\pred{y}{t+h}{t} &= \ell_{t} + hb_{t} + s_{t-m+\h+} \\
\ell_{t} &= \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)b_{t-1}\\
s_{t} &= \gamma (y_{t}-\ell_{t-1}-b_{t-1}) + (1-\gamma)s_{t-m},\end{align*}$$

where $\h+=\lfloor(h-1)\mod~m\rfloor+1$, which ensures that the
estimates of the seasonal indices used for forecasting come from the
final year of the sample. The level equation shows a weighted average
between the seasonally adjusted observation $(y_{t} - s_{t-m})$ and the
non-seasonal forecast $(\ell_{t-1}+b_{t-1})$ for time $t$. The trend
equation is identical to Holt’s linear method. The seasonal equation
shows a weighted average between the current seasonal index,
$(y_{t}-\ell_{t-1}-b_{t-1})$, and the seasonal index of the same season
last year (i.e., $m$ time periods ago).

The equation for the seasonal component is often expressed as
$$s_{t} = \gamma^* (y_{t}-\ell_{t})+ (1-\gamma^*)s_{t-m}.$$ If we
substitute $\ell_t$ from the smoothing equation for the level of the
component form above, we get
$$s_{t} = \gamma^*(1-\alpha) (y_{t}-\ell_{t-1}-b_{t-1})+ [1-\gamma^*(1-\alpha)]s_{t-m}$$
which is identical to the smoothing equation for the seasonal component
we specify here with $\gamma=\gamma^*(1-\alpha)$. The usual parameter
restriction is $0\le\gamma^*\le1$, which translates to
$0\le\gamma\le 1-\alpha$.

### Holt {-}-Winters multiplicative method

The component form for the multiplicative method is:

$$\begin{align*}
\pred{y}{t+h}{t} &= (\ell_{t} + hb_{t})s_{t-m+h_{m}^{+}}. \\
\ell_{t} &= \alpha \frac{y_{t}}{s_{t-m}} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
b_{t} &= \beta^*(\ell_{t}-\ell_{t-1}) + (1 - \beta^*)b_{t-1}                \\
s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + b_{t-1})} + (1 - \gamma)s_{t-m}\end{align*}$$

[International tourist visitor nights in Australia]

![Forecasting international visitor nights in Australia using
Holt-Winters method with both additive and multiplicative
seasonality.](fig_7_HW.pdf)

\@ref(fig-7-HW)

aust \<- window(austourists,start=2005) fit1 \<-
hw(aust,seasonal=“additive”) fit2 \<- hw(aust,seasonal=“multiplicative”)

plot(fit2,ylab=“International visitor night in Australia (millions)”,
plot.conf=FALSE, type=“o”, fcol=“white”, xlab=“Year”)
lines(fitted(fit1), col=“red”, lty=2) lines(fitted(fit2), col=“green”,
lty=2) lines(fit1$mean, type="o", col="red")
lines(fit2$mean, type=“o”, col=“green”) legend(“topleft”,lty=1, pch=1,
col=1:3, c(“data”,“Holt Winters’ Additive”,“Holt Winters’
Multiplicative”))

In this example we employ the Holt-Winters method with both additive and
multiplicative seasonality to forecast tourists visitor nights in
Australia by international arrivals. Figure \@ref(fig-7-HW) shows the data
alongside the one-step-ahead training forecasts over the sample period
2005Q1--2010Q4 and the forecasts for the period 2011Q1--2012Q4. The data
show an obvious seasonal pattern with peaks observed in the March
quarter of each year as this corresponds to the Australian summer.

The application of the method with additive and multiplicative
seasonality are presented in Tables \@ref(tbl-7-HWadd) and \@ref(tbl-7-HWmulti)
respectively. The results show that the method with the multiplicative
seasonality fits the data best. This was expected as the time plot shows
the seasonal variation in the data increases as the level of the series
increases. This is also reflected in the two sets of forecasts; the
forecasts generated by the method with the multiplicative seasonality
portray larger and increasing seasonal variation as the level of the
forecasts increases compared to the forecasts generated by the method
with additive seasonality.

\@ref(tbl-7-HWadd) =0.4cm

<span>lYYYYYY</span> Qtr-Year & t & y~t~ & ~t~ & b~t~ & s~t~ & ~t~\
2004 Q1 & -3 & - & - & - & 10.7 & -\
2004 Q2 & -2 & - & - & - & -9.5 & -\
2004 Q3 & -1 & - & - & - & -2.6 & -\
2004 Q4 & 0 & & 33.8 & 0.65 & 1.4 &\
2005 Q1 & 1 & 41.7 & 34.4 & 0.57 & 10.7 & 45.1\
2005 Q2 & 2 & 24.0 & 34.9 & 0.53 & -9.5 & 25.5\
2005 Q3 & 3 & 32.3 & 35.4 & 0.52 & -2.6 & 32.9\
2005 Q4 & 4 & 37.3 & 35.9 & 0.52 & 1.4 & 37.3\
2006 Q1 & 5 & 46.2 & 36.4 & 0.50 & 10.7 & 47.1\
2006 Q2 & 6 & 29.3 & 37.0 & 0.54 & -9.5 & 27.5\
2006 Q3 & 7 & 36.5 & 37.6 & 0.58 & -2.6 & 35.0\
2006 Q4 & 8 & 43.0 & 38.2 & 0.66 & 1.4 & 39.5\
2007 Q1 & 9 & 48.9 & 38.9 & 0.64 & 10.7 & 49.6\
2007 Q2 & 10 & 31.2 & 39.6 & 0.67 & -9.5 & 30.0\
2007 Q3 & 11 & 37.7 & 40.2 & 0.67 & -2.6 & 37.7\
2007 Q4 & 12 & 40.4 & 40.9 & 0.63 & 1.4 & 42.3\
2008 Q1 & 13 & 51.2 & 41.5 & 0.61 & 10.7 & 52.1\
2008 Q2 & 14 & 31.9 & 42.0 & 0.59 & -9.5 & 32.6\
2008 Q3 & 15 & 41.0 & 42.7 & 0.61 & -2.6 & 40.1\
2008 Q4 & 16 & 43.8 & 43.2 & 0.59 & 1.4 & 44.7\
2009 Q1 & 17 & 55.6 & 43.9 & 0.62 & 10.7 & 54.5\
2009 Q2 & 18 & 33.9 & 44.4 & 0.59 & -9.5 & 35.0\
2009 Q3 & 19 & 42.1 & 45.0 & 0.58 & -2.6 & 42.5\
2009 Q4 & 20 & 45.6 & 45.6 & 0.55 & 1.4 & 47.0\
2010 Q1 & 21 & 59.8 & 46.2 & 0.62 & 10.7 & 56.8\
2010 Q2 & 22 & 35.2 & 46.8 & 0.57 & -9.5 & 37.3\
2010 Q3 & 23 & 44.3 & 47.3 & 0.56 & -2.6 & 44.8\
2010 Q4 & 24 & 47.9 & 47.8 & 0.53 & 1.4 & 49.3\
[0.2cm] & h & & & & & y~T+h|T~\
2011 Q1 & 1 & & & & & 59.0\
2011 Q2 & 2 & & & & & 39.4\
2011 Q3 & 3 & & & & & 46.9\
2011 Q4 & 4 & & & & & 51.3\
2012 Q1 & 5 & & & & & 61.1\
2012 Q2 & 6 & & & & & 41.5\
2012 Q3 & 7 & & & & & 49.0\
2012 Q4 & 8 & & & & & 53.4\

The smoothing parameters and initial estimates for the components have
been estimated by minimizing SSE ($\alpha=0.025$, $\beta^*=0.023$,
$\gamma=0$ and SSE$=60.27$, RMSE$=1.585$).

\@ref(tbl-7-HWmulti) =0.4cm

<span>lYYYYYY</span> Qtr-Year & t & y~t~ & ~t~ & b~t~ & s~t~ & ~t~\
2004 Q1 & -3 & - & - & - & 1.3 & -\
2004 Q2 & -2 & - & - & - & 0.8 & -\
2004 Q3 & -1 & - & - & - & 0.9 & -\
2004 Q4 & 0 & & 32.2 & 0.93 & 1.0 &\
2005 Q1 & 1 & 41.7 & 33.1 & 0.93 & 1.3 & 41.8\
2005 Q2 & 2 & 24.0 & 32.9 & 0.78 & 0.8 & 25.9\
2005 Q3 & 3 & 32.3 & 33.9 & 0.81 & 0.9 & 31.9\
2005 Q4 & 4 & 37.3 & 35.4 & 0.91 & 1.0 & 35.7\
2006 Q1 & 5 & 46.2 & 36.4 & 0.92 & 1.3 & 45.9\
2006 Q2 & 6 & 29.3 & 37.9 & 1.00 & 0.8 & 28.4\
2006 Q3 & 7 & 36.5 & 38.7 & 0.98 & 0.9 & 36.8\
2006 Q4 & 8 & 43.0 & 40.6 & 1.11 & 1.0 & 40.8\
2007 Q1 & 9 & 48.9 & 40.4 & 0.92 & 1.3 & 52.7\
2007 Q2 & 10 & 31.2 & 41.2 & 0.90 & 0.8 & 31.5\
2007 Q3 & 11 & 37.7 & 41.1 & 0.76 & 0.9 & 39.8\
2007 Q4 & 12 & 40.4 & 40.8 & 0.60 & 1.0 & 43.0\
2008 Q1 & 13 & 51.2 & 41.0 & 0.54 & 1.3 & 52.3\
2008 Q2 & 14 & 31.9 & 41.7 & 0.57 & 0.8 & 31.6\
2008 Q3 & 15 & 41.0 & 42.7 & 0.63 & 0.9 & 40.0\
2008 Q4 & 16 & 43.8 & 43.0 & 0.58 & 1.0 & 44.6\
2009 Q1 & 17 & 55.6 & 43.7 & 0.61 & 1.3 & 55.1\
2009 Q2 & 18 & 33.9 & 44.4 & 0.61 & 0.8 & 33.8\
2009 Q3 & 19 & 42.1 & 44.8 & 0.58 & 0.9 & 42.6\
2009 Q4 & 20 & 45.6 & 44.9 & 0.52 & 1.0 & 46.6\
2010 Q1 & 21 & 59.8 & 46.2 & 0.63 & 1.3 & 57.5\
2010 Q2 & 22 & 35.2 & 46.6 & 0.59 & 0.8 & 35.7\
2010 Q3 & 23 & 44.3 & 47.0 & 0.57 & 0.9 & 44.6\
2010 Q4 & 24 & 47.9 & 47.2 & 0.51 & 1.0 & 48.9\
[0.2cm] & h & & & & & y~T+h|T~\
2011 Q1 & 1 & & & & & 60.3\
2011 Q2 & 2 & & & & & 36.7\
2011 Q3 & 3 & & & & & 46.1\
2011 Q4 & 4 & & & & & 50.6\
2012 Q1 & 5 & & & & & 62.8\
2012 Q2 & 6 & & & & & 38.2\
2012 Q3 & 7 & & & & & 48.0\
2012 Q4 & 8 & & & & & 52.7\

The smoothing parameters and initial estimates for the components have
been estimated by minimizing SSE ($\alpha=0$, $\beta^*=0$ and $\gamma=0$
and SSE$=34.6$, RMSE$=1.201$).

![Estimated components for Holt-Winters method with additive and
multiplicative seasonal components.](fig_7_LevelTrendSeas.pdf)

\@ref(fig-7-LevelTrendSeas)

states \<- cbind(fit1$model$states[,1:3],fit2$model$states[,1:3])
colnames(states) \<-
c(“level”,“slope”,“seasonal”,“level”,“slope”,“seasonal”) plot(states,
xlab=“Year”) fit1$model$state[,1:3] fitted(fit1) fit1[[“mean”]]

### Holt {-}-Winters damped method

A method that is often the single most accurate forecasting method for
seasonal data is the Holt-Winters method with a damped trend and
multiplicative seasonality:

$$\begin{align*}
\pred{y}{t+h}{t} &= [\ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t}]s_{t-m+\h+}. \\
\ell_{t} &= \alpha(y_{t} / s_{t-m}) + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1}             \\
s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + \phi b_{t-1})} + (1 - \gamma)s_{t-m}\end{align*}$$

hw(x, damped=TRUE, seasonal=“multiplicative”)

##A taxonomy of exponential smoothing methods {#sec-7-6-Taxonomy}

Exponential smoothing methods are not restricted to those we have
presented so far. By considering variations in the combination of the
trend and seasonal components, fifteen exponential smoothing methods are
possible, listed in Table \@ref(tbl-7-classification). Each method is
labelled by a pair of letters (T,S) defining the type of ‘Trend’ and
‘Seasonal’ components. For example, (A,M) is the method with an additive
trend and multiplicative seasonality; (M,N) is the method with
multiplicative trend and no seasonality; and so on.

\@ref(tbl-7-classification)=.09cm

<span>@llccc@</span>& &\
& N & A & M\
 &  (None)  & (Additive) & (Multiplicative)\
 &&&&\
N & (None) & (N,N) & (N,A) & (N,M)\
&&&&\
A & (Additive) & (A,N) & (A,A) & (A,M)\
&&&&\
A& (Additive damped) & (A,N) & (A,A) & (A,M)\

Some of these methods we have already seen:

<span>c@<span> = </span>l</span> (N,N) & simple exponential smoothing\
(A,N) & Holt’s linear method\
(A,N) & additive damped trend method\
(A,A) & additive Holt-Winters method\
(A,M) & multiplicative Holt-Winters method\
(A,M) & Holt-Winters damped method\

[!t]

=0.1cm

<span>clll</span> &\
& & &\
 & $\hat{y}_{t+h|t} = \ell_t$ &
$\hat{y}_{t+h|t} = \ell_t + s_{t-m+h_m^+}$ &
$\hat{y}_{t+h|t}= \ell_ts_{t-m+h_m^+}$\
<span>**N**</span> & $\ell_t = \alpha y_t + (1-\alpha) \ell_{t-1}$ &
$\ell_t = \alpha (y_t - s_{t-m}) + (1-\alpha) \ell_{t-1}$ &
$\ell_t = \alpha (y_t / s_{t-m}) + (1-\alpha) \ell_{t-1}$\
& & $s_t = \gamma (y_t - \ell_{t-1}) + (1-\gamma) s_{t-m}$ &
$s_t = \gamma (y_t / \ell_{t-1}) + (1-\gamma) s_{t-m}$\
& $\hat{y}_{t+h|t} = \ell_t+hb_t$ &
$\hat{y}_{t+h|t} = \ell_t +hb_t +s_{t-m+h_m^+}$ &
$\hat{y}_{t+h|t}= (\ell_t+hb_t)s_{t-m+h_m^+}$\
<span>**A**</span> &
$\ell_t = \alpha y_t + (1-\alpha) (\ell_{t-1}+b_{t-1})$ &
$\ell_t = \alpha (y_t - s_{t-m}) + (1-\alpha)(\ell_{t-1}+b_{t-1})$ &
$\ell_t = \alpha (y_t / s_{t-m}) + (1-\alpha)(\ell_{t-1}+b_{t-1})$\

& $b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) b_{t-1}$ &
$b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) b_{t-1}$ &
$b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) b_{t-1}$\
& & $s_t = \gamma (y_t - \ell_{t-1}-b_{t-1}) + (1-\gamma)s_{t-m}$ &
$s_t = \gamma (y_t / (\ell_{t-1}+b_{t-1})) + (1-\gamma)s_{t-m}$\
& $\hat{y}_{t+h|t} = \ell_t+\dampfactor b_t$ &
$\hat{y}_{t+h|t} = \ell_t+\dampfactor b_t+s_{t-m+h_m^+}$ &
$\hat{y}_{t+h|t}= (\ell_t+\dampfactor b_t)s_{t-m+h_m^+}$\
<span>**A**</span> &
$\ell_t = \alpha y_t + (1-\alpha) (\ell_{t-1}+\phi b_{t-1})$ &
$\ell_t = \alpha (y_t - s_{t-m}) + (1-\alpha) (\ell_{t-1}+\phi b_{t-1})$
&
$\ell_t = \alpha (y_t / s_{t-m}) + (1-\alpha) (\ell_{t-1}+\phi b_{t-1})$\
& $b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) \phi b_{t-1}$ &
$b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) \phi b_{t-1}$ &
$b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) \phi b_{t-1}$\
& & $s_t = \gamma (y_t - \ell_{t-1}-\phi b_{t-1}) + (1-\gamma)s_{t-m}$ &
$s_t = \gamma (y_t / (\ell_{t-1}+\phi b_{t-1})) + (1-\gamma)s_{t-m}$\

Table : Formulae for recursive calculations and point forecasts. In each
case, $\ell_t$ denotes the series level at time $t$, $b_t$ denotes the
slope at time $t$, $s_t$ denotes the seasonal component of the series at
time $t$, and $m$ denotes the number of seasons in a year; $\alpha$,
$\beta^*$, $\gamma$ and $\phi$ are smoothing parameters,
$\phi_h = \phi+\phi^2+\dots+\phi^{h}$ and
$h_m^+ = \lfloor(h-1) \text{~mod~} m\rfloor + 1$.

\@ref(tbl-7-pegels)

This type of classification was proposed by Pegels (1969). It was later
extended by Gardner (1985) to include methods with additive damped trend
and by Taylor (2003) to include methods with multiplicative damped
trend. We do not consider the multiplicative trend methods in this book
as they tend to produce poor forecasts. See Hyndman et al (2008) for a
more thorough discussion of all exponential smoothing methods.

Table \@ref(tbl-7-pegels) gives the recursive formulae for applying all nine
possible exponential smoothing methods. Each cell includes the forecast
equation for generating $h$-step-ahead forecasts and the smoothing
equations for applying the method.

##Innovations state space models for exponential smoothing {#sec-7-ETS}

In the rest of this chapter we study statistical models that underlie
the exponential smoothing methods we have considered so far. The
exponential smoothing methods presented in Table \@ref(tbl-7-pegels) are
algorithms that generate point forecasts. The statistical models in this
section generate the same point forecasts, but can also generate
prediction (or forecast) intervals. A statistical model is a stochastic
(or random) data generating process that can produce an entire forecast
distribution. The general statistical framework we will introduce also
provides a platform for using the model selection criteria introduced in
Chapter \@ref(ch5), thus allowing the choice of model to be made in an
objective manner.

Each model consists of a measurement equation that describes the
observed data and some transition equations that describe how the
unobserved components or states (level, trend, seasonal) change over
time. Hence these are referred to as “state space models”.

For each method there exist two models: one with additive errors and one
with multiplicative errors. The point forecasts produced by the models
are identical if they use the same smoothing parameter values. They
will, however, generate different prediction intervals.

To distinguish between a model with additive errors and one with
multiplicative errors (and to also distinguish the models from the
methods) we add a third letter to the classification of
Table \@ref(tbl-7-classification). We label each state space model as
ETS($\cdot,\cdot,\cdot$) for (Error, Trend, Seasonal). This label can
also be thought of as ExponenTial Smoothing. Using the same notation as
in Table \@ref(tbl-7-classification), the possibilities for each component
are: Error $=\{$A,M$\}$, Trend $=\{$N,A,A$\}$ and Seasonal
$=\{$N,A,M$\}$. Therefore, in total there exist 18 such state space
models: 9 with additive errors and 9 with multiplicative errors.

### ETS {-}(A,N,N): simple exponential smoothing with additive errors

The third form of simple exponential smoothing is obtained by
re-arranging the level equation in the component form to get what we
refer to as the error correction form
\begin{align*}
\ell_{t} %&= \alpha y_{t}+\ell_{t-1}-\alpha\ell_{t-1}\\
         &= \ell_{t-1}+\alpha( y_{t}-\ell_{t-1})\\
         &= \ell_{t-1}+\alpha e_{t},
\end{align*}
where $e_{t}=y_{t}-\ell_{t-1}=y_{t}-\pred{y}{t}{t-1}$ for $t=1,\dots,T$.
That is, $e_{t}$ is the one-step error at time $t$ computed on the
training data. The training data errors lead to the
adjustment/correction of the estimated level throughout the smoothing
process for $t=1,\dots,T$.

For example, if the error at time $t$ is negative, then
$\pred{y}{t}{t-1}>y_t$ and so the level at time $t-1$ has been
over-estimated. The new level $\ell_t$ is then the previous level
$\ell_{t-1}$ adjusted downwards. The closer $\alpha$ is to one the
“rougher” the estimate of the level (large adjustments take place). The
smaller the $\alpha$ the “smoother” the level (small adjustments take
place).

As discussed in Section \@ref(sec-7-1-SES), the error correction form of
simple exponential smoothing is given by

$$\ell_t=\ell_{t-1}+\alpha e_t,$$

where $e_t = y_t - \ell_{t-1}$ and $\pred{y}{t}{t-1} = \ell_{t-1}$.
Thus, $e_t = y_t - \pred{y}{t}{t-1}$ represents a one-step forecast
error and we can write $y_t = \ell_{t-1} + e_t$.

To make this into an innovations state space model, all we need to do is
specify the probability distribution for $e_t$. For a model with
additive errors, we assume that one-step forecast errors $e_t$ are
normally distributed white noise with mean 0 and variance $\sigma^2$. A
short-hand notation for this is
$e_t = \varepsilon_t\sim\text{NID}(0,\sigma^2)$; NID stands for
“normally and independently distributed”.

Then the equations of the model can be written

$$\begin{align*}
y_t &= \ell_{t-1} + \varepsilon_t \label{eq-ann-1a}\\
\ell_t&=\ell_{t-1}+\alpha \varepsilon_t. \label{eq-ann-2a}\end{align*}$$

We refer to as the *measurement* (or observation) equation and as the
*state* (or transition) equation. These two equations, together with the
statistical distribution of the errors, form a fully specified
statistical model. Specifically, these constitute an innovations state
space model underlying simple exponential smoothing.

The term “innovations” comes from the fact that all equations in this
type of specification use the same random error process,
$\varepsilon_t$. For the same reason this formulation is also referred
to as a “single source of error” model in contrast to alternative
multiple source of error formulations, which we do not present here.

The measurement equation shows the relationship between the observations
and the unobserved states. In this case observation $y_t$ is a linear
function of the level $\ell_{t-1}$, the predictable part of $y_t$, and
the random error $\varepsilon_t$, the unpredictable part of $y_t$. For
other innovations state space models, this relationship may be
nonlinear.

The transition equation shows the evolution of the state through time.
The influence of the smoothing parameter $\alpha$ is the same as for the
methods discussed earlier. For example $\alpha$ governs the degree of
change in successive levels. The higher the value of $\alpha$, the more
rapid the changes in the level; the lower the value of $\alpha$, the
smoother the changes. At the lowest extreme, where $\alpha=0$, the level
of the series does not change over time. At the other extreme, where
$\alpha=1$, the model reduces to a random walk model,
$y_t=y_{t-1}+\varepsilon_t$.

### ETS {-}(M,N,N): simple exponential smoothing with multiplicative errors

In a similar fashion, we can specify models with multiplicative errors
by writing the one-step random errors as relative errors:

$$\begin{align*}
\varepsilon_t&=\frac{y_t-\pred{y}{t}{t-1}}{\pred{y}{t}{t-1}}\end{align*}$$

where $\varepsilon_t \sim \text{NID}(0,\sigma^2)$. Substituting
$\pred{y}{t}{t-1}=\ell_{t-1}$ gives
$y_t = \ell_{t-1}+\ell_{t-1}\varepsilon_t$ and
$e_t = y_t - \pred{y}{t}{t-1} = \ell_{t-1}\varepsilon_t$.

Then we can write the multiplicative form of the state space model as

$$\begin{align*}
y_t&=\ell_{t-1}(1+\varepsilon_t)\\
\ell_t&=\ell_{t-1}(1+\alpha \varepsilon_t).\end{align*}$$

### ETS {-}(A,A,N): Holt’s linear method with additive errors {#sec-7-AAN}

For this model, we assume that one-step forecast errors are given by
$\varepsilon_t=y_t-\ell_{t-1}-b_{t-1} \sim \text{NID}(0,\sigma^2)$.
Substituting this into the error correction equations for Holt’s linear
method we obtain

$$\begin{align*}
y_t&=\ell_{t-1}+b_{t-1}+\varepsilon_t\\
\ell_t&=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
b_t&=b_{t-1}+\beta \varepsilon_t,\end{align*}$$

where for simplicity we have set $\beta=\alpha \beta^*$.

### ETS {-}(M,A,N): Holt’s linear method with multiplicative errors

Specifying one-step forecast errors as relative errors such that
$$\varepsilon_t=\frac{y_t-(\ell_{t-1}+b_{t-1})}{(\ell_{t-1}+b_{t-1})}$$
and following a similar approach as above, the innovations state space
model underlying Holt’s linear method with multiplicative errors is
specified as

$$\begin{align*}
y_t&=(\ell_{t-1}+b_{t-1})(1+\varepsilon_t)\\
\ell_t&=(\ell_{t-1}+b_{t-1})(1+\alpha \varepsilon_t)\\
b_t&=b_{t-1}+\beta(\ell_{t-1}+b_{t-1}) \varepsilon_t\end{align*}$$

where again $\beta=\alpha \beta^*$ and
$\varepsilon_t \sim \text{NID}(0,\sigma^2)$.

### Other ETS models {-}

In a similar fashion, we can write an innovations state space model for
each of the exponential smoothing methods of Table \@ref(tbl-7-pegels).
Table [table:2a] presents the equations for all of the models in the ETS
framework.

[!p]

=0.1cm

<span>clll</span>\
**Trend &\
& & &\
 <span>**N**</span> & $y_{t} = \ell_{t-1}+\varepsilon_t$ &
$y_{t} = \ell_{t-1} + s_{t-m}+\varepsilon_t$ &
$y_{t} = \ell_{t-1} s_{t-m}+\varepsilon_t$\
& $\ell_t = \ell_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + \alpha  \varepsilon_t/s_{t-m}$\
&&$s_t = s_{t-m} + \gamma  \varepsilon_t$ &
$s_t = s_{t-m} + \gamma  \varepsilon_t/\ell_{t-1}$\
& $y_{t} = \ell_{t-1} + b_{t-1}+\varepsilon_t$ &
$y_{t} = \ell_{t-1} + b_{t-1} + s_{ t-m}+\varepsilon_t$ &
$y_{t} = (\ell_{t-1}+b_{t-1})s_{t-m}+\varepsilon_t$\
<span>**A**</span> &
$\ell_t = \ell_{t-1} + b_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + b_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + b_{t-1} + \alpha
    \varepsilon_t/s_{t-m}$\
& $b_t = b_{t-1} + \beta  \varepsilon_t$ &
$b_t = b_{t-1} + \beta  \varepsilon_t$ &
$b_t = b_{t-1} + \beta  \varepsilon_t/s_{t-m}$\
& & $s_t =  s_{t-m} + \gamma  \varepsilon_t$ &
$s_t =  s_{t-m} + \gamma  \varepsilon_t/(\ell_{t-1}+b_{t-1})$\
& $y_{t} =  \ell_{t-1} + \phi b_{t-1}+\varepsilon_t$ &
$y_{t} =  \ell_{t-1} + \phi b_{t-1} + s_{t-m}+\varepsilon_t$ &
$y_{t} = (\ell_{t-1} + \phi b_{t-1})s_{t-m}+\varepsilon_t$\
<span>**A**</span> &
$\ell_t = \ell_{t-1} + \phi b_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + \phi b_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + \phi b_{t-1} + \alpha
    \varepsilon_t/s_{t-m}$\
& $b_t = \phi b_{t-1} + \beta  \varepsilon_t$ &
$b_t = \phi b_{t-1} + \beta  \varepsilon_t$ &
$b_t = \phi b_{t-1} + \beta  \varepsilon_t/s_{t-m}$\
& & $s_t =  s_{t-m} + \gamma  \varepsilon_t$ &
$s_t =  s_{t-m} + \gamma  \varepsilon_t/(\ell_{t-1}+\phi
    b_{t-1})$\
\
\
**Trend &\
& & &\
 <span>**N**</span> & $y_{t} = \ell_{t-1}(1 + \varepsilon_t)$ &
$y_{t} = (\ell_{t-1} + s_{t-m})(1 + \varepsilon_t)$ &
$y_{t} = \ell_{t-1} s_{t-m}(1 + \varepsilon_t)$\
& $\ell_t = \ell_{t-1}(1 + \alpha \varepsilon_t)$ &
$\ell_t = \ell_{t-1} + \alpha(\ell_{t-1} + s_{t-m})
    \varepsilon_t$ & $\ell_t = \ell_{t-1}(1 + \alpha \varepsilon_t)$\
&&$s_t = s_{t-m} + \gamma  (\ell_{t-1} + s_{t-m})\varepsilon_t$ &
$s_t = s_{t-m}(1 + \gamma  \varepsilon_t)$\
& $y_{t} = (\ell_{t-1} + b_{t-1})(1 + \varepsilon_t)$ &
$y_{t} = (\ell_{t-1} + b_{t-1} + s_{ t-m})(1 + \varepsilon_t)$ &
$y_{t} = (\ell_{t-1}+b_{t-1})s_{t-m}(1 + \varepsilon_t)$\
<span>**A**</span> &
$\ell_t = (\ell_{t-1} + b_{t-1})(1 + \alpha  \varepsilon_t)$ &
$\ell_t = \ell_{t-1} + b_{t-1} + \alpha (\ell_{t-1} + b_{t-1}+ s_{ t-m}) \varepsilon_t$
& $\ell_t = (\ell_{t-1} + b_{t-1})(1 + \alpha\varepsilon_t)$\
& $b_t = b_{t-1} + \beta (\ell_{t-1} + b_{t-1}) \varepsilon_t$ &
$b_t = b_{t-1} + \beta (\ell_{t-1} + b_{t-1} + s_{ t-m})\varepsilon_t$ &
$b_t = b_{t-1} + \beta (\ell_{t-1}+b_{t-1})\varepsilon_t$\
& &
$s_t =  s_{t-m} + \gamma (\ell_{t-1} + b_{t-1} + s_{ t-m})\varepsilon_t$
& $s_t =  s_{t-m}(1 + \gamma \varepsilon_t)$\
& $y_{t} =  (\ell_{t-1} + \phi b_{t-1})(1 + \varepsilon_t)$ &
$y_{t} =  (\ell_{t-1} + \phi b_{t-1} + s_{t-m})(1 + \varepsilon_t)$ &
$y_{t} = (\ell_{t-1} + \phi b_{t-1})s_{t-m}(1 + \varepsilon_t)$\
<span>**A**</span> &
$\ell_t = (\ell_{t-1} + \phi b_{t-1})(1 + \alpha \varepsilon_t)$ &
$\ell_t = \ell_{t-1} + \phi b_{t-1} + \alpha (\ell_{t-1}+ \phi b_{t-1} + s_{t-m}) \varepsilon_t$
& $\ell_t = (\ell_{t-1} + \phi b_{t-1})(1 + \alpha \varepsilon_t)$\
& $b_t = \phi b_{t-1} + \beta(\ell_{t-1} + \phi b_{t-1})\varepsilon_t$ &
$b_t = \phi b_{t-1} + \beta  (\ell_{t-1} + \phi b_{t-1} +s_{t-m})\varepsilon_t$
& $b_t = \phi b_{t-1} + \beta (\ell_{t-1} + \phi b_{t-1})\varepsilon_t$\
& &
$s_t =  s_{t-m} + \gamma (\ell_{t-1} + \phi b_{t-1} + s_{t-m})\varepsilon_t$
& $s_t =  s_{t-m}(1+ \gamma \varepsilon_t)$\
****

Table : <span>State space equations for each of the models in the ETS
framework.</span>

[table:2a]

### Estimating ETS models {-}

An alternative to estimating the parameters by minimizing the sum of
squared errors, is to maximize the “likelihood”. The likelihood is the
probability of the data arising from the specified model. So a large
likelihood is associated with a good model. For an additive error model,
maximizing the likelihood gives the same results as minimizing the sum
of squared errors. However, different results will be obtained for
multiplicative error models. In this section, we will estimate the
smoothing parameters $\alpha$, $\beta$, $\gamma$ and $\phi$, and the
initial states $\ell_0$, $b_0$, $s_0,s_{-1},\dots,s_{-m+1}$, by
maximizing the likelihood.

The possible values that the smoothing parameters can take is
restricted. Traditionally the parameters have been constrained to lie
between 0 and 1 so that the equations can be interpreted as weighted
averages. That is, $0< \alpha,\beta^*,\gamma^*,\phi<1$. For the state
space models, we have set $\beta=\alpha\beta^*$ and
$\gamma=(1-\alpha)\gamma^*$. Therefore the traditional restrictions
translate to $0< \alpha <1$,  $0 < \beta < \alpha$  and
$0< \gamma < 1-\alpha$. In practice, the damping parameter $\phi$ is
usually constrained further to prevent numerical difficulties in
estimating the model. A common constraint is to set $0.8<\phi<0.98$.

Another way to view the parameters is through a consideration of the
mathematical properties of the state space models. Then the parameters
are constrained to prevent observations in the distant past having a
continuing effect on current forecasts. This leads to some
*admissibility* constraints on the parameters which are usually (but not
always) less restrictive than the usual region. For example for the
ETS(A,N,N) model, the usual parameter region is $0< \alpha <1$ but the
admissible region is $0< \alpha <2$. For the ETS(A,A,N) model, the usual
parameter region is $0<\alpha<1$ and $0<\beta<\alpha$ but the admissible
region is $0<\alpha<2$ and $0<\beta<4-2\alpha$.

### Model selection {-}

A great advantage of the ETS statistical framework is that information
criteria can be used for model selection. The AIC, AIC$_{\text{c}}$ and
BIC, introduced in Section \@ref(sec-5-3-SelectingPredictors), can be used
here to determine which of the 30 ETS models is most appropriate for a
given time series.

For ETS models, Akaike’s Information Criterion (AIC) is defined as
$$\text{AIC} = -2\log(L) + 2k,$$ where $L$ is the likelihood of the
model and $k$ is the total number of parameters and initial states that
have been estimated.

The AIC corrected for small sample bias (AIC$_\text{c}$) is defined as
$$\text{AIC}_{\text{c}} = \text{AIC} + \frac{2(k+1)(k+2)}{T-k},$$ and
the Bayesian Information Criterion (BIC) is
$$\text{BIC} = \text{AIC} + k[\log(T)-2].$$

Three of the combinations of (Error, Trend, Seasonal) can lead to
numerical difficulties. Specifically, the models that can cause such
instabilities are ETS(A,N,M), ETS(A,A,M), and ETS(A,A,M). We normally do
not consider these particular combinations when selecting a model.

Models with multiplicative errors are useful when the data are strictly
positive, but are not numerically stable when the data contain zeros or
negative values. Therefore multiplicative errors models will not be
considered if the time series is not strictly positive. In that case
only the six fully additive models will be applied.

### The  {-}`ets()` function in R {#the-ets-function-in-r .unnumbered}

The models can be estimated in R using the `ets()` function in the
forecast package. The R code below shows all the possible arguments this
function takes, and their default values. If only the time series is
specified, and all other arguments are left at their default values,
then an appropriate model will be selected automatically. We explain
each of the arguments below.

ets(y, model=“ZZZ”, damped=NULL, alpha=NULL, beta=NULL, gamma=NULL,
phi=NULL, additive.only=FALSE, lambda=NULL, lower=c(rep(0.0001,3), 0.8),
upper=c(rep(0.9999,3),0.98),
opt.crit=c(“lik”,“amse”,“mse”,“sigma”,“mae”), nmse=3,
bounds=c(“both”,“usual”,“admissible”), ic=c(“aicc”,“aic”,“bic”),
restrict=TRUE)

y
:   The time series to be forecast.

model
:   A three-letter code indicating the model to be estimated using the
    ETS classification and notation. The possible inputs are “N” for
    none, “A” for additive, “M” for multiplicative, or “Z” for automatic
    selection. If any of the inputs is left as “Z” then this component
    is selected according to the information criterion chosen. The
    default value of `ZZZ` ensures that all components are selected
    using the information criterion.

damped
:   If `damped=TRUE`, then a damped trend will be used (either A or M).
    If `damped=FALSE`, then a non-damped trend will used. If
    `damped=NULL` (the default), then either a damped or a non-damped
    trend will be selected according to the information criterion
    chosen.

alpha, beta, gamma, phi
:   The values of the smoothing parameters can be specified using these
    arguments. If they are set to `NULL` (the default setting for each
    of them), the parameters are estimated.

additive.only
:   Only models with additive components will be considered if
    `additive.only=TRUE`. Otherwise all models will be

lambda
:   Box-Cox transformation parameter. It will be ignored if
    `lambda=NULL` (the default value). Otherwise, the time series will
    be transformed before the model is estimated. When `lambda` is not
    `NULL`, `additive.only` is set to `TRUE`.

lower, upper
:   Lower and upper bounds for the parameter estimates $\alpha$,
    $\beta^*$, $\gamma^*$ and $\phi$.

opt.crit
:   The optimization criterion to be used for estimation. The default
    setting is maximum likelihood estimation, used when `opt.crit=lik`.

bounds
:   This specifies the constraints to be used on the parameters. The
    traditional constraints are set using `bounds="usual"` and the
    admissible constraints are set using `bounds="admissible"`. The
    default (`bounds="both"`) requires the parameters to satisfy both
    sets of constraints.

ic
:   The information criterion to be used in selecting models, set by
    default to `aicc`.

restrict
:   If `restrict=TRUE` (the default), the models that cause numerical
    difficulties are not considered in model selection.

### Forecasting with ETS models {-}

Point forecasts are obtained from the models by iterating the equations
for $t=T+1,\dots,T+h$ and setting all $\varepsilon_t=0$ for $t>T$.

For example, for model ETS(M,A,N),
$y_{T+1} = (\ell_T + b_T )(1+ \varepsilon_{T+1}).$ Therefore
$\pred{y}{T+1}{T}=\ell_{T}+b_{T}.$ Similarly,

$$\begin{align*}
y_{T+2} &= (\ell_{T+1} + b_{T+1})(1 + \varepsilon_{T+1})\\
  &= \left[(\ell_T + b_T)(1+ \alpha\varepsilon_{T+1}) + b_T + \beta (\ell_T + b_T)\varepsilon_{T+1}\right] ( 1 + \varepsilon_{T+1}).\end{align*}$$

Therefore, $\pred{y}{T+2}{T}= \ell_{T}+2b_{T},$ and so on. These
forecasts are identical to the forecasts from Holt’s linear method and
also those from model ETS(A,A,N). So the point forecasts obtained from
the method and from the two models that underlie the method are
identical (assuming the same parameter values are used).

A big advantage of the models is that prediction intervals can also be
generated --- something that cannot be done using the methods. The
prediction intervals will differ between models with additive and
multiplicative methods.

For some models, there are exact formulae that enable prediction
intervals to be calculated. A more general approach that works for all
models is to simulate future sample paths, conditional on the last
estimate of the states, and to obtain prediction intervals from the
percentiles of these simulated future paths. These options are available
in R using the `forecast` function in the forecast package. The R code
below shows the all the possible arguments this function takes when
applied to an ETS model. We explain each the arguments in what follows.

forecast(object, h=ifelse(object[[“m”]]\>1, 2\*object[[“m”]], 10),
level=c(80,95), fan=FALSE, simulate=FALSE, bootstrap=FALSE, npaths=5000,
PI=TRUE, lambda=object[[“lambda”]], ...)

object
:   The object returned by the `ets()` function.

h
:   The forecast horizon --- the number of periods to be forecast.

level
:   The confidence level for the prediction intervals.

fan
:   If `fan=TRUE`, `level=seq(50,99,by=1)`. This is suitable for fan
    plots.

simulate
:   If `simulate=TRUE`, prediction intervals are produced by simulation
    rather than using algebraic formulae. Simulation will also be used
    (even if `simulate=FALSE`) where there are no algebraic formulae
    available for the particular model.

bootstrap
:   If `bootstrap=TRUE` and `simulate=TRUE`, then the simulated
    prediction intervals use re-sampled errors rather than normally
    distributed errors.

npaths
:   The number of sample paths used in computing simulated prediction
    intervals.

PI
:   If `PI=TRUE`, then prediction intervals are produced; otherwise only
    point forecasts are calculated. If `PI=FALSE`, then `level`, `fan`,
    `simulate`, `bootstrap` and `npaths` are all ignored.

lambda
:   The Box-Cox transformation parameter. This is ignored if
    `lambda=NULL`. Otherwise, forecasts are back-transformed via an
    inverse Box-Cox transformation.

### Example 7 {-}.1    Oil production example (revisited)

Figure \@ref(fig-7-Oil) shows the point forecasts and prediction intervals
from an estimated ETS(A,N,N) model. The estimates for the smoothing
parameter $\alpha$ and the initial level $\ell_0$ are 0.89 and 447.49
respectively, identical to the estimates for the simple exponential
smoothing method estimated earlier (see beginning of
Example \@ref(ex-7-SES)). The plot shows the importance of generating
prediction intervals. The intervals here are relatively wide, so
interpreting the point forecasts without accounting for the large
uncertainty can be very misleading.

![Point forecasts and 80% and 95% prediction intervals from model
ETS(A,N,N).](fig_7_Oil)

\@ref(fig-7-Oil)

oildata \<- window(oil,start=1996,end=2007) fit \<- ets(oildata,
model=“ANN”) plot(forecast(fit, h=3), ylab=“Oil (millions of tones)”)
fit[[“par”]]

alpha l 0.892 447.489

### Example 7 {-}.4    International tourist visitor nights in Australia 

We now employ the ETS statistical framework to forecast tourists visitor
nights in Australia by international arrivals over the period 2011--2012.
We let the `ets()` function select the model by minimizing the AICc.

vndata \<- window(austourists, start=2005) fit \<- ets(vndata)
summary(fit)

ETS(M,A,M)

Call: ets(y = vndata)

Smoothing parameters: alpha = 0.4504 beta = 4e-04 gamma = 0.0046

Initial states: l = 32.4349 b = 0.6533 s=1.0275 0.9463 0.7613 1.2648

sigma: 0.0332

AIC AICc BIC 105.972 115.572 115.397

Training set error measures: ME RMSE MAE MPE MAPE MASE ACF1 Training set
-0.081646 1.33328 1.0647 -0.221243 2.65469 0.374361 -0.089119

The model selected is ETS(M,A,M):
\begin{align*}
y_{t} &= (\ell_{t-1} + b_{t-1})s_{t-m}(1 + \varepsilon_t)\\
\ell_t &= (\ell_{t-1} + b_{t-1})(1 + \alpha \varepsilon_t)\\
b_t &=b_{t-1} + \beta(\ell_{t-1} + b_{t_1})\varepsilon_t\\
s_t &=  s_{t-m}(1+ \gamma \varepsilon_t).
\end{align*}

The parameter estimates are $\alpha=0.4504$, $\beta=0.0004$, and
$\gamma=0.0046$. The output returns the estimates for the initial states
$\ell_0$, $b_0$, $s_{0}$, $s_{-1}$, $s_{-2}$ and $s_{-3}.$ The training
RMSE for this model is slightly lower than the Holt-Winter methods with
additive and multiplicative seasonality presented in
Tables \@ref(tbl-7-HWadd) and \@ref(tbl-7-HWmulti) respectively.
Figure \@ref(fig-7-MAdMstates) shows the states over time while
Figure \@ref(fig-7-MAdMforecasts) shows point forecasts and prediction
intervals generated from the model. The intervals are much narrower than
the prediction intervals in the oil production example.

![Graphical representation of the estimated states over
time.](fig_7_MAMstates)

\@ref(fig-7-MAdMstates)

plot(fit)

![Forecasting international visitor nights in Australia from an
ETS(M,A,M) model.](fig_7_MAMforecasts)

\@ref(fig-7-MAdMforecasts)

plot(forecast(fit,h=8), ylab=“International visitor night in Australia
(millions)”)

\@ref(ex1) Data set `books` contains the daily sales of paperback and
hardcover books at the same store. The task is to forecast the next four
days’ sales for paperback and hardcover books (data set `books`).

[(a)]

Plot the series and discuss the main features of the data.

\@ref(1b) Use simple exponential smoothing with the `ses` function (setting
`initial="simple"`) and explore different values of $\alpha$ for the
`paperback` series. Record the within-sample SSE for the one-step
forecasts. Plot SSE against $\alpha$ and find which value of $\alpha$
works best. What is the effect of $\alpha$ on the forecasts?

Now let `ses` select the optimal value of $\alpha$. Use this value to
generate forecasts for the next four days. Compare your results with
(\@ref(1b)).

Repeat but with `initial="optimal"`. How much difference does an optimal
initial level make?

Repeat steps (b)--(d) with the `hardcover` series.

\@ref(ex2)

[(a)]

Apply Holt’s linear method to the `paperback` and `hardback` series and
compute four-day forecasts in each case.

Compare the SSE measures of Holt’s method for the two series to those of
simple exponential smoothing in the previous question. Discuss the
merits of the two forecasting methods for these data sets.

Compare the forecasts for the two series using both methods. Which do
you think is best?

Calculate a 95% prediction interval for the first forecast for each
series using both methods, assuming normal errors. Compare your
forecasts with those produced by R.

For this exercise, use the price of a dozen eggs in the United States
from 1900--1993 (data set `eggs`). Experiment with the various options in
the `holt()` function to see how much the forecasts change with damped
or exponential trend. Also try changing the parameter values for
$\alpha$ and $\beta$ to see how they affect the forecasts. Try to
develop an intuition of what each parameter and argument is doing to the
forecasts.

[Hint: use `h=100` when calling `holt()` so you can clearly see the
differences between the various options when plotting the forecasts.]

Which model gives the best RMSE?

For this exercise, use the quarterly UK passenger vehicle production
data from 1977:1--2005:1 (data set `ukcars`).

[(a)]

Plot the data and describe the main features of the series.

Decompose the series using STL and obtain the seasonally adjusted data.

Forecast the next two years of the series using an additive damped trend
method applied to the seasonally adjusted data. Then reseasonalize the
forecasts. Record the parameters of the method and report the RMSE of
the one-step forecasts from your method.

Forecast the next two years of the series using Holt’s linear method
applied to the seasonally adjusted data. Then reseasonalize the
forecasts. Record the parameters of the method and report the RMSE of of
the one-step forecasts from your method.

Now use `ets()` to choose a seasonal model for the data.

Compare the RMSE of the fitted model with the RMSE of the model you
obtained using an STL decomposition with Holt’s method. Which gives the
better in-sample fits?

Compare the forecasts from the two approaches? Which seems most
reasonable?

For this exercise, use the monthly Australian short-term overseas
visitors data, May 1985--April 2005. (Data set: `visitors`.)

[(a)]

Make a time plot of your data and describe the main features of the
series.

Forecast the next two years using Holt-Winters’ multiplicative method.

Why is multiplicative seasonality necessary here?

Experiment with making the trend exponential and/or damped.

Compare the RMSE of the one-step forecasts from the various methods.
Which do you prefer?

Now fit each of the following models to the same data:

[(i)]

a multiplicative Holt-Winters’ method;

an ETS model;

an additive ETS model applied to a Box-Cox transformed series;

a seasonal naive method applied to the Box-Cox transformed series;

an STL decomposition applied to the Box-Cox transformed data followed by
an ETS model applied to the seasonally adjusted (transformed) data.

For each model, look at the residual diagnostics and compare the
forecasts for the next two years. Which do you prefer?

[^1]: In some books, it is called “single exponential smoothing”.

