# Linear regression models {#ch-regression}

In this chapter we discuss linear regression models. The basic concept is that we forecast the time series of interest ($y$) assuming it has a linear relationship with other time series ($x$) we observe.

For example, we might wish to  forecast monthly sales ($y$) with total advertising spend ($x$) as the predictor.  Or we might forecast daily electricity demand ($y$) using temperature ($x_1$) and the day of week ($x_2$) as predictors.

The forecast variable ($y$) is sometimes also called the regressand, dependent or explained variable. The predictor variables ($x$) are sometimes also called the regressors, independent or explanatory variables. In this book we will always refer to them as the "forecast variable" and "predictor variables".

## The linear model {#Regr-Intro}

### Simple linear regression {-}

$$  y_t = \beta_0 + \beta_1 x_t + \varepsilon_t. $$

An example of data from such a model is shown in Figure \@ref(fig:SLRpop1). The parameters $\beta_0$ and $\beta_1$ denote the intercept and the slope of the line respectively. The intercept $\beta_0$ represents the predicted value of $y$ when $x=0$. The slope $\beta_1$ represents the average predicted change in $y$ resulting from a one unit change in $x$.

```{r SLRpop1, fig.cap="An example of data from a linear regression model.", echo=FALSE, cache=TRUE, warning=FALSE, message=FALSE}
set.seed(2)
x <- runif(50, 0, 4)
df <- data.frame(x=x, 
                 y=3 + 10*x + rnorm(50, 0, 10))
ggplot(df, aes(x, y)) + 
  geom_point() +
  geom_abline(slope=10,intercept=3, col="#990000",
              size=0.3) +
  geom_label(x=.3,y=40,parse=TRUE, col="#990000",
             label=" beta[0] + beta[1] * x") +
  geom_segment(x=.3, y=36, xend=0, yend=4, 
        arrow=arrow(length = unit(0.02, "npc")),
        size=0.2, col='#990000') +
  geom_label(x=1.5,y=55,parse=TRUE, col="#000099",
           label="y[t] == beta[0] + beta[1] * x[t] + epsilon[t]") +
  geom_segment(x=1.5, y=52, xend=df$x[19]-0.03, yend=df$y[19]+1.5, 
               arrow=arrow(length = unit(0.02, "npc")),
               size=0.2, col='#000099') +
  geom_segment(x=df$x[19], y=df$y[19],
               xend=df$x[19], yend=3+10*df$x[19],
               col="#009900", size=0.2,
               arrow=arrow(length=unit(0.02,"npc"),
                           ends="both")) +
  geom_label(x=df$x[19]-0.07, 
             y=(df$y[19]+ 3+10*df$x[19])/2,
             col="#009900", label="epsilon[t]",
             parse=TRUE)
```

Notice that the observations do not lie on the straight line but are scattered around it. We can think of each observation $y_t$ consisting of the systematic or explained part of the model, $\beta_0+\beta_1x_t$, and the random "error", $\varepsilon_t$. The "error" term does not imply a mistake, but a deviation from the underlying straight line model. It captures anything that may affect $y_t$ other than $x_t$.



#### Example: US consumption expenditure {-}

Figure \@ref(fig:ConsInc) shows time series of quarterly percentage changes (growth rates) of real personal consumption expenditure ($y$) and real personal disposable income ($x$) for the US from 1960Q1 to 2016Q3.
  
```{r ConsInc, echo=TRUE, cache=TRUE, fig.cap="Percentage changes in personal consumption expenditure and personal income for the US."}

autoplot(uschange[,1:2], ylab="% change", xlab="Year") +
guides(colour=guide_legend(title=" "))


```

```{r, cache=TRUE, include=FALSE}
fit.cons <- tslm(Consumption ~ Income, data=uschange) 
```

A scatter plot of consumption against income is shown in Figure \@ref(fig:ConsInc2) along with the estimated regression line
$$
  \hat{y}_t=`r round(coef(fit.cons)[1],2)` + `r round(coef(fit.cons)[2],2)`x_t.
$$
We will show how this line is estimated in Section \@ref(Regr-LSprinciple). The fitted line is positively sloped showing a positive relationship between income and consumption. 

The interpretation of the intercept requires that a value of $x=0$ makes sense. In this case when $x=0$ (when there is no change in personal disposable income since the last quarter) the predicted value of $y$ is `r round(coef(fit.cons)[1], 2)` (there is a predicted increase in personal consumption expenditure of `r round(coef(fit.cons)[1], 2)`%). Even when $x=0$ does not make sense, the intercept is an important part of the model. Without it, the slope coefficient can be distorted unnecessarily. The intercept should always be included unless the requirement is to force the regression line "through the origin". In what follows we assume that an intercept is always included in the model for all our statements to hold true.

The slope coefficient shows that a one unit increase in $x$ (a 1% increase in personal disposable income) results on average in `r round(coef(fit.cons)[2], 2)` units increase in $y$ (an average `r round(coef(fit.cons)[2], 2)`% increase in personal consumption expenditure). Alternatively the estimated equation shows that a value of 1 for $x$ (the percentage increase in personal disposable income) will result in a forecast value of `r round(coef(fit.cons)[1], 2)` + `r  round(coef(fit.cons)[2], 2)`$\times1$ = 0.83 for $y$ (the percentage increase in personal consumption expenditure).


```{r ConsInc2, echo=TRUE, cache=TRUE, fig.cap="Scatterplot of consumption versus income and the fitted regression line."}
ggplot(as.data.frame(uschange), aes(x=Income, y=Consumption)) +
  ylab("Consumption (quarterly % change)") +xlab("Income (quarterly % change)") +
  geom_point() + geom_smooth(method="lm", se=FALSE)
```

The estimation results are shown below.

```{r ConsInc3, echo=TRUE, cache=TRUE}
fit.cons <- tslm(Consumption ~ Income, data=uschange) 
summary(fit.cons)
```



### Multiple linear regression {-}

When there are two or more predictor variables, the model is called a "multiple regression model". The general form of a multiple regression model is
\begin{equation}
  y_t = \beta_{0} + \beta_{1} x_{1,t} + \beta_{2} x_{2,t} + \cdots + \beta_{k} x_{k,t} + \varepsilon_t,
  (\#eq:lm)
\end{equation}
where $y$ is the variable to be forecast and $x_{1},\dots,x_{k}$ are the $k$ predictor variables. Each of the predictor variables must be numerical. The coefficients $\beta_{1},\dots,\beta_{k}$ measure the effect of each predictor after taking account of the effect of all other predictors in the model. Thus, the coefficients measure the *marginal effects* of the predictor variables.

#### Example: US consumption expenditure (revisited) {-}

Figure \@ref(fig:MultiPredictors) shows additional predictors that may be useful for forecasting US consumption expenditure. These are quarterly percentage changes in industrial production and personal savings, and quarterly changes in the unemployment rate (as this is already a percentage). Building a multiple linear regression model can potentially generate more accurate forecasts as we expect consumption expenditure to not only depend on personal income but other predictors as well. 


```{r MultiPredictors, echo=FALSE, fig.cap="Quarterly percentage changes in industrial production and personal savings and quarterly changes in the unemployment rate for the US over the period 1960Q1 to 2016Q3.", cache=TRUE}

autoplot(uschange[,3:5], ylab="", xlab="Year",facets = TRUE,colour=TRUE) +
guides(colour=guide_legend(title=" ")) +guides(colour="none")
```

The first column of Figure \@ref(fig:ScatterMatrix) shows the relationships between consumption expenditure and each of the predictors. The fitted regression lines show positive relationships between consumption and each income and industrial production and negative relationships with each savings and unemployment. The strength of these relationships are shown by the correlation coefficients across the first row.

```{r ScatterMatrix, echo=TRUE, cache=TRUE, fig.cap="A scatterplot matrix of US consumption expenditure and the four predictors."}

df <- as.data.frame(uschange)

my_fn <- function(data, mapping, ...){
  p <- ggplot(data = df, mapping = mapping) + 
    geom_point() + 
    geom_smooth(method=lm, se=FALSE)
  p
}

GGally::ggpairs(df, lower = list(continuous = my_fn))

```

The multiple linear regression model we will try to fit is
\begin{equation}
y_t=\beta_0 + \beta_1 x_{1,t}+ \beta_2 x_{2,t}+ \beta_3 x_{3,t}+ \beta_4 x_{4,t}+\varepsilon_t,
 (\#eq:usMLR)
\end{equation}
where $y$ is the percentage change in real personal consumption expenditure, $x_1$ is the percentage change in real personal disposable income, $x_2$ is the percentage change in industrial production, $x_3$ is the percentage change in personal savings and $x_4$ is the change in the unemployment rate.


### Assumptions {-}

When we use a linear regression model, we are implicitly making some assumptions about the variables in Equation \@ref(eq:lm).

First, we assume that the model is a reasonable approximation to reality; that is, that the relationship between the forecast variable and the predictor variables satisfies this linear equation.

Second, we make the following assumptions about the errors $(\varepsilon_{1},\dots,\varepsilon_{T})$:

  * they have mean zero; otherwise the forecasts will be systematically biased.
  * they are not autocorrelated; otherwise the forecasts will be inefficient as there is more information to be exploited in the data.
  * they are unrelated to the predictor variables; otherwise there would be more information that should be included in the systematic part of the model.

It is also useful to have the errors normally distributed with constant variance in order to easily produce prediction intervals.

Another important assumption in the linear regression model is that each $x$ is not a random variable. If we were performing a controlled experiment in a laboratory, we could control the values of each $x$ (so they would not be random) and observe the resulting values of $y$. With observational data (including most data in business and economics) it is not possible to control the value of $x$, and hence we make this an assumption.

## Least squares estimation {#Regr-LSprinciple}

In practice, of course, we have a collection of observations but we do not know the values of the coefficients $\beta_0,\beta_1, \dots, \beta_k$. These need to be estimated from the data.

The least squares principle provides a way of choosing the coefficients effectively by minimizing the sum of the squared errors. That is, we choose the values of $\beta_0, \beta_1, \dots, \beta_k$ that minimize
$$
  \sum_{t=1}^T \varepsilon_t^2 = \sum_{t=1}^T (y_t - 
  \beta_{0} - \beta_{1} x_{1,t} - \beta_{2} x_{2,t} - \cdots - \beta_{k} x_{k,t})^2.
$$

This is called "least squares" estimation because it gives the least value for the sum of squared errors.  Finding the best estimates of the coefficients is often called "fitting" the model to the data, or sometimes "learning" or "training" the model. The lines shown in Figures \@ref(fig:ConsInc2) and \@ref(fig:ScatterMatrix) were obtained in this way. 

When we refer to the *estimated* coefficients, we will use the notation $\hat\beta_0, \dots, \hat\beta_k$. The equations for these will be given in Section \@ref(Regr-MatrixEquations).

#### Example: US consumption expenditure (revisited) {-}

The following is estimation output from fitting the multiple linear regression model of equation \@ref(eq:usMLR) for forecasting US consumption expenditure. The first column gives the estimates of each $\beta$ coefficient and the second column gives its standard error (i.e., the standard deviation which would be obtained from repeatedly estimating the $\beta$ coefficients on similar data sets). The standard error gives a measure of the uncertainty in the estimated $\beta$ coefficient.

For forecasting purposes, the final two columns are of limited interest. The "t value" is the ratio of an estimated $\beta$ coefficient to its standard error and the last column gives the p-value: the probability of the estimated $\beta$ coefficient being as large as it is if there was no real relationship between the credit score and the predictor. This is useful when studying the effect of each predictor, but is not particularly useful when forecasting.

```{r usestim, echo=TRUE, fig.cap="Multiple regression output from least squares estimation.", cache=TRUE}
fit.consMR <- tslm(Consumption ~ Income + Production + Unemployment + Savings, data=uschange)
summary(fit.consMR)
```

### Fitted values {-}

Predictions of $y_t$ can be obtained by ignoring the error in the regression equation and using the estimated coefficients. Plugging in the values of $x_{1,t},\ldots,x_{k,t}$ for $t=1,\ldots,T$ into the regression equation
\begin{equation}
  \hat{y_t} = \hat\beta_{0} + \hat\beta_{1} x_{1,t} + \hat\beta_{2} x_{2,t} + \cdots + \hat\beta_{k} x_{k,t}
  (\#eq:prediction)
\end{equation}
returns in-sample predictions of $y_t$ referred to as *fitted values*. Note that these are predictions of the data used to estimate the model not genuine forecasts of future values of $y$. The following plots show the actual values compared to the fitted (i.e.,predicted) values for the percentage change in the US consumption expenditure series. The time plot shows that the fitted values follow the actual data fairly closely. This is verified by the strong positive relationship shown by the scatterplot. 


```{r usfitted1, echo=TRUE, cache=TRUE, fig.cap="Time plot of US consumtpion expenditure and predicted expenditure."}
df <- cbind(Data=uschange[,"Consumption"], Fitted=fitted(fit.consMR))
autoplot(df[,1:2]) + xlab("Year") + ylab("") +
  ggtitle("Percentage change in US consumption expenditure") +
  guides(colour=guide_legend(title=" "))
```

```{r usfitted2, echo=TRUE, cache=TRUE, fig.cap="Actual US consumtpion expenditure plotted against predicted US consumtpion expenditure."}
df <- as.data.frame(df)
ggplot(df,aes(x=Data, y=Fitted)) + 
  geom_point() +
  xlab("Fitted (predicted values)") + ylab("Data (actual values)") + 
  ggtitle("Percentage change in US consumption expenditure") +
  scale_color_brewer(palette="Dark2", name="Quarter") +
  geom_smooth(method="lm", se=FALSE)
```



## Some useful predictors {#Regr-UsefulPredictors}

There are several very useful predictors that occur frequently when using regression for time series data.

### Trend {-}

It is very common for time series data to be trending. A linear trend can be modelled by simply using $x_{1,t}=t$ as a predictor,

$$
  y_{t}= \beta_0+\beta_1t+\varepsilon_t,
$$
where $t=1,\ldots,T$. 
  
### Dummy variables {-}

So far, we have assumed that each predictor takes numerical values. But what about when a predictor is a categorical variable taking only two values (e.g., "yes" and "no"). Such a variable might arise, for example, when forecasting daily sales and you want to take account of whether the day is a public holiday or not.  So the predictor takes value "yes" on a public holiday, and  "no" otherwise.

This situation can still be handled within the framework of multiple regression models by creating a "dummy variable" taking value 1 corresponding to "yes" and 0 corresponding to "no". A dummy variable is also known as an "indicator variable".

A dummy variable can also be used to account for an outlier in the data. Rather than omit the outlier, a dummy variable removes its effect. In this case, the dummy variable takes value 1 for that observation and 1 everywhere else. An example is the case where a special event has occured. For example when forecasting tourist arrivals to Sydney Australia we will need to account for the effect of the Sydney summer Olympics in 2000.

If there are more than two categories, then the variable can be coded using several dummy variables (one fewer than the total number of categories).

### Seasonal dummy variables {-}

For example, suppose we are forecasting daily electricity demand and we want to account for the day of the week as a predictor. Then the following dummy variables can be created.

```{r dowdummy, echo=FALSE, cache=TRUE}
df <- matrix("0", nrow=13, ncol=6)
df[1:6,] <- paste(diag(6))
df[8:12,] <- paste(diag(6)[1:5,])
rownames(df) <- rep(c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"),2)[1:13]
colnames(df) <- paste("$x_{",1:6,",t}$",sep="")
df[13,] <- "&#8942;"
rownames(df)[13] <- "&#8942;"
knitr::kable(df)
```

Notice that only six dummy variables are needed to code seven categories. That is because the seventh category (in this case Sunday) is specified when all dummy variables are all set to zero and is captured by the intercept.

Many beginners will try to add a seventh dummy variable for the seventh category. This is known as the "dummy variable trap" because it will cause the regression to fail. There will be one too many parameters to estimate when an intercept is also included. The general rule is to use one fewer dummy variables than categories. So for quarterly data, use three dummy variables; for monthly data, use 11 dummy variables; and for daily data, use six dummy variables.

The interpretation of each of the coefficients associated with the dummy variables is that it is *a measure of the effect of that category relative to the omitted category*. In the above example, the coefficient of $x_{1,t}$ associated with Monday will measure the effect of Monday compared to Sunday on the forecast variable. An example of interpreting estimated dummy variable coefficients capturing the quarterly seasonality of Australian beer production follows. 

#### Example: Australian quarterly beer production {-}

Recall the Australian quarterly beer production data shown below in Figure \@ref(fig:beeragain).

```{r beeragain, echo=TRUE, cache=TRUE, fig.cap="Australian quarterly beer production."}
beer2 <- window(ausbeer, start=1992)
autoplot(beer2) + xlab("Year") + ylab("Megalitres") 
```

We want to forecast the value of future beer production. We can model this data using a regression model with a linear trend and quarterly dummy variables, 
\[
  y_{t} = \beta_{0} + \beta_{1} t + \beta_{2}d_{2,t} + \beta_3 d_{3,t} + \beta_4 d_{4,t} + \varepsilon_{t},
\]
where $d_{i,t} = 1$ if $t$ is in quarter $i$ and 0 otherwise. The first quarter variable has been omitted, so the coefficients associated with the other quarters are measures of the difference between those quarters and the first quarter. 

```{r fig.beerfit, echo=TRUE, cache=TRUE}
fit.beer <- tslm(beer2 ~ trend + season) 
summary(fit.beer)
```

There is a strong downward trend of `r round(fit.beer$coef['trend'], 3)` megalitres per quarter. On average, the second quarter has production of `r -round(fit.beer$coef['season2'], 1)` megalitres lower than the first quarter, the third quarter has production of `r -round(fit.beer$coef['season3'], 1)` megalitres lower than the first quarter, and the fourth quarter has production of `r round(fit.beer$coef['season4'], 1)` megalitres higher than the first quarter.


```{r beerlm2, echo=TRUE, cache=TRUE, fig.cap="Time plot of beer production and predicted beer production."}
df <- cbind(Data=beer2, Fitted=fitted(fit.beer))
autoplot(df[,1:2]) + xlab("Year") + ylab("") +
  ggtitle("Quarterly Beer Production") +
  guides(colour=guide_legend(title=" "))
```

```{r beerlm3, echo=TRUE, cache=TRUE, fig.cap="Actual beer production plotted against predicted beer production."}
df <- as.data.frame(df)
ggplot(df,aes(x=Data, y=Fitted, colour=as.factor(cycle(beer2)))) + 
  geom_point() +
  xlab("Fitted") + ylab("Actual values") + 
  ggtitle("Quarterly beer production") +
  scale_color_brewer(palette="Dark2", name="Quarter")
```


### Intervention variables {-}

It is often necessary to model interventions that may have affected the variable to be forecast. For example, competitor activity, advertising expenditure, industrial action, and so on, can all have an effect.

When the effect lasts only for one period, we use a spike variable. This is a dummy variable taking value one in the period of the intervention and zero elsewhere. A spike variable is equivalent to a dummy variable for handling an
outlier.

Other interventions have an immediate and permanent effect. If an intervention causes a level shift (i.e., the value of the series changes suddenly and permanently from the time of intervention), then we use a step variable. A step variable takes value zero before the intervention and one from the time of intervention onwards.

Another form of permanent effect is a change of slope. Here the intervention is handled using a piecewise linear trend as discussed earlier (where $\tau$ is the time of intervention).

### Trading days {-}

The number of trading days in a month can vary considerably and can have a substantial effect on sales data. To allow for this, the number of trading days in each month can be included as a predictor. An alternative that allows for the effects of different days of the week has the following predictors:

$$\begin{aligned} x_{1} &= \text{\# Mondays in month;} \\ x_{2} &= \text{\#
Tuesdays in month;} \\ & \vdots \\ x_{7} &= \text{\# Sundays in
month.}\end{aligned}$$

### Distributed lags {-}

It is often useful to include advertising expenditure as a predictor. However, since the effect of advertising can last beyond the actual campaign, we need to include lagged values of advertising expenditure. So the following predictors may be used.

$$\begin{aligned} x_{1} &= \text{advertising for previous month;} \\ x_{2} &=
\text{advertising for two months previously;} \\ &\vdots \\ x_{m} &=
\text{advertising for $m$ months previously.}\end{aligned}$$

It is common to require the coefficients to decrease as the lag increases. In Chapter \@ref(ch-dynamic) we discuss methods to allow this constraint to be implemented.

### Easter {-}

Easter is different from most holidays because it is not held on the same date each year and the effect can last for several days. In this case, a dummy variable can be used with value one where any part of the holiday falls in the particular time period and zero otherwise.

For example, with monthly data, when Easter falls in March then the dummy variable takes value 1 in March, when it falls in April, the dummy variable
takes value 1 in April, and when it starts in March and finishes in April, the dummy variable takes value 1 for both months. 

May be add this as an exercies try `easter(ausbeer)` and interpret what you see. 


## Evaluating the regression model {#Regr-EvaluatingSLR}

The difference between the observed $y$ values and the corresponding fitted $\hat{y}$ values are the in-sample forecast errors or "residuals" 
\begin{align*}
  e_t &= y_t - \hat{y}_t \\
      &= y_t - \hat\beta_{0} - \hat\beta_{1} x_{1,t} - \hat\beta_{2} x_{2,t} - \cdots - \hat\beta_{k} x_{k,t}.
\end{align*}
Each residual is the unpredictable component of the associated observation.

The residuals have some useful properties including the following two:
$$
  \sum_{t=1}^{T}{e_t}=0 \quad\text{and}\quad \sum_{t=1}^{T}{x_{k,t}e_t}=0.
$$
As a result of these properties, it is clear that the average of the residuals is zero, and that the correlation between the residuals and the observations for the predictor variable is also zero.

`comment on regression through the origin not necessarily having sum zero resids`

After selecting the regression variables and fitting a regression model, it is necessary to plot the residuals to check that the assumptions of the model have been satisfied. There are a series of plots that should be produced in order to check different aspects of the fitted model and the underlying assumptions.

### ACF plot of residuals {-}

With time series data we are considering in this book it is highly likely that the value of a variable observed in the current time period will be influenced by its value in the previous period, or even the period before that, and so on. Therefore when fitting a regression model to time series data, it is very common to find autocorrelation in the residuals. In this case, the estimated model violates the assumption of no autocorrelation in the errors, and our forecasts may be inefficient --- there is some information left over which should be accounted for in the model in order to obtain better forecasts. The forecasts from a model with autocorrelated errors are still unbiased, and so are not "wrong", but they will usually have larger prediction intervals than they need to. Therefore we should always look at an ACF plot of the residuals.

Figure \@ref(fig:beerlm4) shows a time plot and ACF of the residuals from the model fitted to the beer production data.

```{r beerlm4, echo=TRUE, cache=TRUE, fig.cap="Residuals from the regression model for beer production.", fig.height=3}
res <- residuals(fit.beer) 
p1 <- autoplot(res) + ylab("Residuals") + xlab("Year") 
p2 <- ggAcf(res) + ggtitle("ACF of residuals")
gridExtra::grid.arrange(p1, p2, nrow=1)
```

There seems to be an outlier in the residuals (2004Q4) which suggests that  something unusual happened in that quarter. It would be worth investigating that outlier to see if there were any unusual circumstances or events that may have reduced beer production for the quarter.

The remaining residuals show that the model has captured the patterns in the data quite well, although there is a small amount of autocorrelation left in the residuals (seen in the significant spike in the ACF plot). This suggests that the model can be slightly improved, although it is unlikely to make much difference to the resulting forecasts.

Another test of autocorrelation that is designed to take account of the regression model is the **Durbin-Watson** test. It is used to test the hypothesis that there is no first order autocorrelation in the residuals.  A small p-value indicates there is significant autocorrelation remaining in the residuals. For the beer model, the Durbin-Watson test reveals some significant lag one autocorrelation.

```{r dwtestbeer, echo=TRUE, cache=TRUE}
lmtest::dwtest(fit.beer, alt="two.sided") 
#It is recommended that the two-sided test always be used 
#to check for negative as well as positive autocorrelation
```

Both the ACF plot and the Durbin-Watson test show that there is some autocorrelation remaining in the residuals. This means there is some information remaining in the residuals that can be exploited to obtain better forecasts. The forecasts from the current model are still unbiased, but will have larger prediction intervals than they need to. A better model in this case will be a dynamic-regression model which will be covered in Chapter \@ref(ch-dynamic).

A third possible test is the **Breusch-Godfrey** test designed to look for significant higher-order autocorrelations.

```{r bgtestbeer, echo=TRUE, cache=TRUE}
#Test for autocorrelations up to lag 5. 
lmtest::bgtest(fit.beer,5)
```

### Histogram of residuals {-}

It is always a good idea to check if the residuals are normally distributed. As explained earlier, this is not essential for forecasting, but it does make the calculation of prediction intervals much easier. Using the `checkresiduals` function introduced in Section \@ref(toolbox-resids) we can obtain a histogram of the residuals as well as the other useful residual diagnostics mentioned above. The histogram in Figure \@ref(fig:beerresidcheck) below shows, that the residuals from modelling the beer data seem to be slightly negatively skewed, although that is partly due to the outlier. 

```{r beerresidcheck, echo=TRUE, cache=TRUE, fig.cap="Analysing the residuals from a regression model for beer production.", message=FALSE, warning=FALSE}
checkresiduals(fit.beer)
```

### Residual plots against predictors {-}

We would expect the residuals to be randomly scattered without showing any systematic patterns. A simple and quick way for a first check is to examine a scatterplot of the residuals against each of the predictor variables.

If these scatterplots show a pattern, then the relationship may be nonlinear and the model will need to be modified accordingly. See Section \@ref(Regr-NonLinear) for a discussion of nonlinear regression.

It is also necessary to plot the residuals against any predictors *not* in the model. If these show a pattern, then the predictor may need to be added to the model (possibly in a nonlinear form). 

The residuals from the multiple regression model for forecasting US consumption plotted agaist each predictor in Figure \@ref(fig:resids) seem to be randomly scattered and therefore we would be satisfied with these in this case.

```{r resids, echo=TRUE, cache=TRUE, fig.cap="Scatterplots of residuals versus each predictor."}
df <- as.data.frame(cbind(res=residuals(fit.consMR),Income=uschange[,"Income"],Production=uschange[,"Production"],Savings=uschange[,"Savings"],Unemployment=uschange[,"Unemployment"]))
p1 <- ggplot(df, aes(x=Income, y=res)) + geom_point() + ylab("Residuals")
p2 <- ggplot(df, aes(x=Production, y=res)) + geom_point() + ylab("Residuals")
p3 <- ggplot(df, aes(x=Savings, y=res)) + geom_point() + ylab("Residuals")
p4 <- ggplot(df, aes(x=Unemployment, y=res)) + geom_point() + ylab("Residuals")

gridExtra::grid.arrange(p1, p2, p3, p4, nrow=2)

```

### Residual plots against fitted values {-}

A plot of the residuals against the fitted values should also show no pattern. If a pattern is observed, there may be "heteroscedasticity" in the errors. That is, the variance of the residuals may not be constant. To overcome this problem, a transformation of the forecast variable (such as a logarithm or square root) may be required. 

Figure \@ref(fig:electrendseas) shows the residuals against the fitted values from fitting a linear trend and seasonal dummies to the electricity data we first encountered in Section \@ref(sec-transformations). The plot shows that the trend in the data is nonlinear and that the forecast variable requires a transformation as we observe a heteroscedastic pattern with variation increasing along the x-axis.


```{r electrendseas, echo=TRUE, cache=TRUE, fig.cap="Scatterplot of residuals versus fitted."}
fit <- tslm(elec ~ trend + season) 
df <- as.data.frame(cbind(fitd=fitted(fit), res=residuals(fit)))

ggplot(df, aes(x=fitd, y=res)) +
  ylab("residuals") +xlab("fitted") +
  geom_point()  
```


### Outliers and influential observations {-}

Observations that take on extreme values compared to the majority of the data are called "outliers". Observations that have a large influence on the estimation results of a regression model are called "influential observations". Usually, influential observations are also outliers that are extreme in the $x$ direction.

There are formal methods for detecting outliers and influential observations that are beyond the scope of this textbook. As we suggested at the beginning of Chapter \@ref(ch-graphics), getting familiar with your data prior to performing any analysis is of vital importance. A scatter plot of $y$ against each $x$ is always a useful starting point in regression analysis and often helps to identify unusual observations.

One source for an outlier occurring is an incorrect data entry. Simple descriptive statistics of your data can identify minima and maxima that are not sensible. If such an observation is identified, and it has been incorrectly recorded, it should be immediately removed from the sample.

Outliers also occur when some observations are simply different. In this case it may not be wise for these observations to be removed. If an observation has been identified as a likely outlier it is important to study it and analyze the possible reasons behind it. The decision of removing or retaining such an observation can be a challenging one (especially when outliers are influential observations). It is wise to report results both with and without the removal of such observations.

#### Example {-}
Figure \@ref(fig:outlier) highlights the effect of a single outlier when regressing US consumption on income (the example introduced in Section \@ref(Regr-Intro)). In the left panel the ourlier is only extreme in the direction of $y$ as the percentage change in consumption has been incorrectly recorded as -4%. The red line is the regression line fitted to the data which includes the outlier, compared to the black line which is the line fitted to the data without the outlier. In the right panel the outlier now is also extreme in the direction of $x$ with the 4% decrease in consumption corresponding to a 6% increase in income. In this case the outlier is very influential as the red line now deviates substantially from the black line.


```{r outlier, fig.cap="The effect of outliers and influential observations on regression", fig.height=4, echo=FALSE, cache=TRUE, message=FALSE,warning=FALSE}

yx <- as.data.frame(uschange[,1:2])
fit1 <- lm(Consumption ~ Income, data=yx) 

yx[1,] <- c(-4,1)
fit2 <- lm(Consumption ~ Income, data=yx) 
p1 <- ggplot(yx, aes(x=Income, y=Consumption)) +
  ylab("% change in consumption") +xlab("% change in income") +
  geom_point() + 
  geom_abline(intercept=fit1$coefficients[1],slope = fit1$coefficients[2]) + 
  geom_abline(intercept=fit2$coefficients[1],slope = fit2$coefficients[2],colour="red") +
  geom_point(x=1,y=-4, shape = 1, size = 7,col="blue")

yx[1,] <- c(-4,6)
fit2 <- lm(Consumption ~ Income, data=yx) 
p2 <- ggplot(yx, aes(x=Income, y=Consumption)) +
  ylab("% change in consumption") +xlab("% change in income") +
  geom_point() + 
  geom_abline(intercept=fit1$coefficients[1],slope = fit1$coefficients[2]) +  geom_abline(intercept=fit2$coefficients[1],slope = fit2$coefficients[2],colour="red") +
  geom_point(x=6,y=-4, shape = 1, size = 7,col="blue")

gridExtra::grid.arrange(p1, p2, nrow=1)
```

### Goodness-of-fit {-}

A common way to summarize how well a linear regression model fits the data is via the coefficient of determination or $R^2$. This can be calculated as the square of the correlation between the observed $y$ values and the predicted $\hat{y}$ values. Alternatively, it can also be calculated as, 
$$
  R^2 = \frac{\sum(\hat{y}_{t} - \bar{y})^2}{\sum(y_{t}-\bar{y})^2},
$$ 
where the summations are over all observations. Thus, it reflects the proportion of variation in the forecast variable that is accounted for (or explained) by the regression model.

If the predictions are close to the actual values, we would expect $R^2$ to be close to 1. On the other hand, if the predictions are unrelated to the actual values, then $R^2=0$. In all cases, $R^2$ lies between 0 and 1. `as long as we have an intercept`

In simple linear regression, the value of $R^2$ is also equal to the square of the correlation between $y$ and $x$.

The $R^2$ value is commonly used, often incorrectly, in forecasting. $R^2$ will never decrease when adding an extra predictor to the model and this can lead to overfitting. There are no set rules of what a good $R^2$ value is and typical values of $R^2$ depend on the type of data used. Validating a model’s forecasting performance on the test data is much better than measuring the $R^2$ value on the training data. 

Figure \@ref(fig:beerlm3) plots the actual monthly beer production versus the fitted values. The correlation between these variables is $r=`r round(cor(fit.beer$fitted.values,beer2)[1],2)`$ hence $R^2= `r round(cor(fit.beer$fitted.values,beer2)[1]^2,2)`$.  In this case model does an excellent job as it explains most of the variation in the beer production data. `add here something about the R2 of the SR v MR if it the model selection works out`.

### Standard error of the regression {-}

Another measure of how well the model has fitted the data is the standard deviation of the residuals, which is often known as the "standard error of the regression" and is calculated by
\begin{equation}
  s_e=\sqrt{\frac{1}{T-k-1}\sum_{t=1}^{T}{e_t^2}}.
  (\#eq:Regr-se)
\end{equation}

Notice that here we divide by $T-k-1$ in order to account for the number of estimated parameters in computing the residuals. Normally, we only need to estimate the mean (i.e., one parameter) when computing a standard deviation. The divisor is always $T$ minus the number of parameters estimated in the calculation. Here, we divide by $T−k−1$ because we have estimated k+1 parameters (the intercept and a coefficient for each predictor variable) in computing the residuals. 

The standard error is related to the size of the average error that the model produces. We can compare this error to the sample mean of $y$ or with the standard deviation of $y$ to gain some perspective on the accuracy of the model. In Section \@ref(Regr-LSprinciple) $s_e$ is part of the R estimation output labeled  `Residual standard error`" and takes the value `r round(summary(fit.consMR)$sigma,2)`%.

We should warn here that the evaluation of the standard error can be highly subjective as it is scale dependent. The main reason we introduce it here is that it is required when generating forecast intervals, discussed in Section \@ref(sec-4-5-ForeWithRegr).

### Spurious regression {-}

More often than not, time series data are "non-stationary"; that is, the values of the time series do not fluctuate around a constant mean or with a constant variance. We will deal with time series stationarity in more detail in Chapter \@ref(ch-arima), but here we need to address the effect non-stationary data can have on regression models.

For example consider the two variables plotted below in Figure \@ref(fig:spurious). These appear to be related simply because they both trend upwards in the same manner. However, air passenger traffic in Australia has nothing to do with rice production in Guinea. 


```{r spurious, echo=FALSE, fig.height=5, warning=FALSE, fig.cap="Trending time series data can appear to be related as shown in this example in which air passengers in Australia are regressed against rice production in Guinea.", message=FALSE, cache=TRUE}
aussies=window(ausair,end=2011)
df <- as.data.frame(cbind(GuineaRiceProduction=guinearice,aussies))
p1 <- autoplot(aussies) + xlab("Year") + ylab("Air Passengers in Australia")
p2 <- autoplot(guinearice) + xlab("Year") + ylab("Rice Production in Guinea")
p3 <- ggplot(df,aes(x=GuineaRiceProduction, y=aussies)) + geom_point() + ylab("Air Passengers in Australia (millions)") + xlab("Rice Production in Guinea (million tons)")
gridExtra::grid.arrange(gridExtra::arrangeGrob(p1, p2),p3, ncol = 2)
```

Regressing non-stationary time series can lead to spurious regressions. The output of regressing Australian air passengers on rice production in Guinea is shown below. High $R^2$s and high residual autocorrelation can be signs of spurious regression. We discuss the issues surrounding non-stationary data and spurious regressions in detail in Chapter \@ref(ch-dynamic).

Cases of spurious regression might appear to give reasonable short-term forecasts, but they will generally not continue to work into the future.


```{r, fig.cap="Residuals from a spurious regression.", echo=TRUE}
fit <- tslm(aussies ~ guinearice)
summary(fit)
checkresiduals(fit)
```

## Selecting predictors {#Regr-SelectingPredictors}

When there are many possible predictors, we need some strategy to select the
best predictors to use in a regression model.

A common approach that is *not recommended* is to plot the forecast variable
against a particular predictor and if it shows no noticeable relationship, drop it. This is invalid because it is not always possible to see the relationship from a scatterplot, especially when the effect of other predictors has not been accounted for.

Another common approach which is also invalid is to do a multiple linear
regression on all the predictors and disregard all variables whose $p$-values are greater than 0.05. To start with, statistical significance does not always indicate predictive value. Even if forecasting is not the goal, this is not a good strategy because the $p$-values can be misleading when two or more predictors are correlated with each other (see Section \@ref(Regr-MultiCol)).

Instead, we will use a measure of predictive accuracy. Five such measures are introduced in this section.

### Adjusted R$^2$  {-}

Computer output for regression will always give the $R^2$ value, discussed in Section \@ref(Regr-Intro). However, it is not a good measure of the predictive ability of a model. Imagine a model which produces forecasts that are exactly 20% of the actual values. In that case, the $R^2$ value would be 1 (indicating perfect correlation), but the forecasts are not very close to the actual values.

In addition, $R^2$ does not allow for "degrees of freedom". Adding *any* variable tends to increase the value of $R^2$, even if that variable is irrelevant. For these reasons, forecasters should not use $R^2$ to determine whether a model will give good predictions.

An equivalent idea is to select the model which gives the minimum sum of squared errors (SSE), given by $$\text{SSE} = \sum_{i=1}^N e_{i}^2.$$

Minimizing the SSE is equivalent to maximizing $R^2$ and will always choose the model with the most variables, and so is not a valid way of selecting predictors.

An alternative, designed to overcome these problems, is the adjusted $R^2$
(also called "R-bar-squared"): $$\bar{R}^2 = 1-(1-R^2)\frac{N-1}{N-k-1},$$
where $T$ is the number of observations and $k$ is the number of predictors.
This is an improvement on $R^2$ as it will no longer increase with each added predictor. Using this measure, the best model will be the one with the largest value of $\bar{R}^2$.

Maximizing $\bar{R}^2$ is equivalent to minimizing the following estimate of
the variance of the forecast errors: $$\hat{\sigma}^2 =
\frac{\text{SSE}}{N-k-1}.$$

Maximizing $\bar{R}^2$ works quite well as a method of selecting predictors,
although it does tend to err on the side of selecting too many predictors.

### Cross-validation 
####Make sure that this CV is valid here / possibly manipulate cvTS code to do this properly?? {-}


As discussed in Section \@ref(accuracy), cross-validation is a very
useful way of determining the predictive ability of a model. In general, leave-one-out cross-validation for regression can be carried out using the following steps.

1. Remove observation $t$ from the data set, and fit the model using the remaining data. Then compute the error ($e_{t}^*=y_{t}-\hat{y}_{t}$) for the omitted observation. (This is not the same as the residual because the $t$th observation was not used in estimating the value of $\hat{y}_{t}$.)
2. Repeat step 1 for $t=1,\dots,T$.
3. Compute the MSE from $e_{1}^*,\dots,e_{T}^*$. We shall call this the CV.

For many forecasting models, this is a time-consuming procedure, but for
regression there are very fast methods of calculating CV so it takes no longer than fitting one model to the full data set. The equation for computing CV is given in Section \@ref(Regr-MatrixEquations).

Under this criterion, the best model is the one with the smallest value of CV.

```{r}
# This will not work here
# e=tsCV(uschange[,1],fit.consMR,h=3)
CV(fit.consMR)
```

### Akaike’s Information Criterion {-}

A closely-related method is Akaike’s Information Criterion, which we define as
$$\text{AIC} = T\log\left(\frac{\text{SSE}}{T}\right) + 2(k+2),$$ 
where $T$ is the number of observations used for estimation and $k$ is the number of predictors in the model. Different computer packages use slightly different definitions for the AIC, although they should all lead to the same model being selected. The $k+2$ part of the equation occurs because there are $k+2$ parameters in the model: the $k$ coefficients for the predictors, the intercept and the variance of the residuals. The idea here is to penalize the fit of the model (SSE) with the number of parameters that need to be estimated.

The model with the minimum value of the AIC is often the best model for
forecasting. For large values of $T$, minimizing the AIC is equivalent to
minimizing the CV value.

### Corrected Akaike’s Information Criterion {-}

For small values of $T$, the AIC tends to select too many predictors, and so a bias-corrected version of the AIC has been developed,
$$
\text{AIC}_{\text{c}} =
\text{AIC} + \frac{2(k+2)(k+3)}{T-k-3}.
$$ 
As with the AIC, the AIC$_\text{c}$ should be minimized.

### Schwarz Bayesian Information Criterion {-}

A related measure is Schwarz’s Bayesian Information Criterion (known as SBIC, BIC or SC), 
$$
\text{BIC} = T\log\left(\frac{\text{SSE}}{T}\right) + (k+2)\log(T).
$$ 
As with the AIC, minimizing the BIC is intended to give the best model. The model chosen by BIC is either the same as that chosen by AIC, or one with fewer terms. This is because the BIC penalizes the number of parameters more heavily than the AIC. For large values of $T$, minimizing BIC is similar to leave-$v$-out cross-validation when $v = T[1-1/(\log(T)-1)]$.`is this ok here?`

Many statisticians like to use BIC because it has the feature that if there is a true underlying model, then with enough data the BIC will select that model. However, in reality there is rarely if ever a true underlying model, and even if there was a true underlying model, selecting that model will not necessarily give the best forecasts (because the parameter estimates may not be accurate).

#### Example: US consumption (continued) {-}
To obtain all these measures in R, use `CV(fit)`. In the multiple regression example for forecasting US consumption we considered four predictors. With four predictors, there are $2^4=16$ possible models. Now we can check if all four predictors are actually useful, or whether we can drop one or more of them. All 16 models were fitted and the results are summarised below in Table \@ref(tab:tblusMR). A "1" indicates that the variable was included in the model. The results have been sorted according to the AICc and therefore the best models are given at the top of the table, and the worst at the bottom of the table. There is clear separation between the models in the first four rows and the ones below. This indicates that Income and Savings should both be included in the model. However we would probably only one of Production and Unemployment as these two variables are highly (negatively) correlated, as shown in Figure \@ref(fig:ScatterMatrix). All criteria point towards including Unemployment.

```{r tblusMR, echo=FALSE, cache=TRUE}
fit <- list()
fit[[1]] <- lm(Consumption ~ Income + Production + Savings + Unemployment, data = uschange)
fit[[2]] <- lm(Consumption ~ Income + Production + Savings , data = uschange)
fit[[3]] <- lm(Consumption ~ Income + Production + Unemployment, data = uschange)
fit[[4]] <- lm(Consumption ~ Income + Savings + Unemployment, data = uschange)
fit[[5]] <- lm(Consumption ~ Income + Production , data = uschange)
fit[[6]] <- lm(Consumption ~ Income + Savings , data = uschange)
fit[[7]] <- lm(Consumption ~ Income + Unemployment, data = uschange)
fit[[8]] <- lm(Consumption ~ Income, data = uschange)
fit[[9]] <- lm(Consumption ~ Production + Savings + Unemployment, data = uschange)
fit[[10]] <- lm(Consumption ~ Production + Savings , data = uschange)
fit[[11]] <- lm(Consumption ~ Production + Unemployment, data = uschange)
fit[[12]] <- lm(Consumption ~ Production, data = uschange)
fit[[13]] <- lm(Consumption ~ Savings + Unemployment, data = uschange)
fit[[14]] <- lm(Consumption ~ Savings , data = uschange)
fit[[15]] <- lm(Consumption ~ Unemployment, data = uschange)
fit[[16]] <- lm(Consumption ~ 1, data = uschange)

fit.consBest <- lm(Consumption ~ Income + Savings + Unemployment, data = uschange)

out <- matrix(0, ncol = 9, nrow = 16)
colnames(out) <- c("Income", "Production", "Savings", "Unemployment", names(CV(fit[[1]])))
for (i in 1:16) out[i, 5:9] <- CV(fit[[i]])
out[c(1:8), 1] <- 1
out[c(1:3, 5, 9:12), 2] <- 1
out[c(1:2, 4, 6, 9:10, 13:14), 3] <- 1
out[c(1, 3:4, 7, 9, 11, 13, 15), 4] <- 1
j <- order(out[, 7])

knitr::kable(out[j, ], digits=3, booktabs=TRUE,align = c(rep("c", 4)),caption="All 16 possible models for forecasting US consumption with 4 predictors.")

```


### Best subset regression {-}

Where possible, all potential regression models can be fitted (as was done in the above example) and the best one selected based on one of the measures discussed here. This is known as "best subsets" regression or "all possible subsets" regression.

It is recommended that one of CV, AIC or AIC$_\text{c}$ be used for this purpose. If the value of $T$ is large enough, they will all lead to the same model. Most software packages will at least produce AIC, although CV and AIC$_\text{c}$ will be more accurate for smaller values of $T$.

While $\bar{R}^2$ is very widely used, and has been around longer than the other measures, its tendency to select too many predictor variables makes it less suitable for forecasting than either CV, AIC or AIC$_\text{c}$. Also, the tendency of BIC to select too few variables makes it less suitable for forecasting than either CV, AIC or AIC$_\text{c}$.

### Stepwise regression {-}

If there are a large number of predictors, it is not possible to fit all
possible models. For example, 40 predictors leads to $2^{40} >$ 1 trillion
possible models! Consequently, a strategy is required to limit the number of
models to be explored.

An approach that works quite well is *backwards stepwise regression*: 

* Start with the model containing all potential predictors.
* Remove one predictor at a time. Keep the model if it improves the measure of predictive accuracy.
* Iterate until no further improvement.

If the number of potential predictors is too large, then the backwards stepwise regression will not work and the *forward stepwise regression* can be used instead. This procedure starts with a model that includes only the intercept. Predictors are added one at a time, and the one that most improves the measure of predictive accuracy is retained in the model. The procedure is repeated until no further improvement can be achieved.  

Alternatively for eihter the backwards or forward direction a starting model can be one that that includes a subset of potential predictors. In this case, an extra step needs to be
included. For the backwards procedrue with each step we should also consider adding a predictor and for the forward procedure with each each step we should also consider dropping a predictor. These are usually referred to as *hybrid* procedures.


It is important to realise that a stepwise approach is not guaranteed to lead
to the best possible model. But it almost always leads to a good model. For further details see:

See James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. An Introduction to Statistical Learning. New York: Springer.


## Forecasting with regression {#Regr-ForeWithRegr}

Recall that predictions of $y$ can be otained using equation \@ref(eq:prediction),  
$$  \hat{y_t} = \hat\beta_{0} + \hat\beta_{1} x_{1,t} + \hat\beta_{2} x_{2,t} + \cdots + \hat\beta_{k} x_{k,t},$$
which comprises the estimated coefficients and ignores the error in the regression equation. Plugging in the values of the predictor variables $x_{1,t},\ldots,x_{k,t}$ for $t=1,\ldots,T$ returned the fitted (in-sample) values of $y$. What we are interested in here is forecasting future values of $y$.

### Ex-ante versus ex-post forecasts {-}

When using regression models for time series data, we need to distinguish between the different types of forecasts that can be produced, depending on what is assumed to be known when the forecasts are computed.

*Ex ante forecasts* are those that are made using only the information that is available in advance. For example, ex-ante forecasts for the percentage change in US consumption for the four quarters following the end of the sample, 2016Q4:2017Q3, should only use information that was available *upto* 2016Q3. These are genuine forecasts, made in advance using whatever information is available at the time. Ex-ante forecasts can be obtained by first generating forecasts for each of the predictors. For example we can use one of the simple methods introduced in Section \@ref(sec-2-methods) or more sophisticated pure time series approaches that follow in Chapters \@ref(ch-expsmooth) and \@ref(ch-arima). Alternatively forecasts by some other source, such as a government agency may be available and can be used.

*Ex post forecasts* are those that are made using later information on the predictors. For example, ex post forecasts of consumption may use the actual observations of the predictors, once these have been observed. These are not genuine forecasts, but are useful for studying the behaviour of forecasting models.

The model from which ex-post forecasts are produced should not be estimated using data from the forecast period. That is, ex-post forecasts can assume knowledge of the predictor variable (the $x$ variable), but should not assume knowledge of the data that are to be forecast (the $y$ variable).

A comparative evaluation of ex-ante forecasts and ex-post forecasts can help to separate out the sources of forecast uncertainty. This will show whether forecast errors have arisen due to poor forecasts of the predictor or due to a poor forecasting model.

#### Example: Australian quarterly beer production {-}

Normally, we cannot use actual future values of the predictor variables when producing ex-ante forecasts because their values will not be known in advance. However, the special predictors introduced in Section \@ref(Regr-UsefulPredictors) are all known in advance, as they are based on calendar variables (e.g., seasonal dummy variables or public holiday indicators) or deterministic functions of time. In such cases, there is
no difference betweeen ex ante and ex post forecasts.

```{r beerlm1, echo=TRUE, cache=TRUE, fig.cap="Forecasts from the regression model for beer production. The dark shaded region shows 80% prediction intervals and the light shaded region shows 95% prediction intervals."}
fcast <- forecast(fit.beer) 
autoplot(fcast) + ggtitle("Forecasts of beer production using linear regression")
```

### Scenario based forecasting {-}

In this setting the forecaster assumes possible scenarios for the predictor variables that are of interest. For example, a US policy maker may be interested in comparing the projected change in consumption if there is a constant growth of 1% and 0.5% respectively for income and savings, versus a repsective decline of 1% and 0.5% for each of the four quarters following the end of the sample, with no change in the unemployment rate. The resulting forecasts are calculated and shown in Figure \@ref(fig:ConsInc4).

```{r, ConsInc4, echo=TRUE, cache=TRUE, fig.cap="Forecasting percentage changes in personal consumption expenditure for the US under scenario based forecasting."}

fit.consBest <- tslm(Consumption ~ Income + Savings + Unemployment, data = uschange)
newdata=data.frame(cbind(Income=c(1,1,1,1),Savings=c(0.5,0.5,0.5,0.5),Unemployment=c(0,0,0,0)))
fcast.up <- forecast(fit.consBest, newdata=newdata) 
newdata=data.frame(cbind(Income=c(-1,-1,-1,-1),Savings=c(-0.5,-0.5,-0.5,-0.5),Unemployment=c(0,0,0,0)))
fcast.down <- forecast(fit.consBest, newdata=newdata) 

# autoplot is giving me trouble - 
autoplot(uschange[,1]) +ylab("% change in US consumption") +
  autolayer(fcast.up, PI=TRUE, series="increase") +
  autolayer(fcast.down, PI=TRUE, series="decrease") +
  guides(colour=guide_legend(title="Scenario"))

```

Prediction intervals for scenario based forecasts do not include the uncertainty associated with the future values of the predictor variables. They assume the value of the predictor is known in advance.

### Predicition intervals {-}

With each forecast for the change in consumption Figure \@ref(fig:ConsInc4) a 95% and an 80% prediction interval is also included. The general formulation of how to calculate prediction intervals for multiple regression models is presented in Section \@ref(Regr-MatrixEquations). As this involves some matrix algebra we present here the case for calculating prediction intervals for a simple regression where a forecast can be generated using the equation, 
$$\hat{y}=\hat{\beta}_0+\hat{\beta}_1x.$$
Assuming that the regression errors are normally distributed, an approximate 95% predcition interval (also called a forecast interval) associated with this forecast is given by
\begin{equation} 
  \hat{y} \pm 1.96 s_e\sqrt{1+\frac{1}{T}+\frac{(x-\bar{x})^2}{(T-1)s_x^2}},
  (\#eq:Regr-pi)
\end{equation}
where $T$ is the total number of observations, $\bar{x}$ is the mean of the observed $x$ values, $s_x$ is the standard deviation of the observed $x$ values and $s_e$ is the standard error of the regression given by Equation \@ref(eq:Regr-se). Similarly, an 80% forecast interval can be obtained by replacing 1.96 by 1.28. Other prediction intervals can be obtained by replacing the 1.96 with the appropriate value given in Table \@ref(tab:pcmultipliers). If R is used to obtain forecast intervals, more exact calculations are obtained (especially for small values of $T$) than what is given by Equation \@ref(eq:Regr-pi).

Equation \@ref(eq:Regr-pi) shows that the forecast interval is wider when $x$ is far from $\bar{x}$. That is,  we are more certain about our forecasts when considering values of the predictor variable close to its sample mean. 

##### No need for the following I think {-}
I was going to show the difference between a PI for the mean income in the next quarter v a PI for an extreme case. 
The estimated simple regression line in the US consumpion example is
\[ \hat{y}_t=`r round(coef(fit.cons)[1],2)` + `r round(coef(fit.cons)[2],2)`x_t.
\]

```{r, ConsSimple, echo=TRUE, cache=TRUE, fig.cap="Prediction intervals for the mean and extreme increase."}

avInc <- mean(uschange[,"Income"])

fcast.up <- forecast(fit.cons, newdata=data.frame(Income=rep(avInc,8))) 
fcast.down <- forecast(fit.cons, newdata=data.frame(Income=rep(10,8))) 

fcast.up$upper[1,2]-fcast.up$lower[1,2]
fcast.down$upper[1,2]-fcast.down$lower[1,2]

# autoplot is giving me trouble - 
autoplot(uschange[,1]) +ylab("% change in US consumption") +
  autolayer(fcast.up, PI=TRUE, series="increase") +
  autolayer(fcast.down, PI=TRUE, series="decrease") +
  guides(colour=guide_legend(title="Scenario"))

```



For the Chevrolet Aveo (the first car in the list) $x_1$=25 mpg and $y_1=6.6$ tons of CO$_2$ per year. The model returns a fitted value of $\hat{y}_1$=7.00, i.e., $e_1=-0.4$. For a car with City driving fuel economy $x=30$ mpg, the average footprint forecasted is $\hat{y}=5.90$ tons of CO$_2$ per year. The corresponding 95\% and 80\% forecast intervals are [4.95, 6.84] and [5.28, 6.51] respectively (calculated using R)\@.



### Building a predictive regression model {-}


## Matrix formulation {#Regr-MatrixEquations}

*Warning: this is a more advanced optional section and assumes knowledge of
matrix algebra.*

Recall that multiple regression model can be written  as   $$y_{t} = \beta_{0} + \beta_{1}
x_{1,t} + \beta_{2} x_{2,t} + \cdots + \beta_{k} x_{k,t} + \varepsilon_{t}.$$
This expresses the relationship between a single value of the forecast variable
and the predictors. It can be convenient to write this in matrix form where all
the values of the forecast variable are given in a single equation. Let 
$\bm{y} = (y_{1},\dots,y_{T})'$, 
$\bm{\varepsilon} = (\varepsilon_{1},\dots,\varepsilon_{T})'$, 
$\bm{\beta} = (\beta_{0},\dots,\beta_{k})'$ and
$$
\bm{X} = \left[\begin{matrix}
  1 & x_{1,1} & x_{2,1} & \dots & x_{k,1}\\ 
  1 & x_{1,2} & x_{2,2} & \dots & x_{k,2}\\
  \vdots& \vdots& \vdots&& \vdots\\
  1 & x_{1,T}& x_{2,T}& \dots& x_{k,T}
\end{matrix}\right].
$$ 
Then
$$\bm{y} = \bm{X}\bm{\beta} + \bm{\varepsilon}.$$

Note that the $\bm{X}$ matrix has $T$ rows reflecting the number of observations and $k+1$ columns reflecting the number of predictors and the intercept which is represented by the column of ones. 

### Least squares estimation {-}

Least squares estimation is obtained by minimizing the expression
$\bm{\varepsilon}'\bm{\varepsilon} = (\bm{y} - \bm{X}\bm{\beta})'(\bm{y} -
\bm{X}\bm{\beta})$. It can be shown that this is minimized when $\bm{\beta}$
takes the value $$\hat{\bm{\beta}} = (\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y}$$ 
This is sometimes known as the "normal equation". The estimated coefficients
require the inversion of the matrix $\bm{X}'\bm{X}$. If $\bm{X}$ is not of full column rank then matrix $\bm{X}'\bm{X}$ is singular and the model cannot be estimated. This will occur, for example, if you fall for the "dummy variable trap", i.e., having the same number of dummy variables as there are categories of a categorical predictor, as discussed in Section \@ref(Regr-UsefulPredictors).

The residual variance is estimated using $$\hat{\sigma}^2 =
\frac{1}{T-k-1}(\bm{y} - \bm{X}\hat{\bm{\beta}})'(\bm{y} -
\bm{X}\hat{\bm{\beta}}).$$

### Fitted values and cross-validation {-}

The normal equation shows that the fitted values can be calculated using
$$\bm{\hat{y}} = \bm{X}\hat{\bm{\beta}} =
\bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{y} = \bm{H}\bm{y},$$ where $\bm{H} =
\bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}'$ is known as the "hat-matrix" because it is
used to compute $\bm{\hat{y}}$ ("y-hat").

If the diagonal values of $\bm{H}$ are denoted by $h_{1},\dots,h_{T}$, then the
cross-validation statistic can be computed using $$\text{CV} =
\frac{1}{T}\sum_{t=1}^T [e_{t}/(1-h_{t})]^2,$$ where $e_{t}$ is the residual
obtained from fitting the model to all $T$ observations. Thus, it is not
necessary to actually fit $T$ separate models when computing the CV statistic.

### Forecasts and prediction intervals {-}

Let $\bm{x}^*$ be a row vector containing the values of the predictors (in the same format as $\bm{X}$) for which we want to generate a forecast . Then the forecast is given by
$$\hat{y} = \bm{x}^*\hat{\bm{\beta}}=\bm{x}^*(\bm{X}'\bm{X})^{-1}\bm{X}'\bm{Y}$$ and its variance is given by $$\sigma^2 \left[1 + \bm{x}^* (\bm{X}'\bm{X})^{-1} (\bm{x}^*)'\right].$$ A 95% prediction interval can be calculated (assuming normally distributed errors) as $$\hat{y} \pm 1.96 \hat{\sigma} \sqrt{1 + \bm{x}^* (\bm{X}'\bm{X})^{-1} (\bm{x}^*)'}.$$ This takes account of the uncertainty due to the error term $\varepsilon$ and the uncertainty in the coefficient estimates. However, it
ignores any errors in $\bm{x}^*$. So if the future values of the predictors are
uncertain, then the prediction interval calculated using this expression will
be too narrow.

## Nonlinear regression {#Regr-NonLinear}

Although the linear relationship assumed so far in this chapter is often adequate, there are many cases for which a nonlinear functional form is more suitable. To keep things simple in this section suppose we have only one predictor $x$. 

Simply transforming the forecast variable $y$ and/or the predictor variable $x$ and then estimating a regression model using the transformed variables is the simplest way of obtaining a nonlinear specification. The most commonly used transformation is the (natural) logarithmic (see Section \@ref(sec-transformations)).

A **log-log** functional form is specified as 
$$
  \log y=\beta_0+\beta_1 \log x +\varepsilon.
$$
In this model, the slope $\beta_1$ is interpreted as an elasticity, $\beta_1$ is the average percentage change in $y$ resulting from a $1\%$ change in $x$. Other useful forms can also be specified. The **log-linear** form is specified by only transforming the forecast variable and the **linear-log** form is obtained by transforming the predictror. 

Recall that in order to perform a logarithmic transformation to a variable, all its observed values must be greater than zero. In the case that variable $x$ contains zeros we use the transformation $\log(x+1)$. That is, we add one to the value of the variable and then take logarithms. This has a similar effect to taking logarithms but avoids the problem of zeros. It also has the neat side-effect of zeros on the original scale remaining zeros on the transformed scale. 

There are cases for which simply transforming the data will not be adequate and a more general specification may be required. Then the model we use is
$$y=f(x) +\varepsilon$$
where $f$ is a nonlinear function. In standard (linear) regression, $f(x)=\beta_{0} + \beta_{1} x$. In the specification of nonlinear regression that follows, we allow $f$ to be a more flexible nonlinear function of $x$, compared to simply a logarithmic or other transformation. 

One of the simplest specifications is to make $f$ **piecewise linear**. That is, we introduce points where the slope of $f$ can change. These points are called "knots". This can be achieved by letting $x_{1,t}=x$ and introducing variable $x_{2,t}$ such that 
\begin{align*}
  x_{2,t} = (x-c)_+ &= \left\{ 
             \begin{array}{ll} 
               0 & x < c\\ 
               (x-c) &  x \ge c
             \end{array}\right.
\end{align*}
The notation $(x-c)_+$ means the value $x-c$, if it is positive and 0 otherwise. This forces the slope to bend at point $c$. Additional bends can be included in the relationship by adding further variables of the above form. 

An example of this follows by considering $x=t$ and fitting a piecewise linear trend to a times series. 

Piecewise linear relationships constructed in this way are a special case of **regression splines**. In general, a linear regression spline is obtained using
$$ x_{1}= x \quad x_{2} = (x-c_{1})_+ \quad\dots\quad x_{k} = (x-c_{k-1})_+$$
where $c_{1},\dots,c_{k-1}$ are the knots (the points at which the line can bend). Selecting the number of knots ($k-1$) and where they should be positioned can be difficult and somewhat arbitrary. Some automatic knot selection algorithms are available in some software, but are not yet widely used.

A smoother result is obtained using piecewise cubics rather than piecewise lines. These are constrained so they are continuous (they join up) and they are smooth (so there are no sudden changes of direction as we see with piecewise linear splines). In general, a cubic regression spline is written as
$$ x_{1}= x \quad x_{2}=x^2 \quad x_3=x^3 \quad x_4 = (x-c_{1})_+ \quad\dots\quad x_{k} = (x-c_{k-3})_+.$$
Cubic splines usually give a better fit to the data. However, forecasting values of $y$ when $x$ is outside the range of the historical data becomes very unreliable.  

### Forecasting with a nonlinear trend {-} 
In Section \@ref(Regr-UsefulPredictors) fitting a linear trend to a time series by setting $x=t$ was introduced. The simplest way of fitting a nonlinear trend is using quadratic or higher order trends obtained by specifying 
\[
  x_{1,t} =t,\quad x_{2,t}=t^2,\quad \ldots
\]
However, it is not recommended that quadratic or higher order trends are used in forecasting. When they are extrapolated, the resulting forecasts are often very unrealistic.

A better approach is to use the piecewise specification introduced above and fit a piecwise linear trend which bends at some point in time. We can think of this as a nonlinear trend constructed of linear pieces. If the trend bends at time $\tau$, then it can be specified by simply replacing $x=t$ and $c=\tau$ above such that we include the predictors,
\begin{align*}
  x_{1,t} & = t \\ 
  x_{2,t} = (t-\tau)_+ &= \left\{ 
             \begin{array}{ll} 
               0 & t < \tau\\ 
               (t-\tau) &  t \ge \tau
             \end{array}\right.
\end{align*}
in the model. If the associated coefficients of $x_{1,t}$ and $x_{2,t}$ are $\beta_1$ and $\beta_2$, then $\beta_1$ gives the slope of the trend before time $\tau$, while the slope of the line after time $\tau$ is given by $\beta_1+\beta_2$. Additional bends can be included in the relationship by adding further variables of the form $(t-\tau)_+$ where $\tau$ is the "knot" or point in time at which the line should bend. 

#### Example: Boston marathon winning times {-}

Figure \@ref(fig:marathonLinear) below in the left panel shows the Boston marathon winning times (in minutes) since it started in 1897. The time series shows a general downward trend as the winning times have been improving over the years. The panel on the right shows the residuals from fitting a linear trend to the data. The plot shows an obvious nonlinear pattern which has not been captured by the linear trend. 

```{r marathonLinear, echo=FALSE, fig.cap="Fitting a linear trend to the Boston marathon winning times is inadequate", message=TRUE, warning=FALSE, cache=TRUE}
fit.lin <- tslm(marathon ~ trend)

p1 <- autoplot(marathon) +
  autolayer(fitted(fit.lin), series = "Linear trend") + 
  xlab("Year") +  ylab("Winning times in minutes") +
  guides(colour=guide_legend(title=" ")) + 
  theme(legend.position = "none")

p2 <- autoplot(residuals(fit.lin)) +
    xlab("Year") + ylab("Residuals from a linear trend")

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

Fitting an exponential trend to the data can be achieved by transforming the $y$ variable so that the model to be fitted is,
$$\log y_t=\beta_0+\beta_1 t +\varepsilon_t.$$
The fitted exponential trend and forecasts are shown in Figure \@ref(fig:marathonNonlinear) below. Although the exponential trend does not seem to fit the data much better than the linear trend, it gives a more sensible projection in that the winning times will decrease in the future but at a decaying rate rather than a linear non-changing rate. 

Simply eyeballing the winning times reveals three very different periods. There is a lot of volatility in the winning times from the beginning up to about 1940 with the winning times improving/decreasing overall but with significant increases in the decade of the 1920s. The almost linear decrease in times after 1940 is then followed by a flattening out after the 1980s with almost an upturn towards the end of the sample. Following these observations we specify as knots the years 1940 and 1980. We should warn here that subjectively identifying suitable knots in the data can lead to overfitting which can be detrimental to the forecast performance of a model and should be performed wihth caution.

```{r marathonNonlinear, echo=TRUE, message=TRUE, warning=FALSE, fig.cap="Projecting forecasts from a linear, exponential, piecewise linear trends and a cubic spline for the Boston marathon winning times"}

fit.lin <- tslm(marathon ~ trend)
fit.exp <- tslm(marathon ~ trend, lambda = 0)
h=10
fcasts.lin <- forecast(fit.lin,h=h)
fcasts.exp <- forecast(fit.exp,h=h)

t.break1=1940
t.break2=1980
t <- time(marathon)
t1 <- ts(pmax(0,t-t.break1),start=1897,freq=1)
t2 <- ts(pmax(0,t-t.break2),start=1897,freq=1)
fit.pw=tslm(marathon ~ t + t1 + t2)
t.new <- t[length(t)]+seq(h)
t1.new <- t1[length(t1)]+seq(h)
t2.new <- t2[length(t2)]+seq(h)
newdata <- data.frame(cbind(t=t.new,t1=t1.new,t2=t2.new))
fcasts.pw <- forecast(fit.pw,newdata = newdata)
fit.spline=tslm(marathon ~ t + I(t^2) + I(t^3) + I(t1^3) + I(t2^3))
fcasts.spline <- forecast(fit.spline, newdata = newdata)
autoplot(marathon) +
  autolayer(fitted(fit.lin), series = "Linear") +
  autolayer(fitted(fit.exp), series="Exponetial") +
  autolayer(fitted(fit.pw), series = "Piecewise") +
  autolayer(fitted(fit.spline), series = "Cubic spline") +
  autolayer(fcasts.pw, series="Piecewise") +
  autolayer(fcasts.lin$mean, series = "Linear") +
  autolayer(fcasts.exp$mean, series="Exponetial") +
  autolayer(fcasts.spline$mean, series="Cubic spline") +
  xlab("Year") +  ylab("Winning times in minutes") +
  ggtitle("Boston Marathon") +guides(colour=guide_legend(title=" "))

```

Figure \@ref(fig:marathonNonlinear) above shows the fitted lines and forecasts from a linear and an exponential trend as well as a piecewise linear and a cubic spline. The cubic spline fits the data extremely well as expected, however the projection forward from this shows that it can be very unreliable when projecting outside the range of the historical data (as warned above) `make sure that this makes sense`. The piecewise linear trend fits the data better than both the exponential and linear trends and seems to return sensible forecasts. The residuals plotted in the Figure \@ref(fig:residPiecewise) below show that the model has mostly captured the trend well although there is some autocorrelation remaining (we will deal with this in Chapter \ref(ch-advanced)). We should however again warn here that the projection forward from the piecewise linear trend can be very sensitive to the choice of location of the knots, especially the ones located towards the end of the sample. For example moving the knot placed at 1980 even five years forward will reveal very different forecasts. The wide prediction interval associated with the forecasts form the piecewise linear trend reflects the volatility observed in the historical winning times.

**change the label in the checkresiduals as these are from a nonlinear regression - I suggest simply removing the Linear - Residuals from a regression model.**
```{r residPiecewise, fig.cap="Residuals from the piecewise trend.", message=FALSE, warning=FALSE}
checkresiduals(fit.pw)
```


## Correlation, causation and forecasting {#Regr-MultiCol}


### Correlation is not causation {-}

It is important not to confuse correlation with causation, or causation with forecasting. A variable $x$ may be useful for predicting a variable $y$, but that does not mean $x$ is causing $y$. It is possible that $x$ *is* causing $y$, but it may be that the relationship between them is more complicated than simple causality.

For example, it is possible to model the number of drownings at a beach resort each month with the number of ice-creams sold in the same period. The model can give reasonable forecasts, not because ice-creams cause drownings, but because people eat more ice-creams on hot days when they are also more likely to go swimming. So the two variables (ice-cream sales and drownings) are correlated, but one is not causing the other. It is important to understand that **correlations are useful for forecasting, even when there is no causal relationship between the two variables**.

However, often a better model is possible if a causal mechanism can be determined. In this example, both ice-cream sales and drownings will be
affected by the temperature and by the numbers of people visiting the beach resort. Again, high temperatures do not actually *cause* people to drown, but they are more directly related to why people are swimming. So a better model
for drownings will probably include temperatures and visitor numbers and exclude ice-cream sales.

### Confounded predictors {-}

A related issue involves confounding variables. Suppose we are forecasting monthly sales of a company for 2012, using data from 2000--2011. In January 2008 a new competitor came into the market and started taking some market share. At the same time, the economy began to decline. In your forecasting model, you include both competitor activity (measured using advertising time on a local television station) and the health of the economy (measured using GDP). It will not be possible to separate the effects of these two predictors because they
are correlated. We say two variables are **confounded** when their effects on
the forecast variable cannot be separated. Any pair of correlated predictors
will have some level of confounding, but we would not normally describe them as
confounded unless there was a relatively high level of correlation between
them.

Confounding is not really a problem for forecasting, as we can still compute
forecasts without needing to separate out the effects of the predictors.
However, it becomes a problem with scenario forecasting as the scenarios should
take account of the relationships between predictors. It is also a problem if
some historical analysis of the contributions of various predictors is
required.

### Multicollinearity and forecasting {-}

A closely related issue is **multicollinearity** which occurs when similar
information is provided by two or more of the predictor variables in a multiple
regression. It can occur in a number of ways.

Two predictors are highly correlated with each other (that is, they have a
correlation coefficient close to +1 or -1). In this case, knowing the value of
one of the variables tells you a lot about the value of the other variable.
Hence, they are providing similar information.

A linear combination of predictors is highly correlated with another linear
combination of predictors. In this case, knowing the value of the first group
of predictors tells you a lot about the value of the second group of
predictors. Hence, they are providing similar information.

The dummy variable trap is a special case of multicollinearity. Suppose you
have quarterly data and use four dummy variables, $D_1,D_2,D_3$ and $D_4$. Then
$D_4=1-D_1-D_2-D_3$, so there is perfect correlation between $D_4$ and
$D_1+D_2+D_3$.

When multicollinearity occurs in a multiple regression model, there are several
consequences that you need to be aware of.

If there is perfect correlation (i.e., a correlation of +1 or -1, such as in
the dummy variable trap), it is not possible to estimate the regression model.

If there is high correlation (close to but not equal to +1 or -1), then the
estimation of the regression coefficients is computationally difficult. In
fact, some software (notably Microsoft Excel) may give highly inaccurate
estimates of the coefficients. Most reputable statistical software will use
algorithms to limit the effect of multicollinearity on the coefficient
estimates, but you do need to be careful. The major software packages such as
R, SPSS, SAS and Stata all use estimation algorithms to avoid the problem as
much as possible.

The uncertainty associated with individual regression coefficients will be
large. This is because they are difficult to estimate. Consequently,
statistical tests (e.g., t-tests) on regression coefficients are unreliable.
(In forecasting we are rarely interested in such tests.) Also, it will not be
possible to make accurate statements about the contribution of each separate
predictor to the forecast.

Forecasts will be unreliable if the values of the future predictors are outside
the range of the historical values of the predictors. For example, suppose you
have fitted a regression model with predictors $X$ and $Z$ which are highly
correlated with each other, and suppose that the values of $X$ in the fitting
data ranged between 0 and 100. Then forecasts based on $X>100$ or $X<0$ will be
unreliable. It is always a little dangerous when future values of the
predictors lie much outside the historical range, but it is especially
problematic when multicollinearity is present.

Note that if you are using good statistical software, if you are not interested
in the specific contributions of each predictor, and if the future values of
your predictor variables are within their historical ranges, there is nothing
to worry about --- multicollinearity is not a problem.



## Exercises


1. Electricity consumption was recorded for a small town on 12 randomly chosen days. The following maximum temperatures (degrees Celsius) and consumption (megawatt-hours) were recorded for each day.

```{r, echo=FALSE, cache=TRUE}
# Not sure how to print this in landscape. Have tried various things even using # rowsnames but kable needs colnames

day <- seq(1,nrow(econsumption),1)
tbl <- cbind(day,econsumption)
colnames(tbl) <- c("Day","Mwh","Temp")

knitr::kable(tbl, booktabs=TRUE,align = c(rep("c", 2)),caption="All 16 possible models for forecasting US consumption with 4 predictors.", floating.environment="sidewaystable")

```

a. Plot the data and find the regression model for Mwh with temperature as an explanatory variable. Why is there a negative relationship?

b. Produce a residual plot. Is the model adequate? Are there any outliers or influential observations?

c. Use the model to predict the electricity consumption that you would expect for a day with maximum temperature $10^\circ$ and a day with maximum temperature $35^\circ$. Do you believe these predictions?

d. Give prediction intervals for your forecasts.

The following R code will get you started:

```{r, eval=FALSE, include=TRUE}
    plot(Mwh ~ temp, data=econsumption) fit <- lm(Mwh ~ temp,
    data=econsumption) plot(residuals(fit) ~ temp, data=econsumption)
    forecast(fit, newdata=data.frame(temp=c(10,35)))
```

2. Data set `olympic` contains the winning times (in seconds) for the men’s 400 meters final in each Olympic Games from 1896 to 2012.

a. Plot the winning time against the year. Describe the main features of the scatterplot.

b. Fit a regression line to the data. Obviously the winning times have been decreasing, but at what *average* rate per year?

c. Plot the residuals against the year. What does this indicate about the suitability of the fitted line?

d. Predict the winning time for the men’s 400 meters final in the 2000, 2004, 2008 and 2012 Olympics. Give a prediction interval for each of your forecasts. What assumptions have you made in these calculations?

e. Find out the actual winning times for these Olympics (see [www.databaseolympics.com](www.databaseolympics.com)). How good were your forecasts and prediction intervals?

An elasticity coefficient is the ratio of the percentage change in the forecast variable ($y$) to the percentage change in the predictor variable ($x$). Mathematically, the elasticity is defined as $(dy/dx)\times(x/y)$. Consider the log-log model, $$\log y=\beta_0+\beta_1 \log x + \varepsilon.$$ Express $y$ as a function of $x$ and show that the coefficient $\beta_1$ is the elasticity coefficient.


The data below (data set `fancy`) concern the monthly sales figures of a shop
which opened in January 1987 and sells gifts, souvenirs, and novelties. The
shop is situated on the wharf at a beach resort town in Queensland, Australia.
The sales volume varies with the seasonal population of tourists. There is a
large influx of visitors to the town at Christmas and for the local surfing
festival, held every March since
1988. Over time, the shop has expanded its premises, range of products, and
      staff.

=0.15cm

<span>lrrrrrrr</span> & **1987 & **1988 & **1989 & **1990 & **1991 & **1992 &
**1993\ **Jan & 1664.81 & 2499.81 & 4717.02 & 5921.10 & 4826.64 & 7615.03 &
10243.24\ **Feb & 2397.53 & 5198.24 & 5702.63 & 5814.58 & 6470.23 & 9849.69 &
11266.88\ **Mar & 2840.71 & 7225.14 & 9957.58 & 12421.25 & 9638.77 & 14558.40 &
21826.84\ **Apr & 3547.29 & 4806.03 & 5304.78 & 6369.77 & 8821.17 & 11587.33 &
17357.33\ **May & 3752.96 & 5900.88 & 6492.43 & 7609.12 & 8722.37 & 9332.56 &
15997.79\ **Jun & 3714.74 & 4951.34 & 6630.80 & 7224.75 & 10209.48 & 13082.09 &
18601.53\ **Jul & 4349.61 & 6179.12 & 7349.62 & 8121.22 & 11276.55 & 16732.78 &
26155.15\ **Aug & 3566.34 & 4752.15 & 8176.62 & 7979.25 & 12552.22 & 19888.61 &
28586.52\ **Sep & 5021.82 & 5496.43 & 8573.17 & 8093.06 & 11637.39 & 23933.38 &
30505.41\ **Oct & 6423.48 & 5835.10 & 9690.50 & 8476.70 & 13606.89 & 25391.35 &
30821.33\ **Nov & 7600.60 & 12600.08 & 15151.84 & 17914.66 & 21822.11 &
36024.80 & 46634.38\ **Dec & 19756.21 & 28541.72 & 34061.01 & 30114.41 &
45060.69 & 80721.71 & 104660.67\
**************************************

[(a)]

Produce a time plot of the data and describe the patterns in the graph.
Identify any unusual or unexpected fluctuations in the time series.

Explain why it is necessary to take logarithms of these data before fitting a model.

Use R to fit a regression model to the logarithms of these sales data with a linear trend, seasonal dummies and a "surfing festival" dummy variable.

Plot the residuals against time and against the fitted values. Do these plots
reveal any problems with the model?

Do boxplots of the residuals for each month. Does this reveal any problems with
the model?

What do the values of the coefficients tell you about each variable?

What does the Durbin-Watson statistic tell you about your model?

Regardless of your answers to the above questions, use your regression model to
predict the monthly sales for 1994, 1995, and 1996. Produce prediction
intervals for each of your forecasts.

Transform your predictions and intervals to obtain predictions and intervals
for the raw data.

How could you improve these predictions by modifying the model?

The data below (data set `texasgas`) shows the demand for natural gas and the
price of natural gas for 20 towns in Texas in 1969.

<span>lcc</span> **<span>City</span> & **<span>Average price</span> $P$ &
**<span>Consumption per customer</span> $C$\ & **<span>(cents per thousand
cubic feet)</span> & **<span>(thousand cubic feet)</span>\ Amarillo & 30 & 134\
Borger & 31 & 112\ Dalhart & 37 & 136\ Shamrock & 42 & 109\ Royalty & 43 & 105\
Texarkana & 45 & 87\ Corpus Christi & 50 & 56\ Palestine & 54 & 43\ Marshall &
54 & 77\ Iowa Park & 57 & 35\ Palo Pinto & 58 & 65\ Millsap & 58 & 56\ Memphis
& 60 & 58\ Granger & 73 & 55\ Llano & 88 & 49\ Brownsville & 89 & 39\ Mercedes
& 92 & 36\ Karnes City & 97 & 46\ Mathis & 100 & 40\ La Pryor & 102 & 42\
**********

[(a)]=0.16cm

Do a scatterplot of consumption against price. The data are clearly not linear. Three possible nonlinear models for the data are given below
\begin{align*} 
  C_i &= \exp(a + bP_i+e_i) \\ 
  C_i &= \left\{
    \begin{array}{ll}
          a_1 + b_1P_i + e_i & \mbox{when $P_i \le 60$} \\ 
          a_2 + b_2P_i + e_i & \mbox{when $P_i > 60$;}
    \end{array}\right.\\
  C_i &= a + b_{1}P + b_{2}P^{2}.
\end{align*}

The second model divides the data into two sections, depending on whether the price is above or below 60 cents per 1,000 cubic feet.

Can you explain why the slope of the fitted line should change with $P$?

Fit the three models and find the coefficients, and residual variance in each case.

For the second model, the parameters $a_1$, $a_2$, $b_1$, $b_2$ can be estimated by simply fitting a regression with four regressors but no constant: (i) a dummy taking value 1 when $P\le60$ and 0 otherwise; (ii) $\text{P1} = P$ when $P\le60$ and 0 otherwise; (iii) a dummy taking value 0 when $P\le60$ and 1 otherwise; (iv) $\text{P2}=P$ when $P>60$ and 0 otherwise.

For each model, find the value of $R^2$ and AIC, and produce a residual plot. Comment on the adequacy of the three models.

For prices 40, 60, 80, 100, and 120 cents per 1,000 cubic feet, compute the forecasted per capita demand using the best model of the three above.

Compute 95% prediction intervals. Make a graph of these prediction intervals and discuss their interpretation.

What is the correlation between $P$ and $P^{2}$? Does this suggest any general problem to be considered in dealing with polynomial regressions---especially of higher orders?




[Linear trend]\@ref(ex-4-LinTrend)

A common feature of time series data is a trend. Using regression we can model and forecast the trend in time series data by including $t=1,\dots,T,$ as a predictor variable: 

$$  y_t=\beta_0+\beta_1t+\varepsilon_t.
  $$

Figure \@ref(fig-4-Tourism) shows a time series plot of aggregate tourist arrivals to Australia over the period 1980 to 2010 with the fitted linear trend line $\hat{y}_t= 0.3375+0.1761t$. Also plotted are the point and forecast intervals for the years 2011 to 2015.

![Forecasting international tourist arrivals to Australia for the period
2011-2015 using a linear trend. 80% and 95% forecast intervals are
shown.](fig_4_Tourism)

\@ref(fig-4-Tourism)

fit.ex4 \ <-  tslm(austa   trend) f \ <-  forecast(fit.ex4, h=5,level=c(80,95))
plot(f, ylab="International tourist arrivals to Australia (millions)",
xlab="t") lines(fitted(fit.ex4),col="blue") summary(fit.ex4)

Coefficients: Estimate Std. Error t value Pr(\>|t|) (Intercept) 0.337535
0.100366 3.363 0.00218 \*\* trend 0.176075 0.005475 32.157 \< 2e-16
\*\*\*




## Further reading





Regression with cross-sectional data

 * Chatterjee, S. and A. S. Hadi (2012). Regression analysis by example. 5th ed. New York: John Wiley & Sons.
 * Fox, J. and H. S. Weisberg (2010). An R Companion to Applied Regression. SAGE Publications, Inc.
 * Harrell, Jr, F. E. (2001). Regression modelling strategies: with applications to linear models, logistic regression, and survival analysis. New York: Springer.
 * Pardoe, I. (2006). Applied regression modeling: a business approach. Hoboken, NJ: John Wiley & Sons.
 * Sheather, S. J. (2009). A modern approach to regression with R. New York: Springer.

Regression with time series data

 * Shumway, R. H. and D. S. Stoffer (2011). Time series analysis and its applications: with R examples. 3rd ed. New York: Springer.
