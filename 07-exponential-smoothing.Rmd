#Exponential smoothing {#expsmooth}

Exponential smoothing was proposed in the late 1950s (Brown 1959, Holt 1957 and Winters 1960 are key pioneering works) and has motivated some of the most successful forecasting methods. Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series which is a great advantage and of major importance to applications in industry.

This chapter is divided into two parts. In the first part (Sections \@ref(sec-7-1-SES)--\@ref(sec-7-6-Taxonomy)) we present in detail the mechanics of the most important exponential smoothing methods and their application in forecasting time series with various characteristics. This is key in understanding the intuition behind these methods. In this setting, selecting and using a forecasting method may appear to be somewhat ad hoc. The selection of the method is generally based on recognising key components of the time series (trend and seasonal) and how these enter the smoothing method (e.g., in an additive, damped or multiplicative manner).

In the second part of the chapter (Section \@ref(sec-7-ETS)) we present statistical models that underlie exponential smoothing methods. These models generate identical point forecasts to the methods discussed in the first part of the chapter, but also generate prediction intervals. Furthermore, this statistical framework allows for genuine model selection between competing models.

##Simple exponential smoothing {#sec-7-1-SES}

The simplest of the exponentially smoothing methods is naturally called "simple exponential smoothing" (SES)^[In some books it is called ``single exponential smoothing'']. This method is suitable for forecasting data with no trend or seasonal pattern. For example, the data in Figure \@ref(fig:7-oil) do not display any clear trending behaviour or any seasonality, although the mean of the data may be changing slowly over time. We have already considered the naïve and the average as possible methods for forecasting such data (Section \@ref(sec-2-methods)).

```{r 7-oil, fig.cap="Oil production in Saudi Arabia from 1996 to 2007."}
oildata <- window(oil,start=1996,end=2007) 
autoplot(oildata) + 
  ylab("Oil (millions of tonnes)") + xlab("Year")
```

Using the naïve method, all forecasts for the future are equal to the last observed value of the series, 
$$
  \hat{y}_{T+h|T} = y_{T},
$$ 
for $h=1,2,\dots$. Hence, the naïve method assumes that the most current observation is the only important one and all previous observations provide no information for the future. This can be thought of as a weighted average where all the weight is given to the last observation.

Using the average method, all future forecasts are equal to a simple average of the observed data,
$$
  \hat{y}_{T+h|T} = \frac1T \sum_{t=1}^T y_t,
$$ 
for $h=1,2,\dots$. Hence, the average method assumes that all observations are of equal importance and they are given equal weight when generating forecasts.

We often want something between these two extremes. For example it may be sensible to attach larger weights to more recent observations than to observations from the distant past. This is exactly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages where the weights decrease exponentially as observations come from further in the past --- the smallest weights are associated with the oldest observations:
\begin{equation}
  \label{eq-7-ses}
  \hat{y}_{T+1|T} = \alpha y_T + \alpha(1-\alpha) y_{T-1} + \alpha(1-\alpha)^2 y_{T-2}+ \alpha(1-\alpha)^3 y_{T-3}+\cdots,
\end{equation}
where $0 \le \alpha \le 1$ is the smoothing parameter. The one-step-ahead forecast for time $T+1$ is a weighted average of all the observations in the series $y_1,\dots,y_T$. The rate at which the weights decrease is controlled by the parameter $\alpha$.

Table \@ref(tab:alpha) shows the weights attached to observations for four different values of $\alpha$ when forecasting using simple exponential smoothing. Note that the sum of the weights even for a small $\alpha$ will be approximately one for any reasonable sample size.

```{r alpha, echo=FALSE}
tab <- as.data.frame(matrix(NA,nrow=6,ncol=4))
rownames(tab) <- c("$y_{T}$", paste("$y_{T-",1:5,"}$",sep=''))
alpha <- c(0.2,0.4,0.6,0.8)
colnames(tab) <- paste("$\\alpha=",alpha,"$",sep="")
for(i in 1:6)
  tab[i,] <- alpha*(1-alpha)^(i-1)
knitr::kable(tab, digits=4,booktabs=TRUE, 
             caption="Exponentially decaying weights attached to observations of the time series when generating forecasts using simple exponential smoothing.")
```

For any $\alpha$ between 0 and 1, the weights attached to the observations decrease exponentially as we go back in time, hence the name "exponential smoothing". If $\alpha$ is small (i.e., close to 0), more weight is given to observations from the more distant past. If $\alpha$ is large (i.e., close to 1), more weight is given to the more recent observations. At the extreme case where $\alpha=1$, $\hat{y}_{T+1|T}=y_T$ and forecasts are equal to the naïve forecasts.

We present two equivalent forms of simple exponential smoothing, each of which leads to the forecast equation (\@ref{eq-7-ses}).

### Weighted average form {-}

The forecast at time $t+1$ is equal to a weighted average between the most recent observation $y_t$ and the most recent forecast $\hat{y}_{t|t-1}$,
$$
 \hat{y}_{t+1|t} = \alpha y_t + (1-\alpha) \hat{y}_{t|t-1}
$$
for $t=1,\dots,T$, where $0 \le \alpha \le 1$ is the smoothing parameter.

The process has to start somewhere, so we let the first forecast of $y_1$ be denoted by $\ell_0$ (which we will have to estimate). Then
\begin{align*}
  \hat{y}_{2|1} &= \alpha y_1 + (1-\alpha) \ell_0\\
  \hat{y}_{3|2} &= \alpha y_2 + (1-\alpha) \hat{y}_{2|1}\\
  \hat{y}_{4|3} &= \alpha y_3 + (1-\alpha) \hat{y}_{3|2}\\
  \vdots\\
  \hat{y}_{T+1|T} &= \alpha y_T + (1-\alpha) \hat{y}_{T|T-1}
\end{align*}
Substituting each equation into the following equation, we obtain
\begin{align*}
  \hat{y}_{3|2}   & = \alpha y_2 + (1-\alpha) \left[\alpha y_1 + (1-\alpha) \ell_0\right]              \\
                 & = \alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0                          \\
  \hat{y}_{4|3}   & = \alpha y_3 + (1-\alpha) [\alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0]\\
                 & = \alpha y_3 + \alpha(1-\alpha) y_2 + \alpha(1-\alpha)^2 y_1 + (1-\alpha)^3 \ell_0 \\
                 & ~~\vdots                                                                           \\
  \hat{y}_{T+1|T} & =  \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} + (1-\alpha)^T \ell_{0}.
\end{align*}
So the weighted average form leads to the same forecast equation .

### Component form {-}

An alternative representation is the component form. For simple exponential smoothing the only component included is the level, $\ell_t$. (Other methods considered later in this chapter may also include a trend $b_t$ and seasonal component $s_t$.) Component form representations of exponential smoothing methods comprise a forecast equation and a smoothing equation for each of the components included in the method. The component form of simple exponential smoothing is given by:
\begin{align*}
  \text{Forecast equation}  && \hat{y}_{t+1|t} & = \ell_{t}\\
  \text{Smoothing equation} && \ell_{t}        & = \alpha y_{t} + (1 - \alpha)\ell_{t-1},
\end{align*}
where $\ell_{t}$ is the level (or the smoothed value) of the series at time $t$. The forecast equation shows that the forecasted value at time $t+1$ is the estimated level at time $t$. The smoothing equation for the level (usually referred to as the level equation) gives the estimated level of the series at each period $t$.

Applying the forecast equation for time $T$ gives, $\hat{y}_{T+1|T} = \ell_{T}$, the most recent estimated level.

If we replace $\ell_t$ by $\hat{y}_{t+1|t}$ and $\ell_{t-1}$ by $\hat{y}_{t|t-1}$ in the smoothing equation, we will recover the weighted average form of simple exponential smoothing.

### Multi-horizon Forecasts {-}

So far we have given forecast equations for only one step ahead. Simple exponential smoothing has a "flat" forecast function, and therefore for longer forecast horizons, 
$$
 \hat{y}_{T+h|T} = \hat{y}_{T+1|T}=\ell_T, \qquad h=2,3,\dots.
$$ 
Remember these forecasts will only be suitable if the time series has no trend or seasonal component.

### Optimization {-}

The application of every exponential smoothing method requires the smoothing parameters and the initial values to be chosen. In particular, for simple exponential smoothing, we need to select the values of $\alpha$ and $\ell_0$. All forecasts can be computed from the data once we know those values. For the methods that follow there is usually more than one smoothing parameter and more than one initial component to be chosen.

There are cases where the smoothing parameters may be chosen in a subjective manner --- the forecaster specifies the value of the smoothing parameters based on previous experience. However, a more robust and objective way to obtain values for the unknown parameters included in any exponential smoothing method is to estimate them from the observed data.

In Section \@ref(sec-4-2-LSprinciple) we estimated the coefficients of a regression model by minimizing the sum of the squared errors (SSE). Similarly, the unknown parameters and the initial values for any exponential smoothing method can be estimated by minimizing the SSE. The errors are specified as $e_t=y_t - \hat{y}_{t|t-1}$ for $t=1,\dots,T$ (the one-step-ahead training errors). Hence we find the values of the unknown parameters and the initial values that minimize
$$
 \text{SSE}=\sum_{t=1}^T(y_t - \hat{y}_{t|t-1})^2=\sum_{t=1}^Te_t^2. \label{eq-7-SSE}
$$

Unlike the regression case (where we have formulae that return the values of the regression coefficients which minimize the SSE) this involves a non-linear minimization problem and we need to use an optimization tool to perform this.

```{r ses, fig.cap="Simple exponential smoothing applied to oil production in Saudi Arabia (1996--2007)."}
fit <- ses(oildata, h=3) 
tmp <- cbind(Data=oildata,Forecasts=fit$mean, Fitted=fitted(fit))
autoplot(tmp) + ylab("Oil (millions of tonnes)") + xlab("Year") 
```

```{r, echo=FALSE}
# Data set for table
x <- oildata
# Generate forecasts
fit <- ses(x, h=3)
# Now set up the table
n <- length(x)
year0 <- min(time(x))-1
tab <- matrix(NA,nrow=n+6,ncol=5)
colnames(tab) <- c("Year","Time","Observation","Level","Forecast")
tab[2:(n+6),1] <- year0 + 0:(n+4)
tab[2:(n+6),2] <- 0:(n+4)
# Add data, level and fitted values
tab[3:(n+2),3] <- x
tab[2:(n+2),4] <- fit$model$state
tab[3:(n+2),5] <- fitted(fit)
# Add forecasts
tab[n+(4:6),1] <- max(time(x))+1:3
tab[n+(4:6),2] <- 1:3
tab[n+(4:6),5] <- fit$mean
# Convert to characters
tab <- as.data.frame(tab)
class(tab$Year) <- class(tab$Time) <- "integer"
tab <- format(tab, digits=5)
# Remove missing values
tab <- apply(tab, 2, function(x){j <- grep("[ ]*NA",x); x[j] <- ""; return(x)})
# Add math notation rows
tab[1,] <- c("","$t$","$y_t$","$\\ell_t$","$\\hat{y}_{t+1|t}$")
tab[n+3,] <- c("","$h$","","","$\\hat{y}_{T+h|T}$")
# Show table
knitr::kable(tab)
```

```{r, echo=FALSE}
# Accuracy of one-step-ahead training errors over period 1--12
tmp <- accuracy(fit)
print(round(c(tmp[,c("MAE","RMSE","MAPE")],SSE=sum(residuals(fit)^2)),1))
alpha <- fit$model$par[1]
l0 <- fit$model$par[2]
```
$\alpha=`r format(alpha,digits=2,nsmall=2)`$ and 
$\ell_0=`r format(l0,digits=1,nsmall=1)`$ 
are obtained by minimizing SSE over periods $t=1,2,\dots,12$.

In this example, simple exponential smoothing is applied to forecast oil production in Saudi Arabia. The black line in Figure \@ref(fig-7-ses) is a plot of the data over the period 1996--2007, which shows a changing level over time but no obvious trending behaviour.

In Table \@ref(tbl-7-ses) we demonstrate the application of simple exponential smoothing. The second last column shows the estimated level for times $t=0$ to $t=12$; the last column shows the forecasts for $h=1,2,3$. Using an optimization tool, we find the values of $\alpha=0.89$ and $\ell_0=447.5$ that minimize the SSE, subject to the restriction that $0\le\alpha\le1$. So the SSE value presented in the last row of the table is smaller for these values of $\alpha$ and $\ell_0$ than for any other values of $\alpha$ and $\ell_0$.

The forecasts for the period 2008--2010 are plotted in
Figure \@ref(fig-7-ses). Also plotted are one-step-ahead training forecasts
alongside the data over the period 1996--2007.

## Trend methods

###Holt’s linear trend method 

Holt (1957) extended simple exponential smoothing to allow forecasting of data with a trend. This method involves a forecast equation and two smoothing equations (one for the level and one for the trend):
\begin{align*}
  \text{Forecast equation}&& \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} \\
  \text{Level equation}   && \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  \text{Trend equation}   && b_{t}    &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)b_{t-1}
\end{align*}
where $\ell_t$ denotes an estimate of the level of the series at time $t$, $b_t$ denotes an estimate of the trend (slope) of the series at time $t$, $\alpha$ is the smoothing parameter for the level, $0\le\alpha\le1$ and $\beta^*$ is the smoothing parameter for the trend, $0\le\beta^*\le1$ (we denote this as $\beta^*$ instead of $\beta$ for reasons that will be explained in Section \@ref(sec-7-ETS)).

As with simple exponential smoothing, the level equation here shows that $\ell_t$ is a weighted average of observation $y_t$ and the one-step-ahead training forecast for time $t$, here given by $\ell_{t-1} + b_{t-1}$. The trend equation shows that $b_t$ is a weighted average of the estimated trend at time $t$ based on $\ell_{t} - \ell_{t-1}$ and $b_{t-1}$, the previous estimate of the trend.

The forecast function is no longer flat but trending. The $h$-step-ahead forecast is equal to the last estimated level plus $h$ times the last estimated trend value. Hence the forecasts are a linear function of $h$.

### Example: Air Passengers {-}

In Table \@ref(tbl-7-Holts) we demonstrate the application of Holt’s linear method to air transportation data for all passengers with an Australian airline.

The smoothing parameters, $\alpha$ and $\beta$, and the initial values $\ell_0$ and $b_0$ are estimated by minimizing the SSE for the one-step training errors as in Section \@ref(sec-7-1-SES).

```{r
air <- window(ausair,start=1990,end=2011) 
fit <- holt(air, h=5)
# Resulting forecasts:
fit$model$state 
fitted(fit) 
fit[["mean"]]
```

```{r, echo=FALSE}
# Data set for table
air <- window(ausair,start=1990,end=2011) 
# Generate forecasts`
fit <- holt(air, h=5)
# Now set up table
tmp <- cbind(air, fit$model$state, fitted(fit))
tsp(tmp) <- NULL
tmp <- cbind(1989:2011,0:(NROW(tmp)-1),tmp)
tmp <- rbind(rep(NA,6),tmp, matrix(NA, nrow=6, ncol=ncol(tmp)))
colnames(tmp) <- c("Year","Time","Observation","Level","Slope","Forecast")
# Add in forecasts
tmp[26:30,6] <- fit$mean
tmp[26:30,2] <- 1:5
# Convert to characters
tmp <- as.data.frame(tmp)
class(tmp[["Year"]]) <- class(tmp[["Time"]]) <- "integer"
tmp <- format(tmp, digits=4)
# Remove missing values
tmp <- apply(tmp, 2, function(x){j <- grep("[ ]*NA",x); x[j] <- ""; return(x)})
# Add math notation
tmp[1,] <- c("","t","$y_t$","$\\ell_t$",
                   "$b_t$", "$\\hat{y}_{t|t-1}$")
tmp[25,] <- c("","h","","","", "$\\hat{y}_{t+h|t}$")
knitr::kable(tmp, booktabs=TRUE, 
             caption="Applying Holt's linear method with $\\alpha=0.7674$ and $\\beta^*=0.0001$ to Australian Air Passenger Data (thousands of passengers).")
```

```{r fig_7_trend, fig.cap="Forecasting Air Passengers in Australia (thousands of passengers). For all methods $\\alpha=0.8$ and $\\beta^*=0.0001$, and for the additive damped trend method $\\phi=0.85$."}

fit2 <- holt(air, damped=TRUE, h=5, phi=0.95) 
tmp <- cbind(Data=air, 
             "Holt's method"=fit[["mean"]], 
             "Damped Holt's method" = fit2[["mean"]])
autoplot(tmp) +
  ggtitle("Forecasts from Holt's method") +
  xlab("Year") + ylab("Air passengers in Australia (thousands)") +
  scale_color_manual(values=c('#d95f02','#000000','#1b9e77'),
                     breaks=c("Data","Holt's method","Damped Holt's method"),
                     name="")
```

###Damped trend methods

The forecasts generated by Holt’s linear method display a constant trend (increasing or decreasing) indefinitely into the future.

Empirical evidence indicates that these methods tend to over-forecast, especially for longer forecast horizons. Motivated by this observation, Gardner and McKenzie (1985) introduced a parameter that "dampens" the trend to a flat line some time in the future. Methods that include a damped trend have proven to be very successful and are arguably the most popular individual methods when forecasts are required automatically for many series.

In conjunction with the smoothing parameters $\alpha$ and $\beta^*$ (with values between 0 and 1 as in Holt’s method), this method also includes a damping parameter $0<\phi<1$:
\begin{align*}
  \hat{y}_{t+h|t} &= \ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t} \\
  \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)\phi b_{t-1}.
\end{align*}
If $\phi=1$ the method is identical to Holt’s linear method. For values between $0$ and $1$, $\phi$ dampens the trend so that it approaches a constant some time in the future. In fact the forecasts converge to $\ell_T+\phi b_T/(1-\phi)$ as $h\rightarrow\infty$ for any value $0<\phi<1$. The effect of this is that short-run forecasts are trended while long-run forecasts are constant.

### Example: Air Passengers (continued) {-}

Figure \@ref(fig-7-trend) shows the one-step training forecasts, and the forecasts for years 2005--2010 generated from Holt’s linear trend method, and additive damped trend. The most optimistic forecasts come from the exponential trend method while the least optimistic come from the damped trend method, with the forecasts generated by Holt’s linear trend method somewhere between the two.

### Example: Sheep in Asia {-}

In this example we compare the forecasting performance of the three exponential smoothing methods we have considered so far in forecasting the sheep livestock population in Asia. The data spans the period 1970--2007. We withhold the period 2001--2007 as a test set, and use the data up to and including year 2000 for the training set (see Section \@ref(accuracy) for a definition of training and test sets). Figure \@ref(fig-7-comp) shows that data and the forecasts from all methods.

The parameters and initial values of the methods are estimated for all methods by minimizing SSE (as specified in Equation ) over the training set. In Table \@ref(tbl-7-comp) we present the estimation results and error measures over the training and the test sets.

```{r}
livestock2 <- window(livestock,start=1970,end=2000) 
fit1 <- ses(livestock2) 
fit2 <- holt(livestock2) 
fit3 <- holt(livestock2,damped=TRUE) 
# Results for first model: 
fit1[["model"]]
# training set 
accuracy(fit1) 
# test set
accuracy(fit1,livestock) 
```

```{r echo=FALSE}
tab <- matrix(NA, ncol=3,nrow=10)
colnames(tab) <- c("SES","Linear trend","Damped trend")
rownames(tab) <- c("$\\alpha$","$\\beta^*$","$\\phi$","$\\ell_0$","$b_0$",
                   "Training RMSE","Test RMSE","Test MAE","Test MAPE","Test MASE")
# SSE
tab[1,1] <- fit1$model$par["alpha"]
tab[4,1] <- fit1$model$par["l"]
tab[6,1] <- sqrt(fit1$model$mse)
tab[c(7:10),1] <- accuracy(fit1,livestock)["Test set",c("RMSE","MAE","MAPE","MASE")]
# Holt
tab[1,2] <- fit2$model$par["alpha"]
tab[2,2] <- fit2$model$par["beta"]/fit1$model$par["alpha"]
tab[4,2] <- fit2$model$par["l"]
tab[5,2] <- fit2$model$par["b"]
tab[6,2] <- sqrt(fit2$model$mse)
tab[c(7:10),2] <- accuracy(fit2,livestock)["Test set",c("RMSE","MAE","MAPE","MASE")]
# Damped trend
tab[1,3] <- fit3$model$par["alpha"]
tab[2,3] <- fit3$model$par["beta"]/fit1$model$par["alpha"]
tab[3,3] <- fit3$model$par["phi"]
tab[4,3] <- fit3$model$par["l"]
tab[5,3] <- fit3$model$par["b"]
tab[6,3] <- sqrt(fit3$model$mse)
tab[c(7:10),3] <- accuracy(fit3,livestock)["Test set",c("RMSE","MAE","MAPE","MASE")]
# Convert to characters
tab <- as.data.frame(formatC(tab, format="f", digits=2))
# Remove missing values
tab <- apply(tab, 2, function(x){j <- grep("[ ]*NA",x); x[j] <- ""; return(x)})
# Show table
knitr::kable(tab, booktabs=TRUE, 
             caption="Estimation results from models for Sheep in Asia")

```

For the simple exponential smoothing method, the estimated smoothing parameter is $\alpha=1$. This is expected as the series is clearly trending over time and simple exponential smoothing requires the largest possible adjustment in each step to capture this trend.

```{r fig-7-leveltrend, fig.cap="Level and slope components for Holt’s linear trend method and the additive damped trend method."}
plot(fit2$model$state) 
plot(fit3$model$state)
```

```{r fig-7-comp, fig.cap="Forecasting livestock, sheep in Asia: comparing forecasting performance of non-seasonal method."}
tmp <- cbind(Data=window(livestock, start=1970),
  SES=fit1$mean, "Holt's"=fit2$mean, "Damped trend"=fit3$mean)
autoplot(tmp) + xlab("Year") + 
  ylab("Livestock, sheep in Asia (millions)") +
  scale_color_manual(name="",
    values=c("#dd0000","#000000","#00dd00","#0000dd"),
    breaks=c("Data","SES","Holt's","Damped trend"))
```

For the other methods, there is also a trend component.The smoothing parameter for the slope parameter is estimated to be zero, indicating that the trend is not changing over time. Of course, the trend estimated using the damped trend methods will change in the future due to the damping.

In Figure \@ref(fig-7-leveltrend), we plot the level and trend components for Holt’s method and for the damped trend method. The slope of the trend component for Holt’s method is constant, showing that the trend is linear. In contrast, the slope of the trend component for the damped trend method is decreasing, showing that the trend is levelling off.

For the damped trend method, the damping parameter $\phi$ is restricted to a maximum of 0.98 (the estimation returned an optimal value of $\phi=1$). This restriction is imposed to ensure that the damped trend method generates noticeably different forecasts from Holt’s linear method, otherwise we get identical forecasts.

The SSE measures calculated over the training set show that Holt’s linear trend method provides the best fit to the data followed by the additive damped trend method. Simple exponential smoothing generates the largest one-step training errors. In Figure \@ref(fig-7-comp) we can examine the forecasts generated by the methods. Pretending that we have not seen the data over the test-set we would conclude that all forecasts are quite plausible especially from the methods that account for the trend in the data.

Comparing the forecasting performance of the methods over the test set in Table \@ref(tbl-7-comp), ??? is the most accurate method according to the MAE, MAPE and MASE, while Holt’s linear method is most accurate according to the RMSE.

Conflicting results like this are very common when performing forecasting competitions between methods. As forecasting tasks can vary by many dimensions (length of forecast horizon, size of test set, forecast error measures, frequency of data, etc.), it is unlikely that one method will be better than all others for all forecasting scenarios. What we require from a forecasting method are consistently sensible forecasts, and these should be frequently evaluated against the task at hand.

##Holt-Winters seasonal method {#sec-7-Taxonomy}

@Holt57 and @Winters60 extended Holt’s method to capture seasonality. The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations --- one for the level $\ell_t$, one for trend $b_t$, and one for the seasonal component denoted by $s_t$, with smoothing parameters $\alpha$, $\beta^*$ and $\gamma$. We use $m$ to denote the period of the seasonality, i.e., the number of seasons in a year. For example, for quarterly data $m=4$, and for monthly data $m=12$.

There are two variations to this method that differ in the nature of the seasonal component. The additive method is preferred when the seasonal variations are roughly constant through the series, while the multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series. With the additive method, the seasonal component is expressed in absolute terms in the scale of the observed series, and in the level equation the series is seasonally adjusted by subtracting the seasonal component. Within each year the seasonal component will add up to approximately zero. With the multiplicative method, the seasonal component is expressed in relative terms (percentages) and the series is seasonally adjusted by dividing through by the seasonal component. Within each year, the seasonal component will sum up to approximately $m$.

### Holt-Winters additive method {-}

The component form for the additive method is:
\begin{align*}
  \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} + s_{t-m+h_{m}^{+}} \\
  \ell_{t} &= \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)b_{t-1}\\
  s_{t} &= \gamma (y_{t}-\ell_{t-1}-b_{t-1}) + (1-\gamma)s_{t-m},
\end{align*}
where $h_{m}^{+}=\lfloor(h-1)\mod~m\rfloor+1$,^[The notation $\lfloor u \rfloor$ means the largest integer not greater than $u$.] which ensures that the estimates of the seasonal indices used for forecasting come from the final year of the sample. The level equation shows a weighted average between the seasonally adjusted observation $(y_{t} - s_{t-m})$ and the non-seasonal forecast $(\ell_{t-1}+b_{t-1})$ for time $t$. The trend equation is identical to Holt’s linear method. The seasonal equation shows a weighted average between the current seasonal index, $(y_{t}-\ell_{t-1}-b_{t-1})$, and the seasonal index of the same season last year (i.e., $m$ time periods ago).

The equation for the seasonal component is often expressed as
$$
 s_{t} = \gamma^* (y_{t}-\ell_{t})+ (1-\gamma^*)s_{t-m}.
$$ 
If we substitute $\ell_t$ from the smoothing equation for the level of the component form above, we get
$$
 s_{t} = \gamma^*(1-\alpha) (y_{t}-\ell_{t-1}-b_{t-1})+ [1-\gamma^*(1-\alpha)]s_{t-m}
$$
which is identical to the smoothing equation for the seasonal component we specify here with $\gamma=\gamma^*(1-\alpha)$. The usual parameter restriction is $0\le\gamma^*\le1$, which translates to $0\le\gamma\le 1-\alpha$.

### Holt-Winters multiplicative method {-}

The component form for the multiplicative method is:
\begin{align*}
  \hat{y}_{t+h|t} &= (\ell_{t} + hb_{t})s_{t-m+h_{m}^{+}}. \\  
  \ell_{t} &= \alpha \frac{y_{t}}{s_{t-m}} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t}-\ell_{t-1}) + (1 - \beta^*)b_{t-1}                \\
  s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + b_{t-1})} + (1 - \gamma)s_{t-m}
\end{align*}

### Example: International tourist visitor nights in Australia {-}

In this example we employ the Holt-Winters method with both additive and multiplicative seasonality to forecast tourists visitor nights in Australia by international arrivals. Figure \@ref(fig:7-HW) shows the data and the forecasts for the period 2016Q1--2017Q4. The data show an obvious seasonal pattern with peaks observed in the March quarter of each year as this corresponds to the Australian summer.

```{r 7-HW, fig.cap="Forecasting international visitor nights in Australia using Holt-Winters method with both additive and multiplicative seasonality."}
aust <- window(austourists,start=2005) 
fit1 <- hw(aust,seasonal="additive") 
fit2 <- hw(aust,seasonal="multiplicative")

tmp <- cbind(Data=aust, 
  "HW additive forecasts" = fit1[["mean"]], 
  "HW multiplicative forecasts" = fit2[["mean"]])

autoplot(tmp) + xlab("Year") +
  ylab("International visitor night in Australia (millions)") +
  scale_color_manual(name="",
    values=c('#000000','#1b9e77','#d95f02'),
    breaks=c("Data","HW additive forecasts","HW multiplicative forecasts"))
```

The application of the method with additive and multiplicative seasonality are presented in Tables \@ref(tbl-7-HWadd) and \@ref(tbl-7-HWmulti) respectively. The results show that the method with the multiplicative seasonality fits the data best. This was expected as the time plot shows the seasonal variation in the data increases as the level of the series increases. This is also reflected in the two sets of forecasts; the forecasts generated by the method with the multiplicative seasonality portray larger and increasing seasonal variation as the level of the forecasts increases compared to the forecasts generated by the method with additive seasonality.

The smoothing parameters and initial estimates for the components have been estimated by minimizing SSE ($\alpha=0.025$, $\beta^*=0.023$, $\gamma=0$ and SSE$=60.27$, RMSE$=1.585$).

The smoothing parameters and initial estimates for the components have been estimated by minimizing SSE ($\alpha=0$, $\beta^*=0$ and $\gamma=0$ and SSE$=34.6$, RMSE$=1.201$).

```{r fig-7-LevelTrendSeas, fig.cap="Estimated components for Holt-Winters method with additive and multiplicative seasonal components.", echo=FALSE}
addstates <- fit1$model$states[,1:3]
multstates <- fit2$model$states[,1:3]
colnames(addstates) <- colnames(multstates) <-
  c("level","slope","season")
p1 <- autoplot(addstates, facets=TRUE) + xlab("Year") +
  ylab("") + ggtitle("Additive states")
p2 <- autoplot(multstates, facets=TRUE) + xlab("Year") +
  ylab("") + ggtitle("Multiplicative states")
gridExtra::grid.arrange(p1,p2,ncol=2)
```

```
fit1$model$state[,1:3] 
fitted(fit1) 
fit1[["mean"]]
```

```{r echo=FALSE}
tab7.5 <- cbind(-3:44,c(rep(NA,4),aust),
        rbind(matrix(NA,3,3),
        as.matrix(as.data.frame(fit1$model$states[,1:3]))),
        c(rep(NA,4),fitted(fit1)))
tab7.5[1:3,5] <- fit1$model$states[1,4:6]
colnames(tab7.5) <- c("$t$","$y_t$","$\\ell_t$","$b_t$","$s_t$",
                   "$\\hat{y}_t$")

knitr::kable(tab7.5,digits=2)
```

### Holt-Winters damped method {-}

A method that is often the single most accurate forecasting method for seasonal data is the Holt-Winters method with a damped trend and multiplicative seasonality:

\begin{align*}
  \hat{y}_{t+h|t} &= \left[\ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t}\right]s_{t-m+h_{m}^{+}}. \\
  \ell_{t} &= \alpha(y_{t} / s_{t-m}) + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1}             \\
  s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + \phi b_{t-1})} + (1 - \gamma)s_{t-m}
\end{align*}

```
hw(x, damped=TRUE, seasonal="multiplicative")
```

##A taxonomy of exponential smoothing methods {#sec-7-6-Taxonomy}

Exponential smoothing methods are not restricted to those we have presented so far. By considering variations in the combination of the trend and seasonal components, fifteen exponential smoothing methods are possible, listed in Table \@ref(tbl-7-classification). Each method is labelled by a pair of letters (T,S) defining the type of ‘Trend’ and ‘Seasonal’ components. For example, (A,M) is the method with an additive trend and multiplicative seasonality; (M,N) is the method with multiplicative trend and no seasonality; and so on.


Table: A two way classification of exponential smoothing methods.

|:---------------------|:-------:|:----------:|:----------------:| 
|                      | Seasonal Component   |                  |
| Trend                | N       | A          | M                |
| Component            | (None)  | (Additive) | (Multiplicative) |
| N (None)             | (N,N)   | (N,A)      | (N,M)            |
| A (Additive)         | (A,N)   | (A,A)      | (A,M)            |
| Ad (Additive damped) | (Ad,N)  | (Ad,A)     | (Ad,M)           |


Some of these methods we have already seen:

|:-----:|:------------------------------------|
| (N,N) | simple exponential smoothing        |
| (A,N) | Holt’s linear method                |
| (A,N) | additive damped trend method        |
| (A,A) | additive Holt-Winters method        |
| (A,M) | multiplicative Holt-Winters method  |
| (A,M) | Holt-Winters damped method          |


\begin{table*}[!t]
\def\dampfactor{\phi_h}
\begin{small}\fontsize{7.5}{10}\rmfamily\tabcolsep=0.1cm
\begin{tabular}{clll} \toprule
{\bf Trend}   & \multicolumn{3}{c}{\bf Seasonal} \\
%\cmidrule{2-4}
& \multicolumn{1}{c}{\bf N} &
\multicolumn{1}{c}{\bf A}
& \multicolumn{1}{c}{\bf M}\\
\cmidrule{2-4}
    & $\hat{y}_{t+h|t} = \ell_t$
    & $\hat{y}_{t+h|t} = \ell_t + s_{t-m+h_m^+}$
    & $\hat{y}_{t+h|t}= \ell_ts_{t-m+h_m^+}$      \\[0.1cm]
{\bf N}
    & $\ell_t = \alpha y_t + (1-\alpha) \ell_{t-1}$
    & $\ell_t = \alpha (y_t - s_{t-m}) + (1-\alpha) \ell_{t-1}$
    & $\ell_t = \alpha (y_t / s_{t-m}) + (1-\alpha) \ell_{t-1}$ \\
    &
    & $s_t = \gamma (y_t - \ell_{t-1}) + (1-\gamma) s_{t-m}$
    & $s_t = \gamma (y_t / \ell_{t-1}) + (1-\gamma) s_{t-m}$                \\
\midrule
    & $\hat{y}_{t+h|t} = \ell_t+hb_t$
    & $\hat{y}_{t+h|t} = \ell_t +hb_t +s_{t-m+h_m^+}$
    & $\hat{y}_{t+h|t}= (\ell_t+hb_t)s_{t-m+h_m^+}$                         \\[0.1cm]
{\bf A}
    & $\ell_t = \alpha y_t + (1-\alpha) (\ell_{t-1}+b_{t-1})$
    & $\ell_t = \alpha (y_t - s_{t-m}) + (1-\alpha)(\ell_{t-1}+b_{t-1})$
    & $\ell_t = \alpha (y_t / s_{t-m}) + (1-\alpha)(\ell_{t-1}+b_{t-1})$    \\

    & $b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) b_{t-1}$
    & $b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) b_{t-1}$
    & $b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) b_{t-1}$             \\
    &
    & $s_t = \gamma (y_t - \ell_{t-1}-b_{t-1}) + (1-\gamma)s_{t-m}$
    & $s_t = \gamma (y_t / (\ell_{t-1}+b_{t-1})) + (1-\gamma)s_{t-m}$\\
\midrule
    & $\hat{y}_{t+h|t} = \ell_t+\dampfactor b_t$
    & $\hat{y}_{t+h|t} = \ell_t+\dampfactor b_t+s_{t-m+h_m^+}$
    & $\hat{y}_{t+h|t}= (\ell_t+\dampfactor b_t)s_{t-m+h_m^+}$ \\[0.1cm]
{\bf A$\damped$ }
    & $\ell_t = \alpha y_t + (1-\alpha) (\ell_{t-1}+\phi b_{t-1})$
    & $\ell_t = \alpha (y_t - s_{t-m}) + (1-\alpha) (\ell_{t-1}+\phi b_{t-1})$
    & $\ell_t = \alpha (y_t / s_{t-m}) + (1-\alpha) (\ell_{t-1}+\phi b_{t-1})$\\
    & $b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) \phi b_{t-1}$
    & $b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) \phi b_{t-1}$
    & $b_t = \beta^* (\ell_t-\ell_{t-1}) + (1-\beta^*) \phi b_{t-1}$\\
    &
    & $s_t = \gamma (y_t - \ell_{t-1}-\phi b_{t-1}) + (1-\gamma)s_{t-m}$
    & $s_t = \gamma (y_t / (\ell_{t-1}+\phi b_{t-1})) + (1-\gamma)s_{t-m}$\\
\bottomrule
\end{tabular}\vspace{0.cm}
\end{small}


Table : Formulae for recursive calculations and point forecasts. In each case, $\ell_t$ denotes the series level at time $t$, $b_t$ denotes the slope at time $t$, $s_t$ denotes the seasonal component of the series at time $t$, and $m$ denotes the number of seasons in a year; $\alpha$, $\beta^*$, $\gamma$ and $\phi$ are smoothing parameters, $\phi_h = \phi+\phi^2+\dots+\phi^{h}$ and $h_m^+ = \lfloor(h-1) \text{~mod~} m\rfloor + 1$.

This type of classification was proposed by Pegels (1969). It was later extended by Gardner (1985) to include methods with additive damped trend and by Taylor (2003) to include methods with multiplicative damped trend. We do not consider the multiplicative trend methods in this book as they tend to produce poor forecasts. See Hyndman et al (2008) for a
more thorough discussion of all exponential smoothing methods.

Table \@ref(tbl-7-pegels) gives the recursive formulae for applying all nine possible exponential smoothing methods. Each cell includes the forecast equation for generating $h$-step-ahead forecasts and the smoothing equations for applying the method.

##Innovations state space models for exponential smoothing {#sec-7-ETS}

In the rest of this chapter we study statistical models that underlie the exponential smoothing methods we have considered so far. The exponential smoothing methods presented in Table \@ref(tbl-7-pegels) are algorithms that generate point forecasts. The statistical models in this section generate the same point forecasts, but can also generate prediction (or forecast) intervals. A statistical model is a stochastic (or random) data generating process that can produce an entire forecast distribution. The general statistical framework we will introduce also provides a platform for using the model selection criteria introduced in Chapter \@ref(ch5), thus allowing the choice of model to be made in an objective manner.

Each model consists of a measurement equation that describes the observed data and some transition equations that describe how the unobserved components or states (level, trend, seasonal) change over time. Hence these are referred to as "state space models".

For each method there exist two models: one with additive errors and one with multiplicative errors. The point forecasts produced by the models are identical if they use the same smoothing parameter values. They will, however, generate different prediction intervals.

To distinguish between a model with additive errors and one with multiplicative errors (and to also distinguish the models from the methods) we add a third letter to the classification of Table \@ref(tbl-7-classification). We label each state space model as ETS($\cdot,\cdot,\cdot$) for (Error, Trend, Seasonal). This label can also be thought of as ExponenTial Smoothing. Using the same notation as in Table \@ref(tbl-7-classification), the possibilities for each component are: Error $=\{$A,M$\}$, Trend $=\{$N,A,A$\}$ and Seasonal $=\{$N,A,M$\}$. Therefore, in total there exist 18 such state space models: 9 with additive errors and 9 with multiplicative errors.

### ETS(A,N,N): simple exponential smoothing with additive errors {-}

The third form of simple exponential smoothing is obtained by re-arranging the level equation in the component form to get what we refer to as the error correction form
\begin{align*}
\ell_{t} %&= \alpha y_{t}+\ell_{t-1}-\alpha\ell_{t-1}\\
         &= \ell_{t-1}+\alpha( y_{t}-\ell_{t-1})\\
         &= \ell_{t-1}+\alpha e_{t}
\end{align*}
where $e_{t}=y_{t}-\ell_{t-1}=y_{t}-\hat{y}_{t|t-1}$ for $t=1,\dots,T$. That is, $e_{t}$ is the one-step error at time $t$ computed on the training data. The training data errors lead to the adjustment/correction of the estimated level throughout the smoothing process for $t=1,\dots,T$.

For example, if the error at time $t$ is negative, then $\hat{y}_{t|t-1}>y_t$ and so the level at time $t-1$ has been over-estimated. The new level $\ell_t$ is then the previous level $\ell_{t-1}$ adjusted downwards. The closer $\alpha$ is to one the "rougher" the estimate of the level (large adjustments take place). The smaller the $\alpha$ the "smoother" the level (small adjustments take place).

As discussed in Section \@ref(sec-7-1-SES), the error correction form of simple exponential smoothing is given by
$$
  \ell_t=\ell_{t-1}+\alpha e_t,
$$
where $e_t = y_t - \ell_{t-1}$ and $\hat{y}_{t|t-1} = \ell_{t-1}$. Thus, $e_t = y_t - \hat{y}_{t|t-1}$ represents a one-step forecast error and we can write $y_t = \ell_{t-1} + e_t$.

To make this into an innovations state space model, all we need to do is specify the probability distribution for $e_t$. For a model with additive errors, we assume that one-step forecast errors $e_t$ are normally distributed white noise with mean 0 and variance $\sigma^2$. A short-hand notation for this is $e_t = \varepsilon_t\sim\text{NID}(0,\sigma^2)$; NID stands for "normally and independently distributed".

Then the equations of the model can be written
\begin{align}
  y_t &= \ell_{t-1} + \varepsilon_t \label{eq-ann-1a}\\
  \ell_t&=\ell_{t-1}+\alpha \varepsilon_t. \label{eq-ann-2a}
\end{align}
We refer to ??? as the *measurement* (or observation) equation and ??? as the *state* (or transition) equation. These two equations, together with the statistical distribution of the errors, form a fully specified statistical model. Specifically, these constitute an innovations state space model underlying simple exponential smoothing.

The term "innovations" comes from the fact that all equations in this type of specification use the same random error process, $\varepsilon_t$. For the same reason this formulation is also referred to as a "single source of error" model in contrast to alternative multiple source of error formulations, which we do not present here.

The measurement equation shows the relationship between the observations and the unobserved states. In this case observation $y_t$ is a linear function of the level $\ell_{t-1}$, the predictable part of $y_t$, and the random error $\varepsilon_t$, the unpredictable part of $y_t$. For other innovations state space models, this relationship may be nonlinear.

The transition equation shows the evolution of the state through time. The influence of the smoothing parameter $\alpha$ is the same as for the methods discussed earlier. For example $\alpha$ governs the degree of change in successive levels. The higher the value of $\alpha$, the more rapid the changes in the level; the lower the value of $\alpha$, the smoother the changes. At the lowest extreme, where $\alpha=0$, the level of the series does not change over time. At the other extreme, where $\alpha=1$, the model reduces to a random walk model, $y_t=y_{t-1}+\varepsilon_t$.

### ETS(M,N,N): simple exponential smoothing with multiplicative errors {-}

In a similar fashion, we can specify models with multiplicative errors by writing the one-step random errors as relative errors:
$$
  \varepsilon_t = \frac{y_t-\hat{y}_{t|t-1}}{\hat{y}_{t|t-1}}
$$
where $\varepsilon_t \sim \text{NID}(0,\sigma^2)$. Substituting $\hat{y}_{t|t-1}=\ell_{t-1}$ gives $y_t = \ell_{t-1}+\ell_{t-1}\varepsilon_t$ and $e_t = y_t - \hat{y}_{t|t-1} = \ell_{t-1}\varepsilon_t$.

Then we can write the multiplicative form of the state space model as
\begin{align*}
  y_t&=\ell_{t-1}(1+\varepsilon_t)\\
  \ell_t&=\ell_{t-1}(1+\alpha \varepsilon_t).
\end{align*}

### ETS(A,A,N): Holt’s linear method with additive errors {-}

For this model, we assume that one-step forecast errors are given by $\varepsilon_t=y_t-\ell_{t-1}-b_{t-1} \sim \text{NID}(0,\sigma^2)$. Substituting this into the error correction equations for Holt’s linear method we obtain
\begin{align*}
y_t&=\ell_{t-1}+b_{t-1}+\varepsilon_t\\
\ell_t&=\ell_{t-1}+b_{t-1}+\alpha \varepsilon_t\\
b_t&=b_{t-1}+\beta \varepsilon_t,
\end{align*}

where for simplicity we have set $\beta=\alpha \beta^*$.

### ETS(M,A,N): Holt’s linear method with multiplicative errors {-}

Specifying one-step forecast errors as relative errors such that 
$$
  \varepsilon_t=\frac{y_t-(\ell_{t-1}+b_{t-1})}{(\ell_{t-1}+b_{t-1})}
$$
and following a similar approach as above, the innovations state space model underlying Holt’s linear method with multiplicative errors is specified as
\begin{align*}
y_t&=(\ell_{t-1}+b_{t-1})(1+\varepsilon_t)\\
\ell_t&=(\ell_{t-1}+b_{t-1})(1+\alpha \varepsilon_t)\\
b_t&=b_{t-1}+\beta(\ell_{t-1}+b_{t-1}) \varepsilon_t
\end{align*}

where again $\beta=\alpha \beta^*$ and $\varepsilon_t \sim \text{NID}(0,\sigma^2)$.

### Other ETS models {-}

In a similar fashion, we can write an innovations state space model for each of the exponential smoothing methods of Table \@ref(tbl-7-pegels). Table [table:2a] presents the equations for all of the models in the ETS framework.


```
**Trend &\
& & &\
 **N** & $y_{t} = \ell_{t-1}+\varepsilon_t$ &
$y_{t} = \ell_{t-1} + s_{t-m}+\varepsilon_t$ &
$y_{t} = \ell_{t-1} s_{t-m}+\varepsilon_t$\
& $\ell_t = \ell_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + \alpha  \varepsilon_t/s_{t-m}$\
&&$s_t = s_{t-m} + \gamma  \varepsilon_t$ &
$s_t = s_{t-m} + \gamma  \varepsilon_t/\ell_{t-1}$\
& $y_{t} = \ell_{t-1} + b_{t-1}+\varepsilon_t$ &
$y_{t} = \ell_{t-1} + b_{t-1} + s_{ t-m}+\varepsilon_t$ &
$y_{t} = (\ell_{t-1}+b_{t-1})s_{t-m}+\varepsilon_t$\
**A** &
$\ell_t = \ell_{t-1} + b_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + b_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + b_{t-1} + \alpha
    \varepsilon_t/s_{t-m}$\
& $b_t = b_{t-1} + \beta  \varepsilon_t$ &
$b_t = b_{t-1} + \beta  \varepsilon_t$ &
$b_t = b_{t-1} + \beta  \varepsilon_t/s_{t-m}$\
& & $s_t =  s_{t-m} + \gamma  \varepsilon_t$ &
$s_t =  s_{t-m} + \gamma  \varepsilon_t/(\ell_{t-1}+b_{t-1})$\
& $y_{t} =  \ell_{t-1} + \phi b_{t-1}+\varepsilon_t$ &
$y_{t} =  \ell_{t-1} + \phi b_{t-1} + s_{t-m}+\varepsilon_t$ &
$y_{t} = (\ell_{t-1} + \phi b_{t-1})s_{t-m}+\varepsilon_t$\
**A** &
$\ell_t = \ell_{t-1} + \phi b_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + \phi b_{t-1} + \alpha  \varepsilon_t$ &
$\ell_t = \ell_{t-1} + \phi b_{t-1} + \alpha
    \varepsilon_t/s_{t-m}$\
& $b_t = \phi b_{t-1} + \beta  \varepsilon_t$ &
$b_t = \phi b_{t-1} + \beta  \varepsilon_t$ &
$b_t = \phi b_{t-1} + \beta  \varepsilon_t/s_{t-m}$\
& & $s_t =  s_{t-m} + \gamma  \varepsilon_t$ &
$s_t =  s_{t-m} + \gamma  \varepsilon_t/(\ell_{t-1}+\phi
    b_{t-1})$\
\
\
**Trend &\
& & &\
 **N** & $y_{t} = \ell_{t-1}(1 + \varepsilon_t)$ &
$y_{t} = (\ell_{t-1} + s_{t-m})(1 + \varepsilon_t)$ &
$y_{t} = \ell_{t-1} s_{t-m}(1 + \varepsilon_t)$\
& $\ell_t = \ell_{t-1}(1 + \alpha \varepsilon_t)$ &
$\ell_t = \ell_{t-1} + \alpha(\ell_{t-1} + s_{t-m})
    \varepsilon_t$ & $\ell_t = \ell_{t-1}(1 + \alpha \varepsilon_t)$\
&&$s_t = s_{t-m} + \gamma  (\ell_{t-1} + s_{t-m})\varepsilon_t$ &
$s_t = s_{t-m}(1 + \gamma  \varepsilon_t)$\
& $y_{t} = (\ell_{t-1} + b_{t-1})(1 + \varepsilon_t)$ &
$y_{t} = (\ell_{t-1} + b_{t-1} + s_{ t-m})(1 + \varepsilon_t)$ &
$y_{t} = (\ell_{t-1}+b_{t-1})s_{t-m}(1 + \varepsilon_t)$\
**A** &
$\ell_t = (\ell_{t-1} + b_{t-1})(1 + \alpha  \varepsilon_t)$ &
$\ell_t = \ell_{t-1} + b_{t-1} + \alpha (\ell_{t-1} + b_{t-1}+ s_{ t-m}) \varepsilon_t$
& $\ell_t = (\ell_{t-1} + b_{t-1})(1 + \alpha\varepsilon_t)$\
& $b_t = b_{t-1} + \beta (\ell_{t-1} + b_{t-1}) \varepsilon_t$ &
$b_t = b_{t-1} + \beta (\ell_{t-1} + b_{t-1} + s_{ t-m})\varepsilon_t$ &
$b_t = b_{t-1} + \beta (\ell_{t-1}+b_{t-1})\varepsilon_t$\
& &
$s_t =  s_{t-m} + \gamma (\ell_{t-1} + b_{t-1} + s_{ t-m})\varepsilon_t$
& $s_t =  s_{t-m}(1 + \gamma \varepsilon_t)$\
& $y_{t} =  (\ell_{t-1} + \phi b_{t-1})(1 + \varepsilon_t)$ &
$y_{t} =  (\ell_{t-1} + \phi b_{t-1} + s_{t-m})(1 + \varepsilon_t)$ &
$y_{t} = (\ell_{t-1} + \phi b_{t-1})s_{t-m}(1 + \varepsilon_t)$\
**A** &
$\ell_t = (\ell_{t-1} + \phi b_{t-1})(1 + \alpha \varepsilon_t)$ &
$\ell_t = \ell_{t-1} + \phi b_{t-1} + \alpha (\ell_{t-1}+ \phi b_{t-1} + s_{t-m}) \varepsilon_t$
& $\ell_t = (\ell_{t-1} + \phi b_{t-1})(1 + \alpha \varepsilon_t)$\
& $b_t = \phi b_{t-1} + \beta(\ell_{t-1} + \phi b_{t-1})\varepsilon_t$ &
$b_t = \phi b_{t-1} + \beta  (\ell_{t-1} + \phi b_{t-1} +s_{t-m})\varepsilon_t$
& $b_t = \phi b_{t-1} + \beta (\ell_{t-1} + \phi b_{t-1})\varepsilon_t$\
& &
$s_t =  s_{t-m} + \gamma (\ell_{t-1} + \phi b_{t-1} + s_{t-m})\varepsilon_t$
& $s_t =  s_{t-m}(1+ \gamma \varepsilon_t)$\
****
```

Table : State space equations for each of the models in the ETS
framework.

### Estimating ETS models {-}

An alternative to estimating the parameters by minimizing the sum of squared errors, is to maximize the "likelihood". The likelihood is the probability of the data arising from the specified model. So a large likelihood is associated with a good model. For an additive error model, maximizing the likelihood gives the same results as minimizing the sum of squared errors. However, different results will be obtained for multiplicative error models. In this section, we will estimate the smoothing parameters $\alpha$, $\beta$, $\gamma$ and $\phi$, and the initial states $\ell_0$, $b_0$, $s_0,s_{-1},\dots,s_{-m+1}$, by maximizing the likelihood.

The possible values that the smoothing parameters can take is restricted. Traditionally the parameters have been constrained to lie between 0 and 1 so that the equations can be interpreted as weighted averages. That is, $0< \alpha,\beta^*,\gamma^*,\phi<1$. For the state space models, we have set $\beta=\alpha\beta^*$ and $\gamma=(1-\alpha)\gamma^*$. Therefore the traditional restrictions translate to $0< \alpha <1$,  $0 < \beta < \alpha$  and $0< \gamma < 1-\alpha$. In practice, the damping parameter $\phi$ is usually constrained further to prevent numerical difficulties in estimating the model. A common constraint is to set $0.8<\phi<0.98$.

Another way to view the parameters is through a consideration of the mathematical properties of the state space models. Then the parameters are constrained to prevent observations in the distant past having a continuing effect on current forecasts. This leads to some *admissibility* constraints on the parameters which are usually (but not always) less restrictive than the usual region. For example for the ETS(A,N,N) model, the usual parameter region is $0< \alpha <1$ but the admissible region is $0< \alpha <2$. For the ETS(A,A,N) model, the usual parameter region is $0<\alpha<1$ and $0<\beta<\alpha$ but the admissible region is $0<\alpha<2$ and $0<\beta<4-2\alpha$.

### Model selection {-}

A great advantage of the ETS statistical framework is that information criteria can be used for model selection. The AIC, AIC$_{\text{c}}$ and BIC, introduced in Section \@ref(sec-5-3-SelectingPredictors), can be used here to determine which of the 30 ETS models is most appropriate for a given time series.

For ETS models, Akaike’s Information Criterion (AIC) is defined as
$$
  \text{AIC} = -2\log(L) + 2k, 
$$
where $L$ is the likelihood of the model and $k$ is the total number of parameters and initial states that have been estimated.

The AIC corrected for small sample bias (AIC$_\text{c}$) is defined as
$$
  \text{AIC}_{\text{c}} = \text{AIC} + \frac{2(k+1)(k+2)}{T-k}, 
$$
and the Bayesian Information Criterion (BIC) is
$$
  \text{BIC} = \text{AIC} + k[\log(T)-2].
$$

Three of the combinations of (Error, Trend, Seasonal) can lead to numerical difficulties. Specifically, the models that can cause such instabilities are ETS(A,N,M), ETS(A,A,M), and ETS(A,A,M). We normally do not consider these particular combinations when selecting a model.

Models with multiplicative errors are useful when the data are strictly positive, but are not numerically stable when the data contain zeros or negative values. Therefore multiplicative errors models will not be considered if the time series is not strictly positive. In that case only the six fully additive models will be applied.

### The `ets()` function in R {-}

The models can be estimated in R using the `ets()` function in the forecast package. The R code below shows all the possible arguments this function takes, and their default values. If only the time series is specified, and all other arguments are left at their default values, then an appropriate model will be selected automatically. We explain each of the arguments below.

```
ets(y, model="ZZZ", damped=NULL, alpha=NULL, beta=NULL, gamma=NULL,
  phi=NULL, additive.only=FALSE, lambda=NULL, lower=c(rep(0.0001,3), 0.8),
  upper=c(rep(0.9999,3),0.98),
  opt.crit=c("lik","amse","mse","sigma","mae"), nmse=3,
  bounds=c("both","usual","admissible"), ic=c("aicc","aic","bic"),
  restrict=TRUE)
```

y
:   The time series to be forecast.

model
:   A three-letter code indicating the model to be estimated using the ETS classification and notation. The possible inputs are "N" for none, "A" for additive, "M" for multiplicative, or "Z" for automatic selection. If any of the inputs is left as "Z" then this component is selected according to the information criterion chosen. The default value of `ZZZ` ensures that all components are selected using the information criterion.

damped
:   If `damped=TRUE`, then a damped trend will be used (either A or M). If `damped=FALSE`, then a non-damped trend will used. If `damped=NULL` (the default), then either a damped or a non-damped trend will be selected according to the information criterion chosen.

alpha, beta, gamma, phi
:   The values of the smoothing parameters can be specified using these arguments. If they are set to `NULL` (the default setting for each of them), the parameters are estimated.

additive.only
:   Only models with additive components will be considered if `additive.only=TRUE`. Otherwise all models will be

lambda
:   Box-Cox transformation parameter. It will be ignored if `lambda=NULL` (the default value). Otherwise, the time series will be transformed before the model is estimated. When `lambda` is not `NULL`, `additive.only` is set to `TRUE`.

lower, upper
:   Lower and upper bounds for the parameter estimates $\alpha$, $\beta^*$, $\gamma^*$ and $\phi$.

opt.crit
:   The optimization criterion to be used for estimation. The default setting is maximum likelihood estimation, used when `opt.crit=lik`.

bounds
:   This specifies the constraints to be used on the parameters. The traditional constraints are set using `bounds="usual"` and the admissible constraints are set using `bounds="admissible"`. The default (`bounds="both"`) requires the parameters to satisfy both sets of constraints.

ic
:   The information criterion to be used in selecting models, set by default to `aicc`.

restrict
:   If `restrict=TRUE` (the default), the models that cause numerical difficulties are not considered in model selection.

### Forecasting with ETS models {-}

Point forecasts are obtained from the models by iterating the equations for $t=T+1,\dots,T+h$ and setting all $\varepsilon_t=0$ for $t>T$.

For example, for model ETS(M,A,N), $y_{T+1} = (\ell_T + b_T )(1+ \varepsilon_{T+1}).$ Therefore $\hat{y}_{T+1|T}=\ell_{T}+b_{T}.$ Similarly,
\begin{align*}
y_{T+2} &= (\ell_{T+1} + b_{T+1})(1 + \varepsilon_{T+1})\\
        &= \left[
              (\ell_T + b_T) (1+ \alpha\varepsilon_{T+1}) + 
              b_T + \beta (\ell_T + b_T)\varepsilon_{T+1}
            \right]
   ( 1 + \varepsilon_{T+1}).
\end{align*}
Therefore, $\hat{y}_{T+2|T}= \ell_{T}+2b_{T},$ and so on. These forecasts are identical to the forecasts from Holt’s linear method and also those from model ETS(A,A,N). So the point forecasts obtained from the method and from the two models that underlie the method are identical (assuming the same parameter values are used).

A big advantage of the models is that prediction intervals can also be generated --- something that cannot be done using the methods. The prediction intervals will differ between models with additive and multiplicative methods.

For some models, there are exact formulae that enable prediction intervals to be calculated. A more general approach that works for all models is to simulate future sample paths, conditional on the last estimate of the states, and to obtain prediction intervals from the percentiles of these simulated future paths. These options are available in R using the `forecast` function in the forecast package. The R code below shows the all the possible arguments this function takes when applied to an ETS model. We explain each the arguments in what follows.

```
forecast(object, h=ifelse(object[["m"]]\>1, 2\*object[["m"]], 10),
level=c(80,95), fan=FALSE, simulate=FALSE, bootstrap=FALSE, npaths=5000,
PI=TRUE, lambda=object[["lambda"]], ...)
```

object
:   The object returned by the `ets()` function.

h
:   The forecast horizon --- the number of periods to be forecast.

level
:   The confidence level for the prediction intervals.

fan
:   If `fan=TRUE`, `level=seq(50,99,by=1)`. This is suitable for fan plots.

simulate
:   If `simulate=TRUE`, prediction intervals are produced by simulation rather than using algebraic formulae. Simulation will also be used (even if `simulate=FALSE`) where there are no algebraic formulae available for the particular model.

bootstrap
:   If `bootstrap=TRUE` and `simulate=TRUE`, then the simulated prediction intervals use re-sampled errors rather than normally distributed errors.

npaths
:   The number of sample paths used in computing simulated prediction intervals.

PI
:   If `PI=TRUE`, then prediction intervals are produced; otherwise only point forecasts are calculated. If `PI=FALSE`, then `level`, `fan`, `simulate`, `bootstrap` and `npaths` are all ignored.

lambda
:   The Box-Cox transformation parameter. This is ignored if `lambda=NULL`. Otherwise, forecasts are back-transformed via an inverse Box-Cox transformation.

### Example: Oil production example (revisited) {-}

Figure \@ref(fig-7-Oil) shows the point forecasts and prediction intervals from an estimated ETS(A,N,N) model. The estimates for the smoothing parameter $\alpha$ and the initial level $\ell_0$ are 0.89 and 447.49 respectively, identical to the estimates for the simple exponential smoothing method estimated earlier (see beginning of Example \@ref(ex-7-SES)). The plot shows the importance of generating prediction intervals. The intervals here are relatively wide, so interpreting the point forecasts without accounting for the large uncertainty can be very misleading.

```{r, 7-oil, fig.cap="Point forecasts and 80% and 95% prediction intervals from model ETS(A,N,N)."
oildata <- window(oil,start=1996,end=2007) 
fit <- ets(oildata, model="ANN") 
autoplot(forecast(fit, h=3)) + xlab("Year") + ylab("Oil (millions of tonnes)")
fit[["par"]]
```

### Example: International tourist visitor nights in Australia {-}

We now employ the ETS statistical framework to forecast tourists visitor nights in Australia by international arrivals over the period 2011--2012. We let the `ets()` function select the model by minimizing the AICc.

```{r}
vndata <- window(austourists, start=2005) 
fit <- ets(vndata)
summary(fit)
```

The model selected is ETS(M,A,M):

\begin{align*}
y_{t} &= (\ell_{t-1} + b_{t-1})s_{t-m}(1 + \varepsilon_t)\\
\ell_t &= (\ell_{t-1} + b_{t-1})(1 + \alpha \varepsilon_t)\\
b_t &=b_{t-1} + \beta(\ell_{t-1} + b_{t_1})\varepsilon_t\\
s_t &=  s_{t-m}(1+ \gamma \varepsilon_t).
\end{align*}

The parameter estimates are $\alpha=0.4504$, $\beta=0.0004$, and $\gamma=0.0046$. The output returns the estimates for the initial states $\ell_0$, $b_0$, $s_{0}$, $s_{-1}$, $s_{-2}$ and $s_{-3}.$ The training RMSE for this model is slightly lower than the Holt-Winter methods with additive and multiplicative seasonality presented in Tables \@ref(tbl-7-HWadd) and \@ref(tbl-7-HWmulti) respectively. Figure \@ref(fig-7-MAdMstates) shows the states over time while Figure \@ref(fig-7-MAdMforecasts) shows point forecasts and prediction intervals generated from the model. The intervals are much narrower than the prediction intervals in the oil production example.

```{r MAMstates, fig.cap="Graphical representation of the estimated states over time."}
autoplot(fit)
```

```{r MAMforecasts, fig.cap="Forecasting international visitor nights in Australia from an ETS(M,A,M) model."}
autoplot(forecast(fit,h=8)) + 
  ylab("International visitor night in Australia (millions)")
```

## Exercises

1. Data set `books` contains the daily sales of paperback and
hardcover books at the same store. The task is to forecast the next four
days’ sales for paperback and hardcover books (data set `books`).

 a. Plot the series and discuss the main features of the data.

 b. Use simple exponential smoothing with the `ses` function (setting `initial="simple"`) and explore different values of $\alpha$ for the `paperback` series. Record the within-sample SSE for the one-step forecasts. Plot SSE against $\alpha$ and find which value of $\alpha$ works best. What is the effect of $\alpha$ on the forecasts?

 c. Now let `ses` select the optimal value of $\alpha$. Use this value to generate forecasts for the next four days. Compare your results with (\@ref(1b)).

 d. Repeat but with `initial="optimal"`. How much difference does an optimal initial level make?

Repeat steps (b)--(d) with the `hardcover` series.

2. 
  a. Apply Holt’s linear method to the `paperback` and `hardback` series and compute four-day forecasts in each case.

  b. Compare the SSE measures of Holt’s method for the two series to those of simple exponential smoothing in the previous question. Discuss the merits of the two forecasting methods for these data sets.

  c. Compare the forecasts for the two series using both methods. Which do you think is best?

  d. Calculate a 95% prediction interval for the first forecast for each series using both methods, assuming normal errors. Compare your forecasts with those produced by R.

3. For this exercise, use the price of a dozen eggs in the United States from 1900--1993 (data set `eggs`). Experiment with the various options in the `holt()` function to see how much the forecasts change with damped or exponential trend. Also try changing the parameter values for $\alpha$ and $\beta$ to see how they affect the forecasts. Try to develop an intuition of what each parameter and argument is doing to the forecasts.

[Hint: use `h=100` when calling `holt()` so you can clearly see the differences between the various options when plotting the forecasts.]

Which model gives the best RMSE?

4. For this exercise, use the quarterly UK passenger vehicle production data from 1977:1--2005:1 (data set `ukcars`).

 a. Plot the data and describe the main features of the series.

 b. Decompose the series using STL and obtain the seasonally adjusted data.

 c. Forecast the next two years of the series using an additive damped trend method applied to the seasonally adjusted data. Then reseasonalize the forecasts. Record the parameters of the method and report the RMSE of the one-step forecasts from your method.

 d. Forecast the next two years of the series using Holt’s linear method applied to the seasonally adjusted data. Then reseasonalize the forecasts. Record the parameters of the method and report the RMSE of of the one-step forecasts from your method.

 e. Now use `ets()` to choose a seasonal model for the data.

 f. Compare the RMSE of the fitted model with the RMSE of the model you obtained using an STL decomposition with Holt’s method. Which gives the better in-sample fits?

 g. Compare the forecasts from the two approaches? Which seems most reasonable?


5. For this exercise, use the monthly Australian short-term overseas visitors data, May 1985--April 2005. (Data set: `visitors`.)

 a. Make a time plot of your data and describe the main features of the series.

 b. Forecast the next two years using Holt-Winters’ multiplicative method.

 c. Why is multiplicative seasonality necessary here?

 d. Experiment with making the trend exponential and/or damped.

 e. Compare the RMSE of the one-step forecasts from the various methods. Which do you prefer?

 g. Now fit each of the following models to the same data:

   i) a multiplicative Holt-Winters’ method;

   ii) an ETS model;

   iii) an additive ETS model applied to a Box-Cox transformed series;

   iv) a seasonal naive method applied to the Box-Cox transformed series;

   v) an STL decomposition applied to the Box-Cox transformed data followed by an ETS model applied to the seasonally adjusted (transformed) data.

   vi) For each model, look at the residual diagnostics and compare the forecasts for the next two years. Which do you prefer?

## Further reading

- @Gar1985
- @Gar2006
- @expsmooth08
