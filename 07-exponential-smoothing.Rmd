#Exponential smoothing {#expsmooth}

Exponential smoothing was proposed in the late 1950s (Brown 1959, Holt 1957 and Winters 1960 are key pioneering works) and has motivated some of the most successful forecasting methods. Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series which is a great advantage and of major importance to applications in industry.

This chapter is divided into two parts. In the first part (Sections \@ref(sec-7-1-SES)--\@ref(sec-7-6-Taxonomy)) we present in detail the mechanics of the most important exponential smoothing methods and their application in forecasting time series with various characteristics. This is key in understanding the intuition behind these methods. In this setting, selecting and using a forecasting method may appear to be somewhat ad hoc. The selection of the method is generally based on recognising key components of the time series (trend and seasonal) and how these enter the smoothing method (e.g., in an additive, damped or multiplicative manner).

In the second part of the chapter (Section \@ref(sec-7-ETS)) we present statistical models that underlie exponential smoothing methods. These models generate identical point forecasts to the methods discussed in the first part of the chapter, but also generate prediction intervals. Furthermore, this statistical framework allows for genuine model selection between competing models.

##Simple exponential smoothing {#sec-7-1-SES}

The simplest of the exponentially smoothing methods is naturally called "simple exponential smoothing" (SES)^[In some books it is called ``single exponential smoothing'']. This method is suitable for forecasting data with no trend or seasonal pattern. For example, the data in Figure \@ref(fig:7-oil) do not display any clear trending behaviour or any seasonality, although the mean of the data may be changing slowly over time. We have already considered the naïve and the average as possible methods for forecasting such data (Section \@ref(sec-2-methods)).

```{r oil, fig.cap="Oil production in Saudi Arabia from 1996 to 2007."}
oildata <- window(oil,start=1996,end=2007) 
autoplot(oildata) + 
  ylab("Oil (millions of tonnes)") + xlab("Year")
```

Using the naïve method, all forecasts for the future are equal to the last observed value of the series, 
$$
  \hat{y}_{T+h|T} = y_T,
$$ 
for $h=1,2,\dots$. Hence, the naïve method assumes that the most current observation is the only important one and all previous observations provide no information for the future. This can be thought of as a weighted average where all the weight is given to the last observation.

Using the average method, all future forecasts are equal to a simple average of the observed data,
$$
  \hat{y}_{T+h|T} = \frac1T \sum_{t=1}^T y_t,
$$ 
for $h=1,2,\dots$. Hence, the average method assumes that all observations are of equal importance and they are given equal weight when generating forecasts.

We often want something between these two extremes. For example it may be sensible to attach larger weights to more recent observations than to observations from the distant past. This is exactly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages where the weights decrease exponentially as observations come from further in the past --- the smallest weights are associated with the oldest observations:
$$\label{eq-7-ses}
  \hat{y}_{T+1|T} = \alpha y_T + \alpha(1-\alpha) y_{T-1} + \alpha(1-\alpha)^2 y_{T-2}+ \alpha(1-\alpha)^3 y_{T-3}+\cdots,
$$
where $0 \le \alpha \le 1$ is the smoothing parameter. The one-step-ahead forecast for time $T+1$ is a weighted average of all the observations in the series $y_1,\dots,y_T$. The rate at which the weights decrease is controlled by the parameter $\alpha$.

Table \@ref(tab:alpha) shows the weights attached to observations for four different values of $\alpha$ when forecasting using simple exponential smoothing. Note that the sum of the weights even for a small $\alpha$ will be approximately one for any reasonable sample size.

```{r alpha, echo=FALSE}
tab <- as.data.frame(matrix(NA,nrow=6,ncol=4))
rownames(tab) <- c("$y_{T}$", paste("$y_{T-",1:5,"}$",sep=''))
alpha <- c(0.2,0.4,0.6,0.8)
colnames(tab) <- paste("$\\alpha=",alpha,"$",sep="")
for(i in 1:6)
  tab[i,] <- alpha*(1-alpha)^(i-1)
knitr::kable(tab, digits=4,booktabs=TRUE, 
             caption="Exponentially decaying weights attached to observations of the time series when generating forecasts using simple exponential smoothing.")
```

For any $\alpha$ between 0 and 1, the weights attached to the observations decrease exponentially as we go back in time, hence the name "exponential smoothing". If $\alpha$ is small (i.e., close to 0), more weight is given to observations from the more distant past. If $\alpha$ is large (i.e., close to 1), more weight is given to the more recent observations. At the extreme case where $\alpha=1$, $\hat{y}_{T+1|T}=y_T$ and forecasts are equal to the naïve forecasts.

We present two equivalent forms of simple exponential smoothing, each of which leads to the forecast equation (\@ref{eq-7-ses}).

### Weighted average form {-}

The forecast at time $t+1$ is equal to a weighted average between the most recent observation $y_t$ and the most recent forecast $\hat{y}_{t|t-1}$,
$$
 \hat{y}_{t+1|t} = \alpha y_t + (1-\alpha) \hat{y}_{t|t-1}
$$
for $t=1,\dots,T$, where $0 \le \alpha \le 1$ is the smoothing parameter.

The process has to start somewhere, so we let the first forecast of $y_1$ be denoted by $\ell_0$ (which we will have to estimate). Then
\begin{align*}
  \hat{y}_{2|1} &= \alpha y_1 + (1-\alpha) \ell_0\\
  \hat{y}_{3|2} &= \alpha y_2 + (1-\alpha) \hat{y}_{2|1}\\
  \hat{y}_{4|3} &= \alpha y_3 + (1-\alpha) \hat{y}_{3|2}\\
  \vdots\\
  \hat{y}_{T+1|T} &= \alpha y_T + (1-\alpha) \hat{y}_{T|T-1}
\end{align*}
Substituting each equation into the following equation, we obtain
\begin{align*}
  \hat{y}_{3|2}   & = \alpha y_2 + (1-\alpha) \left[\alpha y_1 + (1-\alpha) \ell_0\right]              \\
                 & = \alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0                          \\
  \hat{y}_{4|3}   & = \alpha y_3 + (1-\alpha) [\alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0]\\
                 & = \alpha y_3 + \alpha(1-\alpha) y_2 + \alpha(1-\alpha)^2 y_1 + (1-\alpha)^3 \ell_0 \\
                 & ~~\vdots                                                                           \\
  \hat{y}_{T+1|T} & =  \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} + (1-\alpha)^T \ell_{0}.
\end{align*}
So the weighted average form leads to the same forecast equation .

### Component form {-}

An alternative representation is the component form. For simple exponential smoothing the only component included is the level, $\ell_t$. (Other methods considered later in this chapter may also include a trend $b_t$ and seasonal component $s_t$.) Component form representations of exponential smoothing methods comprise a forecast equation and a smoothing equation for each of the components included in the method. The component form of simple exponential smoothing is given by:
\begin{align*}
  \text{Forecast equation}  && \hat{y}_{t+1|t} & = \ell_{t}\\
  \text{Smoothing equation} && \ell_{t}        & = \alpha y_{t} + (1 - \alpha)\ell_{t-1},
\end{align*}
where $\ell_{t}$ is the level (or the smoothed value) of the series at time $t$. The forecast equation shows that the forecasted value at time $t+1$ is the estimated level at time $t$. The smoothing equation for the level (usually referred to as the level equation) gives the estimated level of the series at each period $t$.

Applying the forecast equation for time $T$ gives, $\hat{y}_{T+1|T} = \ell_{T}$, the most recent estimated level.

If we replace $\ell_t$ by $\hat{y}_{t+1|t}$ and $\ell_{t-1}$ by $\hat{y}_{t|t-1}$ in the smoothing equation, we will recover the weighted average form of simple exponential smoothing.

### Multi-horizon Forecasts {-}

So far we have given forecast equations for only one step ahead. Simple exponential smoothing has a "flat" forecast function, and therefore for longer forecast horizons, 
$$
 \hat{y}_{T+h|T} = \hat{y}_{T+1|T}=\ell_T, \qquad h=2,3,\dots.
$$ 
Remember these forecasts will only be suitable if the time series has no trend or seasonal component.

### Optimization {-}

The application of every exponential smoothing method requires the smoothing parameters and the initial values to be chosen. In particular, for simple exponential smoothing, we need to select the values of $\alpha$ and $\ell_0$. All forecasts can be computed from the data once we know those values. For the methods that follow there is usually more than one smoothing parameter and more than one initial component to be chosen.

There are cases where the smoothing parameters may be chosen in a subjective manner --- the forecaster specifies the value of the smoothing parameters based on previous experience. However, a more robust and objective way to obtain values for the unknown parameters included in any exponential smoothing method is to estimate them from the observed data.

In Section \@ref(sec-4-2-LSprinciple) we estimated the coefficients of a regression model by minimizing the sum of the squared errors (SSE). Similarly, the unknown parameters and the initial values for any exponential smoothing method can be estimated by minimizing the SSE. The errors are specified as $e_t=y_t - \hat{y}_{t|t-1}$ for $t=1,\dots,T$ (the one-step-ahead training errors). Hence we find the values of the unknown parameters and the initial values that minimize
$$
 \text{SSE}=\sum_{t=1}^T(y_t - \hat{y}_{t|t-1})^2=\sum_{t=1}^Te_t^2. \label{eq-7-SSE}
$$

Unlike the regression case (where we have formulae that return the values of the regression coefficients which minimize the SSE) this involves a non-linear minimization problem and we need to use an optimization tool to perform this.

```{r ses, fig.cap="Simple exponential smoothing applied to oil production in Saudi Arabia (1996--2007)."}
fit <- ses(oildata, h=3) 
tmp <- cbind(Data=oildata,Forecasts=fit$mean, Fitted=fitted(fit))
autoplot(tmp) + ylab("Oil (millions of tonnes)") + xlab("Year") 
```

```{r, echo=FALSE}
# Data set for table
x <- oildata
# Generate forecasts
fit <- ses(x, h=3)
# Now set up the table
n <- length(x)
year0 <- min(time(x))-1
tab <- matrix(NA,nrow=n+6,ncol=5)
colnames(tab) <- c("Year","Time","Observation","Level","Forecast")
tab[2:(n+6),1] <- year0 + 0:(n+4)
tab[2:(n+6),2] <- 0:(n+4)
# Add data, level and fitted values
tab[3:(n+2),3] <- x
tab[2:(n+2),4] <- fit$model$state
tab[3:(n+2),5] <- fitted(fit)
# Add forecasts
tab[n+(4:6),1] <- max(time(x))+1:3
tab[n+(4:6),2] <- 1:3
tab[n+(4:6),5] <- fit$mean
# Convert to characters
tab <- as.data.frame(tab)
class(tab$Year) <- class(tab$Time) <- "integer"
tab <- format(tab, digits=5)
# Remove missing values
tab <- apply(tab, 2, function(x){j <- grep("[ ]*NA",x); x[j] <- ""; return(x)})
# Add math notation rows
tab[1,] <- c("","$t$","$y_t$","$\\ell_t$","$\\hat{y}_{t+1|t}$")
tab[n+3,] <- c("","$h$","","","$\\hat{y}_{T+h|T}$")
# Show table
knitr::kable(tab)
```

```{r, echo=FALSE}
# Accuracy of one-step-ahead training errors over period 1--12
tmp <- accuracy(fit)
print(round(c(tmp[,c("MAE","RMSE","MAPE")],SSE=sum(residuals(fit)^2)),1))
alpha <- fit$model$par[1]
l0 <- fit$model$par[2]
```
$\alpha=`r format(alpha,digits=2,nsmall=2)`$ and 
$\ell_0=`r format(l0,digits=1,nsmall=1)`$ 
are obtained by minimizing SSE over periods $t=1,2,\dots,12$.

In this example, simple exponential smoothing is applied to forecast oil production in Saudi Arabia. The black line in Figure \@ref(fig-7-ses) is a plot of the data over the period 1996--2007, which shows a changing level over time but no obvious trending behaviour.

In Table \@ref(tbl-7-ses) we demonstrate the application of simple exponential smoothing. The second last column shows the estimated level for times $t=0$ to $t=12$; the last column shows the forecasts for $h=1,2,3$. Using an optimization tool, we find the values of $\alpha=0.89$ and $\ell_0=447.5$ that minimize the SSE, subject to the restriction that $0\le\alpha\le1$. So the SSE value presented in the last row of the table is smaller for these values of $\alpha$ and $\ell_0$ than for any other values of $\alpha$ and $\ell_0$.

The forecasts for the period 2008--2010 are plotted in
Figure \@ref(fig-7-ses). Also plotted are one-step-ahead training forecasts
alongside the data over the period 1996--2007.

## Trend methods

###Holt’s linear trend method

Holt (1957) extended simple exponential smoothing to allow forecasting of data with a trend. This method involves a forecast equation and two smoothing equations (one for the level and one for the trend):
\begin{align*}
  \text{Forecast equation}&& \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} \\
  \text{Level equation}   && \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  \text{Trend equation}   && b_{t}    &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)b_{t-1}
\end{align*}
where $\ell_t$ denotes an estimate of the level of the series at time $t$, $b_t$ denotes an estimate of the trend (slope) of the series at time $t$, $\alpha$ is the smoothing parameter for the level, $0\le\alpha\le1$ and $\beta^*$ is the smoothing parameter for the trend, $0\le\beta^*\le1$ (we denote this as $\beta^*$ instead of $\beta$ for reasons that will be explained in Section \@ref(sec-7-ETS)).

As with simple exponential smoothing, the level equation here shows that $\ell_t$ is a weighted average of observation $y_t$ and the one-step-ahead training forecast for time $t$, here given by $\ell_{t-1} + b_{t-1}$. The trend equation shows that $b_t$ is a weighted average of the estimated trend at time $t$ based on $\ell_{t} - \ell_{t-1}$ and $b_{t-1}$, the previous estimate of the trend.

The forecast function is no longer flat but trending. The $h$-step-ahead forecast is equal to the last estimated level plus $h$ times the last estimated trend value. Hence the forecasts are a linear function of $h$.

### Example: Air Passengers {-}

In Table \@ref(tbl-7-Holts) we demonstrate the application of Holt’s linear method to air transportation data for all passengers with an Australian airline.

The smoothing paramters, $\alpha$ and $\beta$, and the initial values $\ell_0$ and $b_0$ are estimated by minimizing the SSE for the one-step training errors as in Section \@ref(sec-7-1-SES).

```{r
air <- window(ausair,start=1990,end=2011) 
fit <- holt(air, h=5)
# Resulting forecasts:
fit$model$state 
fitted(fit) 
fit[["mean"]]
```

```{r, echo=FALSE}
# Data set for table
air <- window(ausair,start=1990,end=2011) 
# Generate forecasts`
fit <- holt(air, h=5)
# Now set up table
tmp <- cbind(air, fit$model$state, fitted(fit))
tsp(tmp) <- NULL
tmp <- cbind(1989:2011,0:(NROW(tmp)-1),tmp)
tmp <- rbind(rep(NA,6),tmp, matrix(NA, nrow=6, ncol=ncol(tmp)))
colnames(tmp) <- c("Year","Time","Observation","Level","Slope","Forecast")
# Add in forecasts
tmp[26:30,6] <- fit$mean
tmp[26:30,2] <- 1:5
# Convert to characters
tmp <- as.data.frame(tmp)
class(tmp[["Year"]]) <- class(tmp[["Time"]]) <- "integer"
tmp <- format(tmp, digits=4)
# Remove missing values
tmp <- apply(tmp, 2, function(x){j <- grep("[ ]*NA",x); x[j] <- ""; return(x)})
# Add math notation
tmp[1,] <- c("","t","$y_t$","$\\ell_t$",
                   "$b_t$", "$\\hat{y}_{t|t-1}$")
tmp[25,] <- c("","h","","","", "$\\hat{y}_{t+h|t}$")
knitr::kable(tmp, booktabs=TRUE, 
             caption="Applying Holt's linear method with $\alpha=0.7674$ and $\beta^*=0.0001$ to Australian Air Passenger Data (thousands of passengers).")
```

```{r fig_7_trend, fig.cap="Forecasting Air Passengers in Australia (thousands of passengers). For all methods $\\alpha=0.8$ and $\\beta^*=0.0001$, and for the additive damped trend method $\\phi=0.85$."}

fit2 <- holt(air, damped=TRUE, h=5, phi=0.95) 
tmp <- cbind(Data=air, 
             "Holt's method"=fit[["mean"]], 
             "Damped Holt's method" = fit2[["mean"]])
autoplot(tmp) +
  ggtitle("Forecasts from Holt's method") +
  xlab("Year") + ylab("Air passengers in Australia (thousands)") +
  scale_color_manual(values=c('#d95f02','#000000','#1b9e77'),
                     breaks=c("Data","Holt's method","Damped Holt's method"),
                     name="Series")
```

###Damped trend methods

The forecasts generated by Holt’s linear method display a constant trend (increasing or decreasing) indefinitely into the future.

Empirical evidence indicates that these methods tend to over-forecast, especially for longer forecast horizons. Motivated by this observation, Gardner and McKenzie (1985) introduced a parameter that “dampens” the trend to a flat line some time in the future. Methods that include a damped trend have proven to be very successful and are arguably the most popular individual methods when forecasts are required automatically for many series.

In conjunction with the smoothing parameters $\alpha$ and $\beta^*$ (with values between 0 and 1 as in Holt’s method), this method also includes a damping parameter $0<\phi<1$:
\begin{align*}
  \hat{y}_{t+h|t} &= \ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t} \\
  \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)\phi b_{t-1}.
\end{align*}
If $\phi=1$ the method is identical to Holt’s linear method. For values between $0$ and $1$, $\phi$ dampens the trend so that it approaches a constant some time in the future. In fact the forecasts converge to $\ell_T+\phi b_T/(1-\phi)$ as $h\rightarrow\infty$ for any value $0<\phi<1$. The effect of this is that short-run forecasts are trended while long-run forecasts are constant.

### Example: Air Passengers (continued) {-}

Figure \@ref(fig-7-trend) shows the one-step training forecasts, and the forecasts for years 2005--2010 generated from Holt’s linear trend method, and additive damped trend. The most optimistic forecasts come from the exponential trend method while the least optimistic come from the damped trend method, with the forecasts generated by Holt’s linear trend method somewhere between the two.

### Example: Sheep in Asia

In this example we compare the forecasting performance of the three exponential smoothing methods we have considered so far in forecasting the sheep livestock population in Asia. The data spans the period 1970--2007. We withhold the period 2001--2007 as a test set, and use the data up to and including year 2000 for the training set (see Section \@ref(accuracy) for a definition of training and test sets). Figure \@ref(fig-7-comp) shows that data and the forecasts from all methods.

The parameters and initial values of the methods are estimated for all methods by minimizing SSE (as specified in Equation ) over the training set. In Table \@ref(tbl-7-comp) we present the estimation results and error measures over the training and the test sets.

```{r}
livestock2 <- window(livestock,start=1970,end=2000) 
fit1 <- ses(livestock2) 
fit2 <- holt(livestock2) 
fit3 <- holt(livestock2,damped=TRUE) 
# Results for first model: 
fit1[["model"]]
# training set 
accuracy(fit1) 
# test set
accuracy(fit1,livestock) 
```

```{r echo=FALSE}
tab <- matrix(NA, ncol=3,nrow=10)
colnames(tab) <- c("SES","Linear trend","Damped trend")
rownames(tab) <- c("$\\alpha$","$\\beta^*$","$\\phi$","$\\ell_0$","$b_0$",
                   "Training RMSE","Test RMSE","Test MAE","Test MAPE","Test MASE")
# SSE
tab[1,1] <- fit1$model$par["alpha"]
tab[4,1] <- fit1$model$par["l"]
tab[6,1] <- sqrt(fit1$model$mse)
tab[c(7:10),1] <- accuracy(fit1,livestock)["Test set",c("RMSE","MAE","MAPE","MASE")]
# Holt
tab[1,2] <- fit2$model$par["alpha"]
tab[2,2] <- fit2$model$par["beta"]/fit1$model$par["alpha"]
tab[4,2] <- fit2$model$par["l"]
tab[5,2] <- fit2$model$par["b"]
tab[6,2] <- sqrt(fit2$model$mse)
tab[c(7:10),2] <- accuracy(fit2,livestock)["Test set",c("RMSE","MAE","MAPE","MASE")]
# Damped trend
tab[1,3] <- fit3$model$par["alpha"]
tab[2,3] <- fit3$model$par["beta"]/fit1$model$par["alpha"]
tab[3,3] <- fit3$model$par["phi"]
tab[4,3] <- fit3$model$par["l"]
tab[5,3] <- fit3$model$par["b"]
tab[6,3] <- sqrt(fit3$model$mse)
tab[c(7:10),3] <- accuracy(fit3,livestock)["Test set",c("RMSE","MAE","MAPE","MASE")]
# Convert to characters
tab <- as.data.frame(formatC(tab, format="f", digits=2))
# Remove missing values
tab <- apply(tab, 2, function(x){j <- grep("[ ]*NA",x); x[j] <- ""; return(x)})
# Show table
knitr::kable(tab, booktabs=TRUE, 
             caption="Estimation results from models for Sheep in Asia")

```

For the simple exponential smoothing method, the estimated smoothing parameter is $\alpha=1$. This is expected as the series is clearly trending over time and simple exponential smoothing requires the largest possible adjustment in each step to capture this trend.

![Level and slope components for Holt’s linear trend method and the
additive damped trend method.](fig_7_LevelTrend.pdf)

\@ref(fig-7-leveltrend)

plot(fit2$model$state) plot(fit4$model$state)

![Forecasting livestock, sheep in Asia: comparing forecasting
performance of non-seasonal methods. ](fig_7_comp.pdf)

\@ref(fig-7-comp)

plot(fit3, type=“o”, ylab=“Livestock, sheep in Asia (millions)”, flwd=1,
plot.conf=FALSE) lines(window(livestock,start=2001),type=“o”)
lines(fit1$mean,col=2)
lines(fit2$mean,col=3) lines(fit4$mean,col=5)
lines(fit5$mean,col=6) legend(“topleft”, lty=1, pch=1, col=1:6,
c(“Data”,“SES”,“Holt’s”,“Damped trend”))

For the other methods, there is also a trend component.The smoothing parameter for the slope parameter is estimated to be zero, indicating that the trend is not changing over time. Of course, the trend estimated using the damped trend methods will change in the future due to the damping.

In Figure \@ref(fig-7-leveltrend), we plot the level and trend components for Holt’s method and for the damped trend method. The slope of the trend component for Holt’s method is constant, showing that the trend is linear. In contrast, the slope of the trend component for the damped trend method is decreasing, showing that the trend is levelling off.

For the damped trend method, the damping parameter $\phi$ is restricted to a maximum of 0.98 (the estimation returned an optimal value of $\phi=1$). This restriction is imposed to ensure that the damped trend method generates noticeably different forecasts from Holt’s linear method, otherwise we get identical forecasts.

The SSE measures calculated over the training set show that Holt’s linear trend method provides the best fit to the data followed by the additive damped trend method. Simple exponential smoothing generates the largest one-step training errors. In Figure \@ref(fig-7-comp) we can examine the forecasts generated by the methods. Pretending that we have not seen the data over the test-set we would conclude that all forecasts are quite plausible especially from the methods that account for the trend in the data.

Comparing the forecasting performance of the methods over the test set in Table \@ref(tbl-7-comp), ??? is the most accurate method according to the MAE, MAPE and MASE, while Holt’s linear method is most accurate according to the RMSE.

Conflicting results like this are very common when performing forecasting competitions between methods. As forecasting tasks can vary by many dimensions (length of forecast horizon, size of test set, forecast error measures, frequency of data, etc.), it is unlikely that one method will be better than all others for all forecasting scenarios. What we require from a forecasting method are consistently sensible forecasts, and these should be frequently evaluated against the task at hand.

##Holt-Winters seasonal method {#sec-7-Taxonomy}

Holt (1957) and Winters (1960) extended Holt’s method to capture seasonality. The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations --- one for the level $\ell_t$, one for trend $b_t$, and one for the seasonal component denoted by $s_t$, with smoothing parameters $\alpha$, $\beta^*$ and $\gamma$. We use $m$ to denote the period of the seasonality, i.e., the number of seasons in a year. For example, for quarterly data $m=4$, and for monthly data $m=12$.

There are two variations to this method that differ in the nature of the seasonal component. The additive method is preferred when the seasonal variations are roughly constant through the series, while the multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series. With the additive method, the seasonal component is expressed in absolute terms in the scale of the observed series, and in the level equation the series is seasonally adjusted by subtracting the seasonal component. Within each year the seasonal component will add up to approximately zero. With the multiplicative method, the seasonal component is expressed in relative terms (percentages) and the series is seasonally adjusted by dividing through by the seasonal component. Within each year, the seasonal component will sum up to approximately $m$.

### Holt {-}-Winters additive method

The component form for the additive method is:

$$\begin{aligned}
\pred{y}{t+h}{t} &= \ell_{t} + hb_{t} + s_{t-m+\h+} \\
\ell_{t} &= \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)b_{t-1}\\
s_{t} &= \gamma (y_{t}-\ell_{t-1}-b_{t-1}) + (1-\gamma)s_{t-m},\end{aligned}$$

where $\h+=\lfloor(h-1)\mod~m\rfloor+1$, which ensures that the estimates of the seasonal indices used for forecasting come from the final year of the sample. The level equation shows a weighted average between the seasonally adjusted observation $(y_{t} - s_{t-m})$ and the non-seasonal forecast $(\ell_{t-1}+b_{t-1})$ for time $t$. The trend equation is identical to Holt’s linear method. The seasonal equation shows a weighted average between the current seasonal index, $(y_{t}-\ell_{t-1}-b_{t-1})$, and the seasonal index of the same season last year (i.e., $m$ time periods ago).

The equation for the seasonal component is often expressed as
$$
 s_{t} = \gamma^* (y_{t}-\ell_{t})+ (1-\gamma^*)s_{t-m}.
$$ 
If we substitute $\ell_t$ from the smoothing equation for the level of the component form above, we get
$$
 s_{t} = \gamma^*(1-\alpha) (y_{t}-\ell_{t-1}-b_{t-1})+ [1-\gamma^*(1-\alpha)]s_{t-m}
$$
which is identical to the smoothing equation for the seasonal component we specify here with $\gamma=\gamma^*(1-\alpha)$. The usual parameter restriction is $0\le\gamma^*\le1$, which translates to $0\le\gamma\le 1-\alpha$.

### Holt {-}-Winters multiplicative method

The component form for the multiplicative method is:

$$\begin{aligned}
\pred{y}{t+h}{t} &= (\ell_{t} + hb_{t})s_{t-m+h_{m}^{+}}. \\
\ell_{t} &= \alpha \frac{y_{t}}{s_{t-m}} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
b_{t} &= \beta^*(\ell_{t}-\ell_{t-1}) + (1 - \beta^*)b_{t-1}                \\
s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + b_{t-1})} + (1 - \gamma)s_{t-m}\end{aligned}$$

[International tourist visitor nights in Australia]

![Forecasting international visitor nights in Australia using
Holt-Winters method with both additive and multiplicative
seasonality.](fig_7_HW.pdf)

\@ref(fig-7-HW)

aust \<- window(austourists,start=2005) fit1 \<-
hw(aust,seasonal=“additive”) fit2 \<- hw(aust,seasonal=“multiplicative”)

plot(fit2,ylab=“International visitor night in Australia (millions)”,
plot.conf=FALSE, type=“o”, fcol=“white”, xlab=“Year”)
lines(fitted(fit1), col=“red”, lty=2) lines(fitted(fit2), col=“green”,
lty=2) lines(fit1$mean, type="o", col="red")
lines(fit2$mean, type=“o”, col=“green”) legend(“topleft”,lty=1, pch=1,
col=1:3, c(“data”,“Holt Winters’ Additive”,“Holt Winters’
Multiplicative”))

In this example we employ the Holt-Winters method with both additive and multiplicative seasonality to forecast tourists visitor nights in Australia by international arrivals. Figure \@ref(fig-7-HW) shows the data alongside the one-step-ahead training forecasts over the sample period 2005Q1--2010Q4 and the forecasts for the period 2011Q1--2012Q4. The data show an obvious seasonal pattern with peaks observed in the March quarter of each year as this corresponds to the Australian summer.

The application of the method with additive and multiplicative seasonality are presented in Tables \@ref(tbl-7-HWadd) and \@ref(tbl-7-HWmulti) respectively. The results show that the method with the multiplicative seasonality fits the data best. This was expected as the time plot shows the seasonal variation in the data increases as the level of the series increases. This is also reflected in the two sets of forecasts; the forecasts generated by the method with the multiplicative seasonality portray larger and increasing seasonal variation as the level of the forecasts increases compared to the forecasts generated by the method with additive seasonality.

\@ref(tbl-7-HWadd) =0.4cm

<span>lYYYYYY</span> Qtr-Year & t & y~t~ & ~t~ & b~t~ & s~t~ & ~t~\
2004 Q1 & -3 & - & - & - & 10.7 & -\
2004 Q2 & -2 & - & - & - & -9.5 & -\
2004 Q3 & -1 & - & - & - & -2.6 & -\
2004 Q4 & 0 & & 33.8 & 0.65 & 1.4 &\
2005 Q1 & 1 & 41.7 & 34.4 & 0.57 & 10.7 & 45.1\
2005 Q2 & 2 & 24.0 & 34.9 & 0.53 & -9.5 & 25.5\
2005 Q3 & 3 & 32.3 & 35.4 & 0.52 & -2.6 & 32.9\
2005 Q4 & 4 & 37.3 & 35.9 & 0.52 & 1.4 & 37.3\
2006 Q1 & 5 & 46.2 & 36.4 & 0.50 & 10.7 & 47.1\
2006 Q2 & 6 & 29.3 & 37.0 & 0.54 & -9.5 & 27.5\
2006 Q3 & 7 & 36.5 & 37.6 & 0.58 & -2.6 & 35.0\
2006 Q4 & 8 & 43.0 & 38.2 & 0.66 & 1.4 & 39.5\
2007 Q1 & 9 & 48.9 & 38.9 & 0.64 & 10.7 & 49.6\
2007 Q2 & 10 & 31.2 & 39.6 & 0.67 & -9.5 & 30.0\
2007 Q3 & 11 & 37.7 & 40.2 & 0.67 & -2.6 & 37.7\
2007 Q4 & 12 & 40.4 & 40.9 & 0.63 & 1.4 & 42.3\
2008 Q1 & 13 & 51.2 & 41.5 & 0.61 & 10.7 & 52.1\
2008 Q2 & 14 & 31.9 & 42.0 & 0.59 & -9.5 & 32.6\
2008 Q3 & 15 & 41.0 & 42.7 & 0.61 & -2.6 & 40.1\
2008 Q4 & 16 & 43.8 & 43.2 & 0.59 & 1.4 & 44.7\
2009 Q1 & 17 & 55.6 & 43.9 & 0.62 & 10.7 & 54.5\
2009 Q2 & 18 & 33.9 & 44.4 & 0.59 & -9.5 & 35.0\
2009 Q3 & 19 & 42.1 & 45.0 & 0.58 & -2.6 & 42.5\
2009 Q4 & 20 & 45.6 & 45.6 & 0.55 & 1.4 & 47.0\
2010 Q1 & 21 & 59.8 & 46.2 & 0.62 & 10.7 & 56.8\
2010 Q2 & 22 & 35.2 & 46.8 & 0.57 & -9.5 & 37.3\
2010 Q3 & 23 & 44.3 & 47.3 & 0.56 & -2.6 & 44.8\
2010 Q4 & 24 & 47.9 & 47.8 & 0.53 & 1.4 & 49.3\
[0.2cm] & h & & & & & y~T+h|T~\
2011 Q1 & 1 & & & & & 59.0\
2011 Q2 & 2 & & & & & 39.4\
2011 Q3 & 3 & & & & & 46.9\
2011 Q4 & 4 & & & & & 51.3\
2012 Q1 & 5 & & & & & 61.1\
2012 Q2 & 6 & & & & & 41.5\
2012 Q3 & 7 & & & & & 49.0\
2012 Q4 & 8 & & & & & 53.4\

The smoothing parameters and initial estimates for the components have been estimated by minimizing SSE ($\alpha=0.025$, $\beta^*=0.023$, $\gamma=0$ and SSE$=60.27$, RMSE$=1.585$).

\@ref(tbl-7-HWmulti) =0.4cm

<span>lYYYYYY</span> Qtr-Year & t & y~t~ & ~t~ & b~t~ & s~t~ & ~t~\
2004 Q1 & -3 & - & - & - & 1.3 & -\
2004 Q2 & -2 & - & - & - & 0.8 & -\
2004 Q3 & -1 & - & - & - & 0.9 & -\
2004 Q4 & 0 & & 32.2 & 0.93 & 1.0 &\
2005 Q1 & 1 & 41.7 & 33.1 & 0.93 & 1.3 & 41.8\
2005 Q2 & 2 & 24.0 & 32.9 & 0.78 & 0.8 & 25.9\
2005 Q3 & 3 & 32.3 & 33.9 & 0.81 & 0.9 & 31.9\
2005 Q4 & 4 & 37.3 & 35.4 & 0.91 & 1.0 & 35.7\
2006 Q1 & 5 & 46.2 & 36.4 & 0.92 & 1.3 & 45.9\
2006 Q2 & 6 & 29.3 & 37.9 & 1.00 & 0.8 & 28.4\
2006 Q3 & 7 & 36.5 & 38.7 & 0.98 & 0.9 & 36.8\
2006 Q4 & 8 & 43.0 & 40.6 & 1.11 & 1.0 & 40.8\
2007 Q1 & 9 & 48.9 & 40.4 & 0.92 & 1.3 & 52.7\
2007 Q2 & 10 & 31.2 & 41.2 & 0.90 & 0.8 & 31.5\
2007 Q3 & 11 & 37.7 & 41.1 & 0.76 & 0.9 & 39.8\
2007 Q4 & 12 & 40.4 & 40.8 & 0.60 & 1.0 & 43.0\
2008 Q1 & 13 & 51.2 & 41.0 & 0.54 & 1.3 & 52.3\
2008 Q2 & 14 & 31.9 & 41.7 & 0.57 & 0.8 & 31.6\
2008 Q3 & 15 & 41.0 & 42.7 & 0.63 & 0.9 & 40.0\
2008 Q4 & 16 & 43.8 & 43.0 & 0.58 & 1.0 & 44.6\
2009 Q1 & 17 & 55.6 & 43.7 & 0.61 & 1.3 & 55.1\
2009 Q2 & 18 & 33.9 & 44.4 & 0.61 & 0.8 & 33.8\
2009 Q3 & 19 & 42.1 & 44.8 & 0.58 & 0.9 & 42.6\
2009 Q4 & 20 & 45.6 & 44.9 & 0.52 & 1.0 & 46.6\
2010 Q1 & 21 & 59.8 & 46.2 & 0.63 & 1.3 & 57.5\
2010 Q2 & 22 & 35.2 & 46.6 & 0.59 & 0.8 & 35.7\
2010 Q3 & 23 & 44.3 & 47.0 & 0.57 & 0.9 & 44.6\
2010 Q4 & 24 & 47.9 & 47.2 & 0.51 & 1.0 & 48.9\
[0.2cm] & h & & & & & y~T+h|T~\
2011 Q1 & 1 & & & & & 60.3\
2011 Q2 & 2 & & & & & 36.7\
2011 Q3 & 3 & & & & & 46.1\
2011 Q4 & 4 & & & & & 50.6\
2012 Q1 & 5 & & & & & 62.8\
2012 Q2 & 6 & & & & & 38.2\
2012 Q3 & 7 & & & & & 48.0\
2012 Q4 & 8 & & & & & 52.7\

The smoothing parameters and initial estimates for the components have been estimated by minimizing SSE ($\alpha=0$, $\beta^*=0$ and $\gamma=0$ and SSE$=34.6$, RMSE$=1.201$).

![Estimated components for Holt-Winters method with additive and
multiplicative seasonal components.](fig_7_LevelTrendSeas.pdf)

\@ref(fig-7-LevelTrendSeas)

states \<- cbind(fit1$model$states[,1:3],fit2$model$states[,1:3])
colnames(states) \<-
c(“level”,“slope”,“seasonal”,“level”,“slope”,“seasonal”) plot(states,
xlab=“Year”) fit1$model$state[,1:3] fitted(fit1) fit1[[“mean”]]

### Holt {-}-Winters damped method

A method that is often the single most accurate forecasting method for seasonal data is the Holt-Winters method with a damped trend and multiplicative seasonality:

$$\begin{aligned}
\pred{y}{t+h}{t} &= [\ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t}]s_{t-m+\h+}. \\
\ell_{t} &= \alpha(y_{t} / s_{t-m}) + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1}             \\
s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + \phi b_{t-1})} + (1 - \gamma)s_{t-m}\end{aligned}$$

hw(x, damped=TRUE, seasonal=“multiplicative”)

