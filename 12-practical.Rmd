# Some practical forecasting issues {#ch-practical}

In this final chapter, we address many practical issues that arise in forecasting, and discuss some possible solutions.

## Weekly, daily and sub-daily data

Weekly, daily and sub-daily data can be challenging for forecasting, although for different reasons. 

### Weekly data {-}

Weekly data is difficult because the seasonal period (the number of weeks in a year) is both large and non-integer. The average number of weeks in a year is `r round(365.25/7, 2)`. Most of the methods we have considered require the seasonal period to be an integer. Even if we approximate it by 52, most of the methods will not handle such a large seasonal period efficiently.

The simplest approach is to use a dynamic harmonic regression model, as discussed in Section \@ref(sec-dhr).  Here is an example using weekly data on US finished motor gasoline products supplied (in thousands of barrels per day) from February 1991 to May 2005. The number of Fourier terms was selected by minimizing the AICc. The order of the ARIMA model is also selected by minimizing the AICc, although that is done within the `auto.arima()` function.

```{r gasweekly, message=FALSE, cache=TRUE}
bestfit <- list(aicc=Inf)
for(i in seq(25))
{
  fit <- auto.arima(gasoline, xreg=fourier(gasoline, K=i), seasonal=FALSE)
  if(fit$aicc < bestfit$aicc)
  {
    bestfit <- fit
    besti <- i
  }
}
fc <- forecast(bestfit, xreg=fourier(gasoline, K=besti, h=104))
autoplot(fc)

```

The fitted model has `r besti` pairs of Fourier terms and can be written as
$$
  y_t = bt + \sum_{j=1}^{`r besti`} \left[ \alpha_j\sin\left(\frac{2\pi j t}{52.18}\right) + \beta_j\cos\left(\frac{2\pi j t}{52.18}\right) \right] + n_t
$$
where $n_t$ is an `r as.character(bestfit)` process. Because $n_t$ is non-stationary, the model is actually estimated on the differences of the variables on both sides of this equation. There are `r 2*besti` parameters to capture the seasonality which is rather a lot, but apparently required according to the AICc selection.  The total number of degrees of freedom is `r length(coef(bestfit))` (the other three coming from the 2 MA parameters and the drift parameter).

An alternative approach is the TBATS model introduced in Section \@ref(sec:complexseasonality). This was the subject of [Exercise 11.2](sec-ex-11.html). In this example, the forecasts are almost identical and there is little to differentiate the two models. The TBATS model is preferable when the seasonality changes over time. The ARIMA approach is preferable if there are covariates that are useful predictors as these can be added as additional regressors.

### Daily and sub-daily data {-}

Daily and sub-daily data are challenging for a different reason --- they often involve multiple seasonal patterns, and so we need to use a method that handles such complex seasonality.

Of course, if the time series is relatively short so that only one type of seasonality is present, than it will be possible to use one of the single-seasonal methods we have discussed (e.g., ETS or seasonal ARIMA). But when the time series is long enough so that some of the longer seasonal periods become apparent, it will be necessary to use dynamic harmonic regression or TBATS, as discussed in Section \@ref(sec:complexseasonality).

However, note that even these models only allow for regular seasonality. Capturing seasonality associated with moving events such as Easter, Id, or the Chinese New Year is more difficult. Even with monthly data, this can be tricky as the festivals can fall in either March or April (for Easter), in January or February (for the Chinese New Year), or at any time of the year (for Id).

The best way to deal with moving holiday effects is to use dummy variables. However, neither ETS nor TBATS models allow for covariates. Amongst the models discussed in this book (and implemented in the forecast package for R), the only choice is a dynamic regression model, where the predictors include any dummy holiday effects (and possibly also the seasonality using Fourier terms).

## Time series of counts

## Ensuring forecasts stay within limits

It is common to want forecasts to be positive, or to require them to be within some specified range $[a,b]$. Both of these situations are relatively easy to handle using transformations.

### Positive forecasts {-}

To impose a positivity constraint, simply work on the log scale, by specifying the Box-Cox parameter $\lambda=0$. For example, consider the real price of a dozen eggs (1900-1993; in cents):

```{r positiveeggs}
eggs %>%
  ets(model="AAN", damped=FALSE, lambda=0) %>%
  forecast(h=50, biasadj=TRUE) %>%
  autoplot()
```    

Because we set `biasadj=TRUE`, the forecasts are the means of the forecast distributions.

### Forecasts constrained to an interval {-}

To see how to handle data constrained to an interval, imagine that the egg prices were constrained to lie within $a=50$ and $b=400$. Then we can transform the data using a scaled logit transform which maps $(a,b)$ to the whole real line:
$$
y = \log\left(\frac{x-a}{b-x}\right),
$$
where $x$ is on the original scale and $y$ is the transformed data. To reverse the transformation, we will use
$$
x  = \frac{(b-a)e^y}{1+e^y} + a.
$$
This is not a built-in transformation, so we will need to do more work.


```{r constrained}
    # Bounds
    a <- 50
    b <- 400
    # Transform data and fit model
    fit <- log((eggs-a)/(b-eggs)) %>%
      ets(model="AAN", damped=FALSE)
    fc <- forecast(fit, h=50)
    # Back-transform forecasts
    fc$mean <- (b-a)*exp(fc$mean)/(1+exp(fc$mean)) + a
    fc$lower <- (b-a)*exp(fc$lower)/(1+exp(fc$lower)) + a
    fc$upper <- (b-a)*exp(fc$upper)/(1+exp(fc$upper)) + a
    fc$x <- eggs
    # Plot result on original scale
    autoplot(fc)
```    

No bias-adjustment has been used here, so the forecasts are the medians of the future distributions. The prediction intervals from these transformations have the same coverage probability as on the transformed scale, because quantiles are preserved under monotonically increasing transformations.

The prediction intervals lie above 50 due to the transformation. As a result of this artificial (and unrealistic) constraint, the forecast distributions have become extremely skewed.

## Forecast combinations {#sec-combinations}

## Prediction intervals for aggregates

## Backcasting

Sometimes it is useful to "backcast" a time series --- that is, forecast in reverse time. Although there are no in-built R functions to do this, it is very easy to implement. The following functions reverse a `ts` object and a `forecast` object. 

```{r backcasting_functions}
# Function to reverse time
reverse_ts <- function(y)
{
  ts(rev(y), start=tsp(y)[1L], frequency=frequency(y))
}
# Function to reverse a forecast
reverse_forecast <- function(object)
{
  h <- length(object$mean)
  f <- frequency(object$mean)
  object$x <- reverse_ts(object$x)
  object$mean <- ts(rev(object$mean), 
                    end=tsp(object$x)[1L]-1/f, frequency=f)
  object$lower <- object$lower[h:1L,]
  object$upper <- object$upper[h:1L,]
  return(object)
}
```

Then we can apply these function to backcast any time series. Here is an example applied to quarterly retail trade in the Euro area. The data are from 1996-2011. We backcast to predict the years 1994-1995.

```{r backcasting}
# Backcast example
euretail %>%
  reverse_ts() %>%
  auto.arima() %>% 
  forecast() %>%
  reverse_forecast() -> bc
autoplot(bc) + ggtitle(paste("Backcasts from",bc$method))
```



## Forecasting very short and very long time series

## Out-of-sample one-step forecasts

## In-sample multi-step forecasts

## Dealing with missing values and outliers

## Bootstrapping and bagging

