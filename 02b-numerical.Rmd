## Numerical data summaries

Numerical summaries of data sets are widely used to capture some essential features of the data with a few numbers. A summary number calculated from the data is called a *statistic*.

###Univariate statistics {-}

For a single data set, the most widely used statistics are the *average* and *median*.

Suppose $T$ denotes the total number of observations and $x_t$ denotes the $t$th observation. Then the average can be written as[^1]
$$ 
 \bar{x} = \frac{1}{T}\sum_{t=1}^T x_{t} = (x_{1} + x_{2} + x_3 + \cdots + x_{T})/T .
$$ 
The average is also called the *sample mean*.

[^1]:  The $\sum$ indicates that the values of $x_{i}$ are to be summed from $i=1$ to $i=T$.

By way of illustration, consider the maximum temperatures at Moorabbin Airport (not far from Monash University in Melbourne, Australia) from 2001 to 2016:

```{r, echo=FALSE}
temps <- window(maxtemp, start=2001)
temptable <- matrix(temps, nrow=2)
pander::pander(format(temptable, digits=3))
```

In this example, $N=16$ and $x_{t}$ denotes the maximum temperature in year $t$. Then the average maximum temperature is
\begin{align*}
\bar{x} & = \frac{1}{16}\sum_{t=1}^{16} x_{t} \\
        & = (x_{1} + x_{2} + x_{3} + \dots + x_{16})/16 \\
        & = (`r temps[1]` + `r temps[2]` + `r temps[3]` + \dots + 
        `r temps[14]` + `r temps[15]` + `r temps[16]`)/16 \\
        & = `r sum(temps)`/16  = `r round(mean(temps),2)` \text{ degrees Celsius}.
\end{align*}

The *median*, on the other hand, is the middle observation when the data are placed in order. In this case, there are 16 observations and so the median is the average of the 8th and 9th largest observations. That is
$$ 
 \text{median} = (`r sort(temps)[8]` + `r sort(temps)[9]`)/2 
               = `r median(temps)`.
$$

*Percentiles* are useful for describing the distribution of data. For example, 90% of the data are no larger than the 90th percentile. In the maximum temperature example, the 90th percentile is 
 `r quantile(temps,0.9)`
because 90% of the data (14 observations) are less than or equal to 
 `r quantile(temps,0.9)`. 
Similarly, the 75th percentile is `r quantile(temps,0.75)` and the 25th percentile is `r quantile(temps,0.25)`. The median is the 50th percentile.

A useful measure of how spread out the data are is the *interquartile range* or IQR. This is simply the difference between the 75th and 25th percentiles. Thus it contains the middle 50% of the data. For the example,
$$ 
 \text{IQR} = (`r quantile(temps,0.75)` - 
 `r format(quantile(temps,0.25),nsmall=2)`) = 
 `r IQR(temps)`. 
$$

An alternative and more common measure of spread is the *standard deviation*. This is given by the formula
$$ 
  s = \sqrt{\frac{1}{T-1} \sum_{t=1}^T (x_{t} - \bar{x})^2}. 
$$
In the example, the standard deviation is
```{r echo=FALSE}
meantemp <- round(mean(temps),2)
```
$$ 
  s = \sqrt{\frac{1}{15} \left[ 
  (`r temps[1]`- `r meantemp`)^2 + 
  (`r temps[2]`- `r meantemp`)^2 + 
  \cdots + 
  (`r temps[16]`- `r meantemp`)^2 \right]} 
  = `r round(sd(temps),2)`
$$

```{r}
temps <- window(maxtemp, start=2001)
summary(temps)
sd(temps)
```

###Bivariate statistics {-}

The most commonly used bivariate statistic is the *correlation coefficient*. It measures the strength of the relationship between two variables and can be written as
$$ 
  r = \frac{\sum (x_{i} - \bar{x})(y_{i}-\bar{y})}{\sqrt{\sum(x_{i}-\bar{x})^2}\sqrt{\sum(y_{i}-\bar{y})^2}}, 
$$
where the first variable is denoted by $x$ and the second variable by $y$. The correlation coefficient only measures the strength of the *linear* relationship; it is possible for two variables to have a strong non-linear relationship but low correlation coefficient. The value of $r$ always lies between -1 and 1 with negative values indicating a negative relationship and positive values indicating a postive relationship.

For example, the correlation between the Roomnights and Takings shown in Figure \@ref(fig:motel2) is 
  `r round(cor(motel[,"Roomnights"], motel[,"Takings"]),2)`. 
The value is positive because the Takings increases as the Roomnights increases. 

```{r corplot, echo=FALSE, fig.cap="Examples of data sets with different levels of correlation."}
library(gridExtra)
corplot <- function(rho) {
  require(mvtnorm, quietly=TRUE)
  x <- as.data.frame(rmvnorm(100, sigma = matrix(c(1, rho, rho, 1), 2, 2)))
  colnames(x) <- c("x","y")
  ggplot(x, aes(x=x,y=y)) + geom_point() +
    xlab("") + ylab("") + 
    ggtitle(paste("Correlation=", format(rho,nsmall=2))) + 
    theme(axis.text.y=element_blank(),axis.ticks=element_blank(),
          axis.text.x=element_blank(),plot.title=element_text(size=10))
}
p1 = corplot(-0.99)
p2 = corplot(-0.75)
p3 = corplot(-0.5)
p4 = corplot(-0.25)
p5 = corplot(0.99)
p6 = corplot(0.75)
p7 = corplot(0.5)
p8 = corplot(0.25)
grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,ncol=4)
```

The graphs in Figure \@ref(fig:corplot) show examples of data sets with varying levels of correlation.  Those in Figure \@ref(fig:anscombe) all have correlation coefficients of 0.82, but they have very different shaped relationships. This shows how important it is not to rely only on correlation coefficients but also to look at the plots of the data.

```{r anscombe, fig.cap="Each of these plots has a correlation coefficient of 0.82. Data from Anscombe F. J. (1973) Graphs in statistical analysis. American Statistician, 27, 17â€“21.",echo=FALSE}
library(gridExtra)
p1 <- ggplot(anscombe) + geom_point(aes(x1, y1)) +
  expand_limits(x=range(anscombe[,1:4])) +
  expand_limits(y=range(anscombe[,5:8])) +
  labs(title = "dataset 1", x="x", y="y")
p2 <- ggplot(anscombe) + geom_point(aes(x2, y2)) +
  expand_limits(x=range(anscombe[,1:4])) +
  expand_limits(y=range(anscombe[,5:8])) +
  labs(title = "dataset 2", x="x", y="y")
p3 <- ggplot(anscombe) + geom_point(aes(x3, y3)) +
  expand_limits(x=range(anscombe[,1:4])) +
  expand_limits(y=range(anscombe[,5:8])) +
  labs(title = "dataset 3", x="x", y="y")
p4 <- ggplot(anscombe) + geom_point(aes(x4, y4)) +
  expand_limits(x=range(anscombe[,1:4])) +
  expand_limits(y=range(anscombe[,5:8])) +
  labs(title = "dataset 4", x="x", y="y")
grid.arrange(p1, p2, p3, p4)
```

###Autocorrelation {-}

Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between *lagged values* of a time series. There are several autocorrelation coefficients, depending on the lag length. For example, $r_{1}$ measures the relationship between $y_{t}$ and $y_{t-1}$, $r_{2}$ measures the relationship between $y_{t}$ and $y_{t-2}$, and so on.

Figure \@ref(fig:beerlagplot) displays scatterplots of the beer production time series where the horizontal axis shows lagged values of the time series. Each graph shows $y_{t}$ plotted against $y_{t-k}$ for different values of $k$. The autocorrelations are the correlations associated with these scatterplots.

```{r beerlagplot, fig.cap="Lagged scatterplots for quarterly beer production.", fig.width=8, fig.height=8}
beer2 <- window(ausbeer, start=1992, end=2006-.1)
gglagplot(beer2, lags=9, do.lines=FALSE)
```

The value of $r_{k}$ can be written as
$$
 r_{k} = \frac{\sum\limits_{t=k+1}^T (y_{t}-\bar{y})(y_{t-k}-\bar{y})}
 {\sum\limits_{t=1}^T (y_{t}-\bar{y})^2}, 
$$
where $T$ is the length of the time series.

The first nine autocorrelation coefficients for the beer production data are given in the following table.

```{r, echo=FALSE}
beeracf <- matrix(acf(c(beer2), lag.max=9,
                      plot=FALSE)$acf[-1,,1], nrow=1)
colnames(beeracf) <- paste("$r_",1:9,"$",sep="")
knitr::kable(beeracf, booktabs=TRUE,
             align="c", digits=3,
             format.args=list(nsmall=3))
```

These correspond to the nine scatterplots in the graph above. The autocorrelation coefficients are normally plotted to form the *autocorrelation function* or ACF. The plot is also known as a *correlogram*.

```{r beeracf, fig.cap="Autocorrelation function of quarterly beer production."}
ggAcf(beer2)
```

In this graph:

-   $r_{4}$ is higher than for the other lags. This is due to the seasonal pattern in the data: the peaks tend to be four quarters apart and the troughs tend to be two quarters apart.
-   $r_{2}$ is more negative than for the other lags because troughs tend to be two quarters behind peaks.

###White noise {-}

Time series that show no autocorrelation are called "white noise". Figure \@ref(fig:wnoise) gives an example of a white noise series.

```{r wnoise, fig.cap="A white noise time series."}
set.seed(30)
x <- ts(rnorm(50))
autoplot(x, main="White noise")
```

```{r wnoiseacf, fig.cap="Autocorrelation function for the white noise series."}
ggAcf(x)
```
For white noise series, we expect each autocorrelation to be close to zero. Of course, they are not exactly equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within $\pm 2/\sqrt{T}$ where $T$ is the length of the time series. It is common to plot these bounds on a graph of the ACF. If there are one or more large spikes outside these bounds, or if more than 5% of spikes are outside these bounds, then the series is probably not white noise.

In this example, $T=50$ and so the bounds are at $\pm 2/\sqrt{50} = \pm 0.28$. All autocorrelation coefficients lie within these limits, confirming that the data are white noise.




