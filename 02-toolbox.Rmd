#The forecaster's toolbox {#toolbox}

Before we discuss any forecasting methods, it is necessary to build a toolbox of techniques that will be useful for many different forecasting situations. Each of the tools discussed in this chapter will be used repeatedly in subsequent chapters as we develop and explore a range of forecasting methods.

##Graphics

The first thing to do in any data analysis task is to plot the data. Graphs enable many features of the data to be visualized including patterns, unusual observations, changes over time, and relationships between variables. The features that are seen in plots of the data must then be incorporated, as far as possible, into the forecasting methods to be used. Just as the type of data determines what forecasting method to use, it also determines what graphs are appropriate.

###Time plots {-}

For time series data, the obvious graph to start with is a time plot. That is, the observations are plotted against the time of observation, with consecutive observations joined by straight lines. The figure below shows the weekly economy passenger load on Ansett Airlines between Australia's two largest cities.

```{r fig.cap="Weekly economy passenger load on Ansett Airlines "}
autoplot(melsyd[,"Economy.Class"],
  main="Economy class passengers: Melbourne-Sydney",
  xlab="Year",ylab="Thousands")
```


The time plot immediately reveals some interesting features.

-   There was a period in 1989 when no passengers were carried --- this was due to an industrial dispute.
-   There was a period of reduced load in 1992. This was due to a trial in which some economy class seats were replaced by business class seats.
-   A large increase in passenger load occurred in the second half of 1991.
-   There are some large dips in load around the start of each year. These are due to holiday effects.
-   There is a long-term fluctuation in the level of the series which increases during 1987, decreases in 1989 and increases again through 1990 and 1991.
-   There are some periods of missing observations.

Any model will need to take account of all these features in order to effectively forecast the passenger load into the future. A simpler time series is shown in Figure \@ref(fig:a10).

```{r a10, fig.cap="Monthly sales of antidiabetic drugs in Australia."}
autoplot(a10, ylab="$ million", xlab="Year", main="Antidiabetic drug sales")
```

Here there is a clear and increasing trend. There is also a strong seasonal pattern that increases in size as the level of the series increases. The sudden drop at the end of each year is caused by a government subsidisation scheme that makes it cost-effective for patients to stockpile drugs at the end of the calendar year. Any forecasts of this series would need to capture the seasonal pattern, and the fact that the trend is changing slowly.

###Time series patterns {-}

In describing these time series, we have used words such as "trend" and "seasonal" which need to be more carefully defined.

-   A *trend* exists when there is a long-term increase or decrease in the data. There is a trend in the antidiabetic drug sales data shown above.
-   A *seasonal* pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. The monthly sales of antidiabetic drugs above shows seasonality partly induced by the change in cost of the drugs at the end of the calendar year.
-   A *cycle* occurs when the data exhibit rises and falls that are not of a fixed period. These fluctuations are usually due to economic conditions and are often related to the "business cycle". The economy class passenger data above showed some indications of cyclic effects.

It is important to distinguish cyclic patterns and seasonal patterns. Seasonal patterns have a fixed and known length, while cyclic patterns have variable and unknown length. The average length of a cycle is usually longer than that of seasonality, and the magnitude of cyclic variation is usually more variable than that of seasonal variation. Cycles and seasonality are discussed further in Section \@ref(sec-6-1-TSpatterns).

Many time series include trend, cycles and seasonality. When choosing a forecasting method, we will first need to identify the time series patterns in the data, and then choose a method that is able to capture the patterns properly.

###Seasonal plots {-}

A seasonal plot is similar to a time plot except that the data are plotted against the individual "seasons" in which the data were observed. An example is given below showing the antidiabetic drug sales.


```{r, fig.cap="Seasonal plot of monthly antidiabetic drug sales in Australia.", out.width="90%"}
ggseasonplot(a10, year.labels=TRUE, year.labels.left=TRUE) +
  ylab("$ million") + ggtitle("Seasonal plot: antidiabetic drug sales")
```

These are exactly the same data shown earlier, but now the data from each season are overlapped. A seasonal plot allows the underlying seasonal pattern to be seen more clearly, and is especially useful in identifying years in which the pattern changes.

In this case, it is clear that there is a large jump in sales in January each year. Actually, these are probably sales in late December as customers stockpile before the end of the calendar year, but the sales are not registered with the government until a week or two later. The graph also shows that there was an unusually low number of sales in March 2008 (most other years show an increase between February and March). The small number of sales in June 2008 is probably due to incomplete counting of sales at the time the data were collected.

###Seasonal subseries plots {-}

An alternative plot that emphasises the seasonal patterns is where the data for each season are collected together in separate mini time plots.

```{r, fig.cap="Seasonal subseries plot of monthly antidiabetic drug sales in Australia."}
ggmonthplot(a10) + ylab("$ million") + 
  ggtitle("Seasonal subseries plot: antidiabetic drug sales")
```

The horizontal lines indicate the means for each month. This form of plot enables the underlying seasonal pattern to be seen clearly, and also shows the changes in seasonality over time. It is especially useful in identifying changes within particular seasons. In this example, the plot is not particularly revealing; but in some cases, this is the most useful way of viewing seasonal changes over time.

###Scatterplots {-}

The graphs discussed so far are useful for visualizing individual time series. It is also useful to explore relationships *between* time series.

Figure \@ref(fig:motel) shows two time series: monthly takings (in \$million) from accommodation at hotels, motels and guest houses in Victoria (top) and total room nights for each corresponding month (in thousands). "Room nights" is the total number of rooms booked multiplied by the number of nights people stayed in those rooms.

```{r motel, fig.cap="Monthly takings and room nights for accommodation in Victoria, Australia."}
autoplot(motel[,2:1]/1000, facet=TRUE) +
  xlab("Year") + ylab("") +
  ggtitle("Total monthly accommodation: Victoria, Australia")
```

We can study how relationship between takings and roomnights by plotting one series against the other.

```{r motel2, fig.cap="Monthly takings plotted against room nights for accommodation in Victoria, Australia."}
qplot(Roomnights/1000, Takings/1000, data=as.data.frame(motel)) +
  ylab("Takings ($million)") + xlab("Room nights (thousands)") 
```

This scatterplot helps us visualize the relationship between the variables.



<!-- ####Scatterplot matrices {-} -->

<!-- When there are several potential predictor variables, it is useful to plot each variable against each other variable. These plots can be arranged in a scatterplot matrix, as shown in Figure ?? -->

<!-- For each panel, the variable on the vertical axis is given by the variable name in that row, and the variable on the horizontal axis is given by the variable name in that column. For example, -->

<!-- The value of the scatterplot matrix is that it enables a quick view of the relationships between all pairs of variables. Outliers can also be seen. In this example, there are -->

## Numerical data summaries

Numerical summaries of data sets are widely used to capture some essential features of the data with a few numbers. A summary number calculated from the data is called a *statistic*.

###Univariate statistics {-}

For a single data set, the most widely used statistics are the *average* and *median*.

Suppose $T$ denotes the total number of observations and $x_t$ denotes the $t$th observation. Then the average can be written as:^[The $\sum$ indicates that the values of $x_{i}$ are to be summed from $i=1$ to $i=T$.]
$$ 
 \bar{x} = \frac{1}{T}\sum_{t=1}^T x_{t} = (x_{1} + x_{2} + x_3 + \cdots + x_{T})/T .
$$ 
The average is also called the *sample mean*.


By way of illustration, consider the maximum temperatures at Moorabbin Airport (not far from Monash University in Melbourne, Australia) from 2001 to 2016:

```{r, echo=FALSE}
temps <- window(maxtemp, start=2001)
temptable <- matrix(temps, nrow=2)
pander::pander(format(temptable, digits=3))
```

In this example, $N=16$ and $x_{t}$ denotes the maximum temperature in year $t$. Then the average maximum temperature is
\begin{align*}
\bar{x} & = \frac{1}{16}\sum_{t=1}^{16} x_{t} \\
        & = (x_{1} + x_{2} + x_{3} + \dots + x_{16})/16 \\
        & = (`r temps[1]` + `r temps[2]` + `r temps[3]` + \dots + 
        `r temps[14]` + `r temps[15]` + `r temps[16]`)/16 \\
        & = `r sum(temps)`/16  = `r round(mean(temps),2)` \text{ degrees Celsius}.
\end{align*}

The *median*, on the other hand, is the middle observation when the data are placed in order. In this case, there are 16 observations and so the median is the average of the 8th and 9th largest observations. That is
$$ 
 \text{median} = (`r sort(temps)[8]` + `r sort(temps)[9]`)/2 
               = `r median(temps)`.
$$

*Percentiles* are useful for describing the distribution of data. For example, 90% of the data are no larger than the 90th percentile. In the maximum temperature example, the 90th percentile is 
 `r quantile(temps,0.9)`
because 90% of the data (14 observations) are less than or equal to 
 `r quantile(temps,0.9)`. 
Similarly, the 75th percentile is `r quantile(temps,0.75)` and the 25th percentile is `r quantile(temps,0.25)`. The median is the 50th percentile.

A useful measure of how spread out the data are is the *interquartile range* or IQR. This is simply the difference between the 75th and 25th percentiles. Thus it contains the middle 50% of the data. For the example,
$$ 
 \text{IQR} = (`r quantile(temps,0.75)` - 
 `r format(quantile(temps,0.25),nsmall=2)`) = 
 `r IQR(temps)`. 
$$

An alternative and more common measure of spread is the *standard deviation*. This is given by the formula
$$ 
  s = \sqrt{\frac{1}{T-1} \sum_{t=1}^T (x_{t} - \bar{x})^2}. 
$$
In the example, the standard deviation is
```{r echo=FALSE}
meantemp <- round(mean(temps),2)
```
$$ 
  s = \sqrt{\frac{1}{15} \left[ 
  (`r temps[1]`- `r meantemp`)^2 + 
  (`r temps[2]`- `r meantemp`)^2 + 
  \cdots + 
  (`r temps[16]`- `r meantemp`)^2 \right]} 
  = `r round(sd(temps),2)`
$$

```{r}
temps <- window(maxtemp, start=2001)
summary(temps)
sd(temps)
```

###Bivariate statistics {-}

The most commonly used bivariate statistic is the *correlation coefficient*. It measures the strength of the relationship between two variables and can be written as
$$ 
  r = \frac{\sum (x_{i} - \bar{x})(y_{i}-\bar{y})}{\sqrt{\sum(x_{i}-\bar{x})^2}\sqrt{\sum(y_{i}-\bar{y})^2}}, 
$$
where the first variable is denoted by $x$ and the second variable by $y$. The correlation coefficient only measures the strength of the *linear* relationship; it is possible for two variables to have a strong non-linear relationship but low correlation coefficient. The value of $r$ always lies between -1 and 1 with negative values indicating a negative relationship and positive values indicating a postive relationship.

For example, the correlation between the Roomnights and Takings shown in Figure \@ref(fig:motel2) is 
  `r round(cor(motel[,"Roomnights"], motel[,"Takings"]),2)`. 
The value is positive because the Takings increases as the Roomnights increases. 

```{r corplot, echo=FALSE, fig.cap="Examples of data sets with different levels of correlation."}
library(gridExtra)
corplot <- function(rho) {
  require(mvtnorm, quietly=TRUE)
  x <- as.data.frame(rmvnorm(100, sigma = matrix(c(1, rho, rho, 1), 2, 2)))
  colnames(x) <- c("x","y")
  ggplot(x, aes(x=x,y=y)) + geom_point() +
    xlab("") + ylab("") + 
    ggtitle(paste("Correlation=", format(rho,nsmall=2))) + 
    theme(axis.text.y=element_blank(),axis.ticks=element_blank(),
          axis.text.x=element_blank(),plot.title=element_text(size=10))
}
p1 = corplot(-0.99)
p2 = corplot(-0.75)
p3 = corplot(-0.5)
p4 = corplot(-0.25)
p5 = corplot(0.99)
p6 = corplot(0.75)
p7 = corplot(0.5)
p8 = corplot(0.25)
grid.arrange(p1,p2,p3,p4,p5,p6,p7,p8,ncol=4)
```

The graphs in Figure \@ref(fig:corplot) show examples of data sets with varying levels of correlation.  Those in Figure \@ref(fig:anscombe) all have correlation coefficients of 0.82, but they have very different shaped relationships. This shows how important it is not to rely only on correlation coefficients but also to look at the plots of the data.

```{r anscombe, fig.cap="Each of these plots has a correlation coefficient of 0.82. Data from Anscombe F. J. (1973) Graphs in statistical analysis. American Statistician, 27, 17–21.",echo=FALSE}
library(gridExtra)
p1 <- ggplot(anscombe) + geom_point(aes(x1, y1)) +
  expand_limits(x=range(anscombe[,1:4])) +
  expand_limits(y=range(anscombe[,5:8])) +
  labs(title = "dataset 1", x="x", y="y")
p2 <- ggplot(anscombe) + geom_point(aes(x2, y2)) +
  expand_limits(x=range(anscombe[,1:4])) +
  expand_limits(y=range(anscombe[,5:8])) +
  labs(title = "dataset 2", x="x", y="y")
p3 <- ggplot(anscombe) + geom_point(aes(x3, y3)) +
  expand_limits(x=range(anscombe[,1:4])) +
  expand_limits(y=range(anscombe[,5:8])) +
  labs(title = "dataset 3", x="x", y="y")
p4 <- ggplot(anscombe) + geom_point(aes(x4, y4)) +
  expand_limits(x=range(anscombe[,1:4])) +
  expand_limits(y=range(anscombe[,5:8])) +
  labs(title = "dataset 4", x="x", y="y")
grid.arrange(p1, p2, p3, p4)
```

###Autocorrelation {-}

Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between *lagged values* of a time series. There are several autocorrelation coefficients, depending on the lag length. For example, $r_{1}$ measures the relationship between $y_{t}$ and $y_{t-1}$, $r_{2}$ measures the relationship between $y_{t}$ and $y_{t-2}$, and so on.

Figure \@ref(fig:beerlagplot) displays scatterplots of the beer production time series where the horizontal axis shows lagged values of the time series. Each graph shows $y_{t}$ plotted against $y_{t-k}$ for different values of $k$. The autocorrelations are the correlations associated with these scatterplots.

```{r beerlagplot, fig.cap="Lagged scatterplots for quarterly beer production.", fig.width=8, fig.height=8}
beer2 <- window(ausbeer, start=1992, end=2006-.1)
gglagplot(beer2, lags=9, do.lines=FALSE)
```

The value of $r_{k}$ can be written as
$$
 r_{k} = \frac{\sum\limits_{t=k+1}^T (y_{t}-\bar{y})(y_{t-k}-\bar{y})}
 {\sum\limits_{t=1}^T (y_{t}-\bar{y})^2}, 
$$
where $T$ is the length of the time series.

The first nine autocorrelation coefficients for the beer production data are given in the following table.

```{r, echo=FALSE}
beeracf <- matrix(acf(c(beer2), lag.max=9,
                      plot=FALSE)$acf[-1,,1], nrow=1)
colnames(beeracf) <- paste("$r_",1:9,"$",sep="")
knitr::kable(beeracf, booktabs=TRUE,
             align="c", digits=3,
             format.args=list(nsmall=3))
```

These correspond to the nine scatterplots in the graph above. The autocorrelation coefficients are normally plotted to form the *autocorrelation function* or ACF. The plot is also known as a *correlogram*.

```{r beeracf, fig.cap="Autocorrelation function of quarterly beer production."}
ggAcf(beer2)
```

In this graph:

-   $r_{4}$ is higher than for the other lags. This is due to the seasonal pattern in the data: the peaks tend to be four quarters apart and the troughs tend to be two quarters apart.
-   $r_{2}$ is more negative than for the other lags because troughs tend to be two quarters behind peaks.

###White noise {-}

Time series that show no autocorrelation are called "white noise". Figure \@ref(fig:wnoise) gives an example of a white noise series.

```{r wnoise, fig.cap="A white noise time series."}
set.seed(30)
x <- ts(rnorm(50))
autoplot(x, main="White noise")
```

```{r wnoiseacf, fig.cap="Autocorrelation function for the white noise series."}
ggAcf(x)
```
For white noise series, we expect each autocorrelation to be close to zero. Of course, they are not exactly equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within $\pm 2/\sqrt{T}$ where $T$ is the length of the time series. It is common to plot these bounds on a graph of the ACF. If there are one or more large spikes outside these bounds, or if more than 5% of spikes are outside these bounds, then the series is probably not white noise.

In this example, $T=50$ and so the bounds are at $\pm 2/\sqrt{50} = \pm 0.28$. All autocorrelation coefficients lie within these limits, confirming that the data are white noise.




## Some simple forecasting methods

Some forecasting methods are very simple and surprisingly effective. Here are four methods that we will use as benchmarks for other forecasting methods.

###Average method {-}

Here, the forecasts of all future values are equal to the mean of the historical data. If we let the historical data be denoted by $y_{1},\dots,y_{T}$, then we can write the forecasts as
$$ 
  \hat{y}_{T+h|T} = \bar{y} = (y_{1}+\dots+y_{T})/T. 
$$
The notation $\hat{y}_{T+h|T}$ is a short-hand for the estimate of $y_{T+h}$ based on the data $y_1,\dots,y_T$.

Although we have used time series notation here, this method can also be used for cross-sectional data (when we are predicting a value not included in the data set). Then the prediction for values not observed is the average of those values that have been observed. The remaining methods in this section are only applicable to time series data.

```r
meanf(y, h) 
# y contains the time series
# h is the forecast horizon
```

###Naïve method {-}

This method is only appropriate for time series data. All forecasts are simply set to be the value of the last observation. That is, the forecasts of all future values are set to be $y_{T}$, where $y_T$ is the last observed value. This method works remarkably well for many economic and financial time series.

```r
naive(y, h)
rwf(y, h) # Alternative
```

###Seasonal naïve method {-}

A similar method is useful for highly seasonal data. In this case, we set each forecast to be equal to the last observed value from the same season of the year (e.g., the same month of the previous year). Formally, the forecast for time $T+h$ is written as
$$ 
  y_{T+h-km} \text{ where $m=$ seasonal period, $k=\lfloor (h-1)/m\rfloor+1$,} 
$$ 
and $\lfloor u \rfloor$ denotes the integer part of $u$. That looks more complicated than it really is. For example, with monthly data, the forecast for all future February values is equal to the last observed February value. With quarterly data, the forecast of all future Q2 values is equal to the last observed Q2 value (where Q2 means the second quarter). Similar rules apply for other months and quarters, and for other seasonal periods.

```r
snaive(y, h)
```


###Drift method {-}

A variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the drift) is set to be the average change seen in the historical data. So the forecast for time $T+h$ is given by
$$ 
  y_{T} + \frac{h}{T-1}\sum_{t=2}^T (y_{t}-y_{t-1}) = y_{T} + h \left( \frac{y_{T} -y_{1}}{T-1}\right). 
$$ 
This is equivalent to drawing a line between the first and last observation, and extrapolating it into the future.

```r
rwf(y, h, drift=TRUE)
```

###Examples {-}

Figure \@ref(fig:beerf) shows the first three methods applied to the quarterly beer production data. 

```{r beerf, fig.cap="Forecasts of Australian quarterly beer production.", warning=FALSE, message=FALSE}
beer2 <- window(ausbeer,start=1992,end=2006-.1)
beerdf <- cbind(Actual = beer2, 
  Mean = meanf(beer2, h=11)$mean,
  Naive = naive(beer2, h=11)$mean,
  'Seasonal Naive' = snaive(beer2, h=11)$mean)
autoplot(beerdf) + 
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  scale_color_manual(values=c('#000000','#1b9e77','#d95f02','#7570b3'),
                     breaks=c("Mean","Naive","Seasonal Naive"),
                     name="Forecast Method")
```

In Figure \@ref(fig:djf), the non-seasonal methods were applied to a series of 250 days of the Dow Jones Index. 

```{r djf, fig.cap="Forecasts based on 250 days of the Dow Jones Index.", warnings=FALSE, message=FALSE}
dj2 <- window(dj,end=250)
djdf <- cbind(Actual=dj2, 
              Mean=meanf(dj2, h=42)$mean,
              Naive=rwf(dj2, h=42)$mean,
              Drift=rwf(dj2, h=42, drift=TRUE)$mean)
autoplot(djdf) +
  ggtitle("Dow Jones Index (daily ending 15 Jul 94)") +
  xlab("Day") + ylab("") +
  scale_color_manual(values=c('#000000','#1b9e77','#d95f02','#7570b3'),
                     breaks=c("Mean","Naive","Drift"),
                     name="Forecast Method")
```

Sometimes one of these simple methods will be the best forecasting method available. But in many cases, these methods will serve as benchmarks rather than the method of choice. That is, whatever forecasting methods we develop, they will be compared to these simple methods to ensure that the new method is better than these simple alternatives. If not, the new method is not worth considering.

## Transformations and adjustments

Adjusting the historical data can often lead to a simpler forecasting model. Here we deal with four kinds of adjustments: mathematical transformations, calendar adjustments, population adjustments and inflation adjustments. The purpose of all these transformations and adjustments is to simplify the patterns in the historical data by removing known sources of variation or by making the pattern more consistent across the whole data set. Simpler patterns usually lead to more accurate forecasts.

###Mathematical transformations {-}

If the data show variation that increases or decreases with the level of the series, then a transformation can be useful. For example, a logarithmic transformation is often useful. If we denote the original observations as $y\_{1},\dots,y\_{T}$ and the transformed observations as $w_{1}, \dots, w_{T}$, then $w_t = \log(y_t)$. Logarithms are useful because they are interpretable: changes in a log value are relative (or percentage) changes on the original scale. So if log base 10 is used, then an increase of 1 on the log scale corresponds to a multiplication of 10 on the original scale. Another useful feature of log transformations is that they constrain the forecasts to stay positive on the original scale.

Sometimes other transformations are also used (although they are not so interpretable). For example, square roots and cube roots can be used. These are called *power transformations* because they can be written in the form $w_{t} = y_{t}^p$.

A useful family of transformations that includes logarithms and power transformations is the family of "Box-Cox transformations", which depend on the parameter $\lambda$ and are defined as follows:
$$ 
w_t  = 
  \begin{cases} 
    \log(y_t) & \text{if $\lambda=0$};  \\ 
    (y_t^\lambda-1)/\lambda & \text{otherwise}. 
  \end{cases}
$$

The logarithm in a Box-Cox transformation is always a natural logarithm (i.e., to base $e$). So if $\lambda=0$, natural logarithms are used, but if $\lambda\ne0$, a power transformation is used followed by some simple scaling.

The following figure shows a logarithmic transformation of the monthly electricity demand data. *Click the figure to see a series of other transformations.*

```{r bcelec, fig.cap="Power transformations for Australian monthly electricity data."}
autoplot(log(elec)) + ylab("Transformed electricity demand") +
 xlab("Year") + ggtitle("Log monthly electricity demand")
```

A good value of $\lambda$ is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler. In this case, $\lambda=0.30$ works quite well, although any value of $\lambda$ between 0 and 0.5 would give similar results.

```r
# The BoxCox.lambda() function will choose a value of lambda for you.
lambda <- BoxCox.lambda(elec) # = 0.27
plot(BoxCox(elec,lambda))
```

Having chosen a transformation, we need to forecast the transformed data. Then, we need to reverse the transformation (or *back-transform*) to obtain forecasts on the original scale. The reverse Box-Cox transformation is given by
$$ 
  y_{t} = 
    \begin{cases} 
      \exp(w_{t}) & \lambda=0;\\ 
      (\lambda w_t+1)^{1/\lambda} & \text{otherwise}.
    \end{cases} 
  $$

### Features of power transformations {-}

-   If some $y_{t}\le0$, no power transformation is possible unless all observations are adjusted by adding a constant to all values.
-   Choose a simple value of $\lambda$. It makes explanations easier.
-   Forecasting results are relatively insensitive to the value of $\lambda$.
-   Often no transformation is needed.
-   Transformations sometimes make little difference to the forecasts but have a large effect on prediction intervals.

###Calendar adjustments {-}

Some variation seen in seasonal data may be due to simple calendar effects. In such cases, it is usually much easier to remove the variation before fitting a forecasting model.

For example, if you are studying monthly milk production on a farm, then there will be variation between the months simply because of the different numbers of days in each month in addition to seasonal variation across the year.

```{r cowmilk, fig.cap="Monthly milk production per cow. Source: Cryer (2006)."}
monthdays <- rep(c(31,28,31,30,31,30,31,31,30,31,30,31),14)
monthdays[26 + (4*12)*(0:2)] <- 29
tmp <- cbind(Monthly = milk, DailyAverage=milk/monthdays)
autoplot(tmp, facet=TRUE) + xlab("Years") + ylab("Pounds") +
  ggtitle("Milk production per cow")
```

Notice how much simpler the seasonal pattern is in the average daily production plot compared to the average monthly production plot. By looking at average daily production instead of average monthly production, we effectively remove the variation due to the different month lengths. Simpler patterns are usually easier to model and lead to more accurate forecasts.

A similar adjustment can be done for sales data when the number of trading days in each month will vary. In this case, the sales per trading day can be modelled instead of the total sales for each month.

###Population adjustments {-}

Any data that are affected by population changes can be adjusted to give per-capita data. That is, consider the data per person (or per thousand people, or per million people) rather than the total. For example, if you are studying the number of hospital beds in a particular region over time, the results are much easier to interpret if you remove the effect of population changes by considering number of beds per thousand people. Then you can see if there have been real increases in the number of beds, or whether the increases are entirely due to population increases. It is possible for the total number of beds to increase, but the number of beds per thousand people to decrease. This occurs when the population is increasing faster than the number of hospital beds. For most data that are affected by population changes, it is best to use per-capita data rather than the totals.

###Inflation adjustments {-}

Data that are affected by the value of money are best adjusted before modelling. For example, data on the average cost of a new house will have increased over the last few decades due to inflation. A \$200,000 house this year is not the same as a \$200,000 house twenty years ago. For this reason, financial time series are usually adjusted so all values are stated in dollar values from a particular year. For example, the house price data may be stated in year 2000 dollars.

To make these adjustments a price index is used. If $z_{t}$ denotes the price index and $y_{t}$ denotes the original house price in year $t$, then $x_{t} = y_{t}/z_{t} * z_{2000}$ gives the adjusted house price at year 2000 dollar values. Price indexes are often constructed by government agencies. For consumer goods, a common price index is the Consumer Price Index (or CPI).
## Evaluating forecast accuracy {#accuracy}

###Forecast accuracy measures {-}

Let $y_{i}$ denote the $i$th observation and $\hat{y}_{i}$ denote a forecast of $y_{i}$.

### Scale-dependent errors {-}

The forecast error is simply $e_{i}=y_{i}-\hat{y}_{i}$, which is on the same scale as the data. Accuracy measures that are based on $e_{i}$ are therefore scale-dependent and cannot be used to make comparisons between series that are on different scales.

The two most commonly used scale-dependent measures are based on the absolute errors or squared errors:
\begin{align*}
  \text{Mean absolute error: MAE} & = \text{mean}(|e_{i}|),\\
  \text{Root mean squared error: RMSE} & = \sqrt{\text{mean}(e_{i}^2)}.
\end{align*}
When comparing forecast methods on a single data set, the MAE is popular as it is easy to understand and compute.

### Percentage errors {-}

The percentage error is given by $p_{i} = 100 e_{i}/y_{i}$. Percentage errors have the advantage of being scale-independent, and so are frequently used to compare forecast performance between different data sets. The most commonly used measure is:
$$
  \text{Mean absolute percentage error: MAPE} = \text{mean}(|p_{i}|).
$$
Measures based on percentage errors have the disadvantage of being infinite or undefined if $y_{i}=0$ for any $i$ in the period of interest, and having extreme values when any $y_{i}$ is close to zero. Another problem with percentage errors that is often overlooked is that they assume a meaningful zero. For example, a percentage error makes no sense when measuring the accuracy of temperature forecasts on the Fahrenheit or Celsius scales.

They also have the disadvantage that they put a heavier penalty on negative errors than on positive errors. This observation led to the use of the so-called "symmetric" MAPE (sMAPE) proposed by Armstrong (1985, p.348), which was used in the M3 forecasting competition. It is defined by
$$ 
  \text{sMAPE} = \text{mean}\left(200|y_{i} - \hat{y}_{i}|/(y_{i}+\hat{y}_{i})\right). 
$$
However, if $y_{i}$ is close to zero, $\hat{y}_{i}$ is also likely to be close to zero. Thus, the measure still involves division by a number close to zero, making the calculation unstable. Also, the value of sMAPE can be negative, so it is not really a measure of "absolute percentage errors" at all.

Hyndman and Koehler (2006) recommend that the sMAPE not be used. It is included here only because it is widely used, although we will not use it in this book.

### Scaled errors {-}

Scaled errors were proposed by Hyndman and Koehler (2006) as an alternative to using percentage errors when comparing forecast accuracy across series on different scales. They proposed scaling the errors based on the *training* MAE from a simple forecast method. For a non-seasonal time series, a useful way to define a scaled error uses naïve forecasts:
$$
  q_{j} = \frac{\displaystyle e_{j}}
    {\displaystyle\frac{1}{T-1}\sum_{t=2}^T |y_{t}-y_{t-1}|}.
$$

Because the numerator and denominator both involve values on the scale of the original data, $q_{j}$ is independent of the scale of the data. A scaled error is less than one if it arises from a better forecast than the average naïve forecast computed on the training data. Conversely, it is greater than one if the forecast is worse than the average naïve forecast computed on the training data. For seasonal time series, a scaled error can be defined using seasonal naïve forecasts: 
$$ 
  q_{j} = \frac{\displaystyle e_{j}}
    {\displaystyle\frac{1}{T-m}\sum_{t=m+1}^T |y_{t}-y_{t-m}|}. 
$$

The *mean absolute scaled error* is simply
$$
  \text{MASE} = \text{mean}(|q_{j}|).
$$
Similarly, the *mean squared scaled error* (MSSE) can be defined where the errors (on the training data and test data) are squared instead of using absolute values.

###Examples {-}

```{r beeraccuracy, fig.cap="Forecasts of Australian quarterly beer production using data up to the end of 2007."}
beer2 <- window(ausbeer,start=1992,end=c(2007,4))
beerfit1 <- meanf(beer2,h=10)
beerfit2 <- rwf(beer2,h=10)
beerfit3 <- snaive(beer2,h=10)
tmp <- cbind(Data=window(ausbeer, start=1992),
             Mean=beerfit1$mean, 
             Naive=beerfit2$mean, 
             SeasonalNaive=beerfit3$mean)
autoplot(tmp) + xlab("Year") + ylab("Megalitres") +
  ggtitle("Forecasts for quarterly beer production") + 
  scale_color_manual(values=c('#000000','#1b9e77','#d95f02','#7570b3'),
                     breaks=c("Mean","Naive","SeasonalNaive"),
                     name="Forecast Method")
```


Figure \ref@(fig:beeraccuracy) shows three forecast methods applied to the quarterly Australian beer production using data only to the end of 2007. The actual values for the period 2008--2010 are also shown. We compute the forecast accuracy measures for this period.


```r
beer3 <- window(ausbeer, start=2008)
accuracy(beerfit1, beer3)
accuracy(beerfit2, beer3)
accuracy(beerfit3, beer3)
```

```{r echo=FALSE}
beer3 <- window(ausbeer, start=2008)
tab <- matrix(NA,ncol=4,nrow=3)
tab[1,] <- accuracy(beerfit1, beer3)[2,c(2,3,5,6)]
tab[2,] <- accuracy(beerfit2, beer3)[2,c(2,3,5,6)]
tab[3,] <- accuracy(beerfit3, beer3)[2,c(2,3,5,6)]
colnames(tab) <- c("RMSE","MAE","MAPE","MASE")
rownames(tab) <- c("Mean method", "Naïve method", "Seasonal naïve method")
knitr::kable(tab, digits=2)
```


It is obvious from the graph that the seasonal naïve method is best for these data, although it can still be improved, as we will discover later. Sometimes, different accuracy measures will lead to different results as to which forecast method is best. However, in this case, all the results point to the seasonal naïve method as the best of these three methods for this data set.

To take a non-seasonal example, consider the Dow Jones Index. The following graph shows the 250 observations ending on 15 July 1994, along with forecasts of the next 42 days obtained from three different methods.

```{r, fig.cap="Forecasts of the Dow Jones Index from 16 July 1994."}
dj2 <- window(dj, end=250)
djfc1 <- meanf(dj2, h=42)
djfc2 <- rwf(dj2, h=42)
djfc3 <- rwf(dj2, drift=TRUE, h=42)

tmp <- cbind(Data=dj, Mean=djfc1$mean, Naive=djfc2$mean, Drift=djfc3$mean)
autoplot(tmp) + xlab("Day") + ylab("") +
  ggtitle("Dow Jones Index (daily ending 15 Jul 94)") +
  scale_color_manual(values=c('#000000','#1b9e77','#d95f02','#7570b3'),
                     breaks=c("Mean","Naive","Drift"),
                     name="Forecast Method")
```

```r
dj3 <- window(dj, start=251)
accuracy(djfc1, dj3)
accuracy(djfc2, dj3)
accuracy(djfc3, dj3)
```

```{r echo=FALSE}
dj3 <- window(dj, start=251)
tab <- matrix(NA,ncol=4,nrow=3)
tab[1,] <- accuracy(djfc1, dj3)[2,c(2,3,5,6)]
tab[2,] <- accuracy(djfc2, dj3)[2,c(2,3,5,6)]
tab[3,] <- accuracy(djfc3, dj3)[2,c(2,3,5,6)]
colnames(tab) <- c("RMSE","MAE","MAPE","MASE")
rownames(tab) <- c("Mean method", "Naïve method", "Drift method")
knitr::kable(tab, digits=2)
```

Here, the best method is the drift method (regardless of which accuracy measure is used).

###Training and test sets {-}

It is important to evaluate forecast accuracy using genuine forecasts. That is, it is invalid to look at how well a model fits the historical data; the accuracy of forecasts can only be determined by considering how well a model performs on new data that were not used when fitting the model. When choosing models, it is common to use a portion of the available data for fitting, and use the rest of the data for testing the model, as was done in the above examples. Then the testing data can be used to measure how well the model is likely to forecast on new data.

The size of the test set is typically about 20% of the total sample, although this value depends on how long the sample is and how far ahead you want to forecast. The size of the test set should ideally be at least as large as the maximum forecast horizon required. The following points should be noted.

-   A model which fits the data well does not necessarily forecast well.
-   A perfect fit can always be obtained by using a model with enough parameters.
-   Over-fitting a model to data is as bad as failing to identify the systematic pattern in the data.

Some references describe the test set as the "hold-out set" because these data are "held out" of the data used for fitting. Other references call the training set the "in-sample data" and the test set the "out-of-sample data". We prefer to use "training set" and "test set" in this book.

###Cross-validation {-}

A more sophisticated version of training/test sets is cross-validation. For cross-sectional data, cross-validation works as follows.

1.  Select observation $i$ for the test set, and use the remaining observations in the training set. Compute the error on the test observation.
2.  Repeat the above step for $i=1,2,\dots,N$ where $N$ is the total number of observations.
3.  Compute the forecast accuracy measures based on the errors obtained.

This is a much more efficient use of the available data, as you only omit one observation at each step. However, it can be very time consuming to implement.

For time series data, the procedure is similar but the training set consists only of observations that occurred *prior* to the observation that forms the test set. Thus, no future observations can be used in constructing the forecast. However, it is not possible to get a reliable forecast based on a very small training set, so the earliest observations are not considered as test sets. Suppose $k$ observations are required to produce a reliable forecast. Then the process works as follows.

1.  Select the observation at time $k+i$ for the test set, and use the observations at times $1,2,\dots,k+i-1$ to estimate the forecasting model. Compute the error on the forecast for time $k+i$.
2.  Repeat the above step for $i=1,2,\dots,T-k$ where $T$ is the total number of observations.
3.  Compute the forecast accuracy measures based on the errors obtained.

This procedure is sometimes known as a "rolling forecasting origin" because the "origin" ($k+i-1$) at which the forecast is based rolls forward in time.

With time series forecasting, one-step forecasts may not be as relevant as multi-step forecasts. In this case, the cross-validation procedure based on a rolling forecasting origin can be modified to allow multi-step errors to be used. Suppose we are interested in models that produce good $h$-step-ahead forecasts.

1.  Select the observation at time $k+h+i-1$ for the test set, and use the observations at times $1,2,\dots,k+i-1$ to estimate the forecasting model. Compute the $h$-step error on the forecast for time $k+h+i-1$.
2.  Repeat the above step for $i=1,2,\dots,T-k-h+1$ where $T$ is the total number of observations.
3.  Compute the forecast accuracy measures based on the errors obtained.

When $h=1$, this gives the same procedure as outlined above.
## Residual diagnostics

A residual in forecasting is the difference between an observed value and its forecast based on other observations: $e_{i} = y_{i}-\hat{y}_{i}$. For time series forecasting, a residual is based on one-step forecasts; that is $\hat{y}_{t}$ is the forecast of $y_{t}$ based on observations $y_{1},\dots,y_{t-1}.$ For cross-sectional forecasting, a residual is based on all other observations; that is $\hat{y}_i$ is the prediction of $y_i$ based on all observations *except* $y_i$.

A good forecasting method will yield residuals with the following properties:

-   The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts.
-   The residuals have zero mean. If the residuals have a mean other than zero, then the forecasts are biased.

Any forecasting method that does not satisfy these properties can be improved. That does not mean that forecasting methods that satisfy these properties can not be improved. It is possible to have several forecasting methods for the same data set, all of which satisfy these properties. Checking these properties is important to see if a method is using all available information well, but it is not a good way for selecting a forecasting method.

If either of these two properties is not satisfied, then the forecasting method can be modified to give better forecasts. Adjusting for bias is easy: if the residuals have mean $m$, then simply add $m$ to all forecasts and the bias problem is solved. Fixing the correlation problem is harder and we will not address it until Chapter \@ref(????).

In addition to these essential properties, it is useful (but not necessary) for the residuals to also have the following two properties.

-   The residuals have constant variance.
-   The residuals are normally distributed.

These two properties make the calculation of prediction intervals easier (see the [next section](http://otexts.com/fpp/2/7) for an example). However, a forecasting method that does not satisfy these properties cannot necessarily be improved. Sometimes applying a transformation such as a logarithm or a square root may assist with these properties, but otherwise there is usually little you can do to ensure your residuals have constant variance and have a normal distribution. Instead, an alternative approach to finding prediction intervals is necessary. Again, we will not address how to do this until later in the book.

###Example: Forecasting the Dow-Jones Index {-}

For stock market indexes, the best forecasting method is often the naïve method. That is each forecast is simply equal to the last observed value, or $\hat{y}_{t} = y_{t-1}$. Hence, the residuals are simply equal to the difference between consecutive observations:
$$
  e_{t} = y_{t} - \hat{y}_{t} = y_{t} - y_{t-1}.
$$

The following graphs show the Dow Jones Index (DJI), and the residuals obtained from forecasting the DJI with the naïve method.

```{r, fig.cap="The Dow Jones Index measured daily to 15 July 1994."}
dj2 <- window(dj, end=250)
autoplot(dj2) + xlab("Day") + ylab("") + 
  ggtitle("Dow Jones Index (daily ending 15 Jul 94)")
```

```{r, fig.cap="Residuals from forecasting the Dow Jones Index with the naïve method."}
res <- residuals(naive(dj2))
autoplot(res) + xlab("Day") + ylab("") + 
  ggtitle("Residuals from naive method")
```

```{r, fig.cap="Histogram of the residuals from the naïve method applied to the Dow Jones Index. The left tail is a little too long for a normal distribution."}
qplot(res, bins=nclass.FD(na.omit(res))) +
  ggtitle("Histogram of residuals")
```

```{r, fig.cap="ACF of the residuals from the naïve method applied to the Dow Jones Index. The lack of correlation suggests the forecasts are good."}
ggAcf(res) + ggtitle("ACF of residuals")
```

These graphs show that the naïve method produces forecasts that appear to account for all available information. The mean of the residuals is very close to zero and there is no significant correlation in the residuals series. The time plot of the residuals shows that the variation of the residuals stays much the same across the historical data, so the residual variance can be treated as constant. However, the histogram suggests that the residuals may not follow a normal distribution --- the left tail looks a little too long. Consequently, forecasts from this method will probably be quite good but prediction intervals computed assuming a normal distribution may be inaccurate.

###Portmanteau tests for autocorrelation {-}

In addition to looking at the ACF plot, we can do a more formal test for autocorrelation by considering a whole set of $r_k$ values as a group, rather than treat each one separately.

Recall that $r_k$ is the autocorrelation for lag $k$. When we look at the ACF plot to see if each spike is within the required limits, we are implicitly carrying out multiple hypothesis tests, each one with a small probability of giving a false positive. When enough of these tests are done, it is likely that at least one will give a false positive and so we may conclude that the residuals have some remaining autocorrelation, when in fact they do not.

In order to overcome this problem, we test whether the first $h$ autocorrelations are significantly different from what would be expected from a white noise process. A test for a group of autocorrelations is called a *portmanteau test*, from a French word describing a suitcase containing a number of items.

One such test is the *Box-Pierce test* based on the following statistic
$$
  Q = T \sum_{k=1}^h r_k^2, 
$$ 
where $h$ is the maximum lag being considered and $T$ is number of observations. If each $r_k$ is close to zero, then $Q$ will be small. If some $r_k$ values are large (positive or negative), then $Q$ will be large. We suggest using $h=10$ for non-seasonal data and $h=2m$ for seasonal data, where $m$ is the period of seasonality. However, the test is not good when $h$ is large, so if these values are larger than $T/5$, then use $h=T/5$

A related (and more accurate) test is the *Ljung-Box test* based on
$$
  Q^* = T(T+2) \sum_{k=1}^h (T-k)^{-1}r_k^2.
$$

Again, large values of $Q^*$ suggest that the autocorrelations do not come from a white noise series.

How large is too large? If the autocorrelations did come from a white noise series, then both $Q$ and $Q^*$ would have a $\chi^2$ distribution with $(h - K)$ degrees of freedom where $K$ is the number of parameters in the model. If they are calculated from raw data (rather than the residuals from a model), then set $K=0$.

For the Dow-Jones example, the naive model has no parameters, so $K=0$ in that case also. For both $Q$ and $Q^*$, the results are not significant (i.e., the p-values are relatively large). So we can conclude that the residuals are not distinguishable from a white noise series.

```{r}
# lag=h and fitdf=K
Box.test(res, lag=10, fitdf=0)

Box.test(res,lag=10, fitdf=0, type="Lj")
```

## Prediction intervals

As discussed in [Section 1/7](../../1/7), a prediction interval gives an interval within which we expect $y_{i}$ to lie with a specified probability. For example, assuming the forecast errors are uncorrelated and normally distributed, then a simple 95% prediction interval for the next observation in a time series is
$$
  \hat{y}_{t} \pm 1.96 \hat\sigma,
$$
where $\hat\sigma$ is an estimate of the standard deviation of the forecast distribution. In forecasting, it is common to calculate 80% intervals and 95% intervals, although any percentage may be used.

When forecasting one-step ahead, the standard deviation of the forecast distribution is almost the same as the standard deviation of the residuals. (In fact, the two standard deviations are identical if there are no parameters to be estimated such as with the naïve method. For forecasting methods involving parameters to be estimated, the standard deviation of the forecast distribution is slightly larger than the residual standard deviation, although this difference is often ignored.)

For example, consider a naïve forecast for the Dow-Jones Index. The last value of the observed series is 3830, so the forecast of the next value of the DJI is 3830. The standard deviation of the residuals from the naïve method is 21.99. Hence, a 95% prediction interval for the next value of the DJI is
$$
  3830 \pm 1.96(21.99) = [3787, 3873].
$$
Similarly, an 80% prediction interval is given by
$$
  3830 \pm 1.28(21.99) = [3802,3858].
$$

The value of the multiplier (1.96 or 1.28) determines the percentage of the prediction interval. The following table gives the values to be used for different percentages.


```{r echo=FALSE}
p <- c(seq(50,95,by=5),96:99)
tab <- data.frame(Percentage=p, Multiplier=qnorm(0.5+p/200))
knitr::kable(tab, digits=2, booktabs=TRUE,
             caption="Multipliers to be used for prediction intervals.")
```

The use of this table and the formula $\hat{y}_{i} \pm k \hat\sigma$ (where $k$ is the multiplier) assumes that the residuals are normally distributed and uncorrelated. If either of these conditions does not hold, then this method of producing a prediction interval cannot be used.

The value of prediction intervals is that they express the uncertainty in the forecasts. If we only produce point forecasts, there is no way of telling how accurate the forecasts are. But if we also produce prediction intervals, then it is clear how much uncertainty is associated with each forecast. For this reason, point forecasts can be of almost no value without accompanying prediction intervals.

To produce a prediction interval, it is necessary to have an estimate of the standard deviation of the forecast distribution. For one-step forecasts for time series, the residual standard deviation provides a good estimate of the forecast standard deviation. But for all other situations, including multi-step forecasts for time series, a more complicated method of calculation is required. These calculations are usually done with standard forecasting software and need not trouble the forecaster (unless he or she is writing the software!).

A common feature of prediction intervals is that they increase in length as the forecast horizon increases. The further ahead we forecast, the more uncertainty is associated with the forecast, and so the prediction intervals grow wider. However, there are some (non-linear) forecasting methods that do not have this attribute.

If a transformation has been used, then the prediction interval should be computed on the transformed scale, and the end points back-transformed to give a prediction interval on the original scale. This approach preserves the probability coverage of the prediction interval, although it will no longer be symmetric around the point forecast.
## Exercises

1. For each of the following series (from the <b>fma</b> package), make a graph of the data. If transforming seems appropriate, do so and describe the effect.

    a.  Monthly total of people on unemployed benefits in Australia (January 1956 -- July 1992).
    b.  Monthly total of accidental deaths in the United States (January 1973 -- December 1978).
    c.  Quarterly production of bricks (in millions of units) at Portland, Australia (March 1956–September 1994).

**Hints:**

 -  `data(package="fma")` will give a list of the available data.
 -  To plot a transformed data set, use `plot(BoxCox(x,0.5))`` where `x` is the name of the data set and 0.5 is the Box-Cox parameter.

2. Use the Dow Jones index (data set `dowjones`) to do the following:

    a.  Produce a time plot of the series.
    b.  Produce forecasts using the drift method and plot them.
    c.  Show that the graphed forecasts are identical to extending the line drawn between the first and last observations.
    d.  Try some of the other benchmark functions to forecast the same data set. Which do you think is best? Why?

3. Consider the daily closing IBM stock prices (data set `ibmclose`).

    a. Produce some plots of the data in order to become familiar with it.
    b.  Split the data into a training set of 300 observations and a test set of 69 observations.
    c.  Try various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?

4.  Consider the sales of new one-family houses in the USA, Jan 1973 -- Nov 1995 (data set `hsales`).

    a. Produce some plots of the data in order to become familiar with it.
    b.  Split the `hsales` data set into a training set and a test set, where the test set is the last two years of data.
    c.  Try various benchmark methods to forecast the training set and compare the results on the test set. Which method did best?

## Further reading

###Graphics and data summary {-}

-   [Cleveland, W. S. (1993). *Visualizing Data*. Hobart Press.](http://www.amazon.com/Visualizing-Data-William-S-Cleveland/dp/0963488406?tag=ote09-20)
-   [Maindonald, J. and H. Braun (2010). *Data analysis and graphics using R: an example-based approach*. 3rd ed. Cambridge, UK: Cambridge University Press.](http://www.amazon.com/Data-Analysis-Graphics-Using-Example-Based/dp/0521762936?tag=ote09-20)

###Simple forecasting methods {-}

-   [Ord, J. K. and R. Fildes (2012). *Principles of business forecasting*. South-Western College Pub.](http://www.amazon.com/Principles-Business-Forecasting-Keith-Ord/dp/0324311273?tag=ote09-20)

###Evaluating forecast accuracy {-}

-   [Hyndman, R. J. and A. B. Koehler (2006). Another look at measures of forecast accuracy. *International Journal of Forecasting* **22**(4), 679–688.](http://robjhyndman.com/papers/another-look-at-measures-of-forecast-accuracy/)
    
